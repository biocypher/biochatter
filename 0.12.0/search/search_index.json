{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Generative AI models have shown tremendous usefulness in increasing accessibility and automation of a wide range of tasks. Yet, their application to the biomedical domain is still limited, in part due to the lack of a common framework for deploying, testing, and evaluating the diverse models and auxiliary technologies that are needed. <code>biochatter</code> is a Python package implementing a generic backend library for the connection of biomedical applications to conversational AI. We describe the framework in this paper; for a more hands-on experience, check out our two web app implementations:</p> <ul> <li> <p> BioChatter Light</p> <p>Agile framework in pure Python built with Streamlit, for fast prototyping and iteration.</p> <p> Go To BioChatter Light</p> </li> <li> <p> BioChatter Next</p> <p>Advanced client-server architecture based on FastAPI and Next.js.</p> <p> Go To BioChatter Next</p> </li> </ul> <p>BioChatter is part of the BioCypher ecosystem, connecting natively to BioCypher knowledge graphs.</p> <p></p> <p>Hot Topics</p> <p>BioChatter natively extends BioCypher knowledge graphs. Check there for more information.</p> <p>We have also recently published a perspective on connecting knowledge and machine learning to enable causal reasoning in biomedicine, with a particular focus on the currently emerging \"foundation models.\" You can read it here.</p> <p> </p>"},{"location":"installation/","title":"Installation","text":"<p>To use the package, install it from PyPI, for instance using pip (<code>pip install biochatter</code>) or Poetry (<code>poetry add biochatter</code>).</p>"},{"location":"installation/#extras","title":"Extras","text":"<p>The package has some optional dependencies that can be installed using the following extras (e.g. <code>pip install biochatter[xinference]</code>):</p> <ul> <li> <p><code>xinference</code>: support for querying open-source LLMs through Xorbits Inference</p> </li> <li> <p><code>ollama</code>: support for querying open-source LLMs through Ollama</p> </li> <li> <p><code>podcast</code>: support for podcast text-to-speech (for the free Google TTS; the paid OpenAI TTS can be used without this extra)</p> </li> <li> <p><code>streamlit</code>: support for streamlit UI functions (used in BioChatter Light)</p> </li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":"<p>BioChatter is a versatile framework that can be used in various ways depending on your needs and technical background. This guide will help you get started based on your user profile and intended use case.</p>"},{"location":"quickstart/#python-developer-profile","title":"Python Developer Profile","text":"<p>If you're a Python developer looking to integrate LLM capabilities into your biomedical application:</p>"},{"location":"quickstart/#basic-installation","title":"Basic Installation","text":"<pre><code>pip install biochatter\n</code></pre>"},{"location":"quickstart/#core-usage-example","title":"Core Usage Example","text":"<pre><code>from biochatter.llm_connect import GptConversation\n\n# Initialize conversation\nconversation = GptConversation(\n    model_name=\"gpt-3.5-turbo\",\n    prompts={},\n)\nconversation.set_api_key(api_key=\"your-api-key\")\n\n# Query the model\nresponse, token_usage, correction = conversation.query(\"Your biomedical question here\")\n</code></pre> <p>This skeleton example is simply passing through the question to the LLM, which is not recommended according to our envisioned use. We recommend integrating at least one of the advanced features and dedicated prompts for model instructions.</p>"},{"location":"quickstart/#advanced-features","title":"Advanced Features","text":"<ol> <li>Knowledge Graph Connectivity: Connect to a BioCypher knowledge graph: <pre><code>from biochatter.prompts import BioCypherPromptEngine\nfrom biochatter.llm_connect import GptConversation\n\n# Create a conversation factory\ndef create_conversation():\n    conversation = GptConversation(model_name=\"gpt-3.5-turbo\", prompts={})\n    conversation.set_api_key(api_key=\"your-api-key\")\n    return conversation\n\n# Initialize the prompt engine with your BioCypher schema\nprompt_engine = BioCypherPromptEngine(\n    schema_config_or_info_path=\"path/to/schema_info.yaml\",\n    # or use schema_config_or_info_dict to pass the schema as a dictionary\n    conversation_factory=create_conversation,\n)\n\n# Generate a Cypher query based on the question\ncypher_query = prompt_engine.generate_query(\n    question=question,\n    query_language=\"Cypher\"  # defaults to Cypher if not specified\n)\n</code></pre></li> </ol> <p>The <code>BioCypherPromptEngine</code> handles: - Entity selection based on your schema - Relationship selection between entities - Property selection for entities and relationships - Query generation in your chosen query language</p> <p>For a complete example of KG integration, check out our Knowledge Graph vignette.</p> <ol> <li>API Integration: Connect to biological databases and APIs: <pre><code>from biochatter.api_agent.base.api_agent import APIAgent\nfrom biochatter.api_agent.web.oncokb import OncoKBQueryBuilder, OncoKBFetcher, OncoKBInterpreter\nfrom biochatter.llm_connect import GptConversation\n\n# Create a conversation factory function\ndef create_conversation():\n    conversation = GptConversation(\n        model_name=\"gpt-3.5-turbo\",  # or your preferred model\n        prompts={},\n        correct=False\n    )\n    conversation.set_api_key(api_key=\"your-api-key\")\n    return conversation\n\n# Create API agent with OncoKB components\nagent = APIAgent(\n    conversation_factory=create_conversation,  # Function to create new conversations\n    query_builder=OncoKBQueryBuilder(),       # Builds queries for OncoKB API\n    fetcher=OncoKBFetcher(),                 # Handles API requests\n    interpreter=OncoKBInterpreter()          # Interprets API responses\n)\n\n# Execute query - this will:\n# 1. Build an appropriate OncoKB query\n# 2. Fetch results from the OncoKB API\n# 3. Interpret the results using the LLM\nresult = agent.execute(\"What is the oncogenic potential of BRAF V600E mutation?\")\n</code></pre></li> </ol> <p>The API Agent architecture allows you to: - Create structured queries for specific APIs - Handle API requests and responses - Interpret results using LLMs - Support multiple API types (REST, Python, etc.)</p> <p>For more examples of supported APIs and custom implementations, check our API documentation.</p> <ol> <li>Vector Database Integration: For semantic search and RAG capabilities: <pre><code>from biochatter.vectorstore import DocumentReader, DocumentEmbedder\nfrom langchain_openai import OpenAIEmbeddings\n\n# Initialize document reader and embedder\nreader = DocumentReader()\n\n# Create embedder with Milvus as vector store\nembedder = DocumentEmbedder(\n    embedding_collection_name=\"your_embeddings\",\n    metadata_collection_name=\"your_metadata\",\n    connection_args={\"host\": \"localhost\", \"port\": \"19530\"}\n)\nembedder.connect()\n\n# Load and embed a document\ndocument = reader.load_document(\"path/to/your/document.pdf\")  # Supports PDF and TXT\ndoc_id = embedder.save_document(document)\n\n# Perform similarity search\nresults = embedder.similarity_search(\n    query=\"Your search query here\",\n    k=3  # Number of results to return\n)\n\n# Clean up when needed\nembedder.remove_document(doc_id)\n</code></pre></li> </ol> <p>This workflow allows: - Document chunking with customizable parameters - Metadata storage and retrieval - Similarity search and retrieval</p> <p>For more details on vector database integration, including advanced features and configurations, check our RAG documentation.</p>"},{"location":"quickstart/#streamlit-gui-developer-profile","title":"Streamlit GUI Developer Profile","text":"<p>For developers who want to create a user-friendly web interface quickly:</p> <ol> <li>Clone the BioChatter Light repository</li> <li>Install dependencies (Poetry recommended)</li> <li>Set up debugging environment using <code>streamlit run app.py</code></li> <li>Modify the app components to introduce your desired functionality (refer to the customisation vignette for more details)</li> </ol>"},{"location":"quickstart/#running-via-the-docker-image","title":"Running via the docker image","text":"<p>We provide a Docker image for BioChatter Light, and we are always happy for contributions. If you have an idea for a generally useful feature or panel, please get in touch (e.g., open an issue). Once the feature has been added to the BioChatter Light repository, it will be available via the official docker image, potentially as an optional tab activated via environment variables in the Docker setup (see the vignette for details). This allows access to the feature in many environments without the need for local installation, for instance using:</p> <pre><code>docker run -d -p 8501:8501 biocypher/biochatter-light --env-file .env\n</code></pre>"},{"location":"quickstart/#rest-api-nextjs-developer-profile","title":"REST API / Next.js Developer Profile","text":"<p>For developers building production-grade web applications:</p>"},{"location":"quickstart/#components","title":"Components","text":"<ol> <li> <p>Backend (FastAPI):</p> <ul> <li> <p>Uses BioChatter Server for the REST API</p> </li> <li> <p>Handles LLM interactions, database connections, and business logic</p> </li> </ul> </li> <li> <p>Frontend (Next.js):</p> <ul> <li> <p>Uses BioChatter Next</p> </li> <li> <p>Provides a modern, responsive UI</p> </li> </ul> </li> </ol>"},{"location":"quickstart/#getting-started-with-docker","title":"Getting Started with Docker","text":"<pre><code># Clone the repository\ngit clone https://github.com/biocypher/biochatter-next\n</code></pre> <p>Configure the Next application, modifying the YAML configuration file. For an example, check the example from our Decider Genetics use case.</p> <pre><code># Start the services\ndocker-compose up -d\n</code></pre>"},{"location":"quickstart/#open-source-model-users","title":"Open-Source Model Users","text":"<p>For users who prefer to use local, open-source LLMs:</p>"},{"location":"quickstart/#using-ollama","title":"Using Ollama","text":"<pre><code>pip install \"biochatter[ollama]\"\n</code></pre> <p>Running the Ollama software on port 11434:</p> <pre><code>from biochatter.llm_connect import OllamaConversation\n\nconversation = OllamaConversation(\n    base_url=\"http://localhost:11434\",\n    prompts={},\n    model_name='llama2',\n    correct=False,\n)\nresponse, token_usage, correction = conversation.query(\"Your question here\")\n</code></pre>"},{"location":"quickstart/#using-xinference","title":"Using Xinference","text":"<pre><code>pip install \"biochatter[xinference]\"\n</code></pre> <p>Running the Xinference software on port 9997:</p> <pre><code>from biochatter.llm_connect import XinferenceConversation\n\nconversation = XinferenceConversation(\n    base_url=\"http://localhost:9997\",\n    prompts={},\n    correct=False,\n)\nresponse, token_usage, correction = conversation.query(\"Your question here\")\n</code></pre>"},{"location":"about/design-philosophy/","title":"Design Philosophy","text":"<p>Our design philosophy is based on open-source principles and governed by the desire to keep up with the rapid pace of LLM development. We adopt an agile style of developing modular components, aiming for quick iteration on proof-of-concept implementations. The rapid introduction of new models and functionalities (e.g., tool binding) necessitates a workflow that can quickly adapt to these changes.</p> <p>This often involves the need of frequent refactorings of individual modules of the codebase, making modularity and separation of concerns a vital concern. Further, in order to facilitate collaboration and onboarding of new contributors, the codebase needs to be easily understandable and well-documented.</p>"},{"location":"about/design-philosophy/#feature-selectiveness","title":"Feature Selectiveness","text":"<p>For the average research initiative, even when factoring in community contributions, it is not feasible to compete with the resources of large companies, which are numerous in the current LLM ecosystem. As a result, it often is not sensible to work on solutions that are subject to high company interests. For instance, early BioChatter developments included a workflow for automating podcast-like summaries of scientific papers; we discontinued this effort in light of later developments by Google to provide this same functionality inside their NotebookLM platform. We aim to continue to be mindful of this issue when deciding on which features to implement, which requires constant monitoring of the broader LLM ecosystem.</p>"},{"location":"about/project/","title":"Our Mission","text":"<p>We aim to provide a flexible and transparent framework for the application of large language models (LLMs) in the realm of biomedical research and applications. We put emphasis on an open-source process, which is vital to keep up with the rapid pace of LLM development. This means we take great care to not unnecessarily reimplement solutions that exist in the broader field, such as the model chaining workflows provided by LangChain.</p> <p>Rather, we look to provide an interface tuned towards the needs of the biomedical community, with focus points on robustness in the face of health-critical applications, native connectivity to data representations used in biomedical research, and a benchmark-forward approach to model evaluation in very specific use cases.</p>"},{"location":"about/project/#our-vision","title":"Our Vision","text":"<p>Our developments have the overarching goal of making biomedical research more productive by improving accessibility of technical workflows. Importantly, we aim to automate low-level tasks to allow researchers to spend more time on higher-level reasoning and data analysis without being burdened by the tedious aspects of handling data and knowledge.</p> <p>Inspired by the supportive role of Integrated Development Environments (IDEs) in programming, we want to support research applications with an \"Integrated Research Environment\" (IRE) in the longer term. The development of this system cannot be done in isolation; if you are interested in contributing in any way, please get in touch!</p>"},{"location":"about/use-cases/","title":"Use Cases","text":"<p>If you are asking yourself \"what is this even used for?\", this page is for you.</p> <p>Large language models (LLMs) have inspired many creative applications in the recent years, and our aim is to channel these applications into a framework that offers more robust and scalable solutions for the biomedical domain. We see BioChatter as a developer toolkit that can be used to create a range of custom applications for specific user bases. As such, our main target audience are developers in the biomedical domain.</p>"},{"location":"about/use-cases/#chatbots","title":"Chatbots","text":"<p>Naturally, the most attractive general use of LLMs is to have a language interface to some functionality that previously was not accessible through such simple means. Depending on the requirements of the application, this can be either trivial (\"Create a paragraph from these bullet points.\") or extremely complex (\"What is the driver of my patient's cancer?\"). Biomedical use cases tend to be biased towards the latter, often necessitating the integration of various sources of knowledge and quality assurance of the conversational system. As a result, BioChatter is not designed to function as a sole chatbot without integration of other data or knowledge; this functionality is provided by numerous companies.</p> <p>Instead, we at least include specific prompts that tune the LLM towards the desired application and user base, which can be configured by the developer implementing the application. More powerful applications are the focus however, including tool use (external software parameterisation), knowledge graph and vector database integration, multimodal inputs, and modular agentic workflows.</p>"},{"location":"about/use-cases/#graphical-user-interfaces","title":"Graphical user interfaces","text":"<p>Increasing accessibility to previously inaccessible functionality naturally implies that the developer needs to think about how to make the functionality available to the (by definition non-technical) user. As a result, we include suggestions for diverse graphical user interface (GUI) frameworks in the ecosystem. Specifically, we provide instructive examples for relatively simple prototyping frameworks based on Streamlit, which we brand as BioChatter Light and which can be used for rapid development of a range of user interfaces:</p> <ul> <li> <p>simple configuration</p> </li> <li> <p>advanced configuration</p> </li> <li> <p>real-world use case</p> </li> </ul> <p>For more complex applications, we also provide a modular system based on current web technologies (FastAPI and Next.js), which we brand as BioChatter Next. The web application is driven by a RESTful API, which we implement in BioChatter Server. An application example can be seen in the real-world use case.</p>"},{"location":"api-docs/api-calling-base/","title":"API Calling: Base Classes","text":"<p>We provide base classes for the API Agent implementation to align individual modules.</p>"},{"location":"api-docs/api-calling-base/#abstract-base-classes","title":"Abstract base classes","text":"<p>Abstract base classes for API interaction components.</p> <p>Provides base classes for query builders, fetchers, and interpreters used in API interactions and result processing.</p>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseAPIModel","title":"<code>BaseAPIModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A base class for all API models.</p> <p>Includes default fields <code>uuid</code> and <code>method_name</code>.</p> Source code in <code>biochatter/api_agent/base/agent_abc.py</code> <pre><code>class BaseAPIModel(BaseModel):\n    \"\"\"A base class for all API models.\n\n    Includes default fields `uuid` and `method_name`.\n    \"\"\"\n\n    uuid: str | None = Field(\n        None,\n        description=\"Unique identifier for the model instance\",\n    )\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseFetcher","title":"<code>BaseFetcher</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for fetchers.</p> <p>A fetcher is responsible for submitting queries (in systems where submission and fetching are separate) and fetching and saving results of queries. It has to implement a <code>fetch_results()</code> method, which can wrap a multi-step procedure to submit and retrieve. Should implement retry method to account for connectivity issues or processing times.</p> Source code in <code>biochatter/api_agent/base/agent_abc.py</code> <pre><code>class BaseFetcher(ABC):\n    \"\"\"Abstract base class for fetchers.\n\n    A fetcher is responsible for submitting queries (in systems where\n    submission and fetching are separate) and fetching and saving results of\n    queries. It has to implement a `fetch_results()` method, which can wrap a\n    multi-step procedure to submit and retrieve. Should implement retry method to\n    account for connectivity issues or processing times.\n    \"\"\"\n\n    @abstractmethod\n    def fetch_results(\n        self,\n        query_models: list[BaseModel],\n        retries: int | None = 3,\n    ):\n        \"\"\"Fetch results by submitting a query.\n\n        Can implement a multi-step procedure if submitting and fetching are\n        distinct processes (e.g., in the case of long processing times as in the\n        case of BLAST).\n\n        Args:\n        ----\n            query_models: list of Pydantic models describing the parameterised\n                queries\n\n        \"\"\"\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseFetcher.fetch_results","title":"<code>fetch_results(query_models, retries=3)</code>  <code>abstractmethod</code>","text":"<p>Fetch results by submitting a query.</p> <p>Can implement a multi-step procedure if submitting and fetching are distinct processes (e.g., in the case of long processing times as in the case of BLAST).</p> <pre><code>query_models: list of Pydantic models describing the parameterised\n    queries\n</code></pre> Source code in <code>biochatter/api_agent/base/agent_abc.py</code> <pre><code>@abstractmethod\ndef fetch_results(\n    self,\n    query_models: list[BaseModel],\n    retries: int | None = 3,\n):\n    \"\"\"Fetch results by submitting a query.\n\n    Can implement a multi-step procedure if submitting and fetching are\n    distinct processes (e.g., in the case of long processing times as in the\n    case of BLAST).\n\n    Args:\n    ----\n        query_models: list of Pydantic models describing the parameterised\n            queries\n\n    \"\"\"\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseInterpreter","title":"<code>BaseInterpreter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for result interpreters.</p> <p>The interpreter is aware of the nature and structure of the results and can extract and summarise information from them.</p> Source code in <code>biochatter/api_agent/base/agent_abc.py</code> <pre><code>class BaseInterpreter(ABC):\n    \"\"\"Abstract base class for result interpreters.\n\n    The interpreter is aware of the nature and structure of the results and can\n    extract and summarise information from them.\n    \"\"\"\n\n    @abstractmethod\n    def summarise_results(\n        self,\n        question: str,\n        conversation_factory: Callable,\n        response_text: str,\n    ) -&gt; str:\n        \"\"\"Summarise an answer based on the given parameters.\n\n        Args:\n        ----\n            question (str): The question that was asked.\n\n            conversation_factory (Callable): A function that creates a\n                BioChatter conversation.\n\n            response_text (str): The response.text returned from the request.\n\n        Returns:\n        -------\n            A summary of the answer.\n\n        Todo:\n        ----\n            Genericise (remove file path and n_lines parameters, and use a\n            generic way to get the results). The child classes should manage the\n            specifics of the results.\n\n        \"\"\"\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseInterpreter.summarise_results","title":"<code>summarise_results(question, conversation_factory, response_text)</code>  <code>abstractmethod</code>","text":"<p>Summarise an answer based on the given parameters.</p> <pre><code>question (str): The question that was asked.\n\nconversation_factory (Callable): A function that creates a\n    BioChatter conversation.\n\nresponse_text (str): The response.text returned from the request.\n</code></pre> <pre><code>A summary of the answer.\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseInterpreter.summarise_results--todo","title":"Todo:","text":"<pre><code>Genericise (remove file path and n_lines parameters, and use a\ngeneric way to get the results). The child classes should manage the\nspecifics of the results.\n</code></pre> Source code in <code>biochatter/api_agent/base/agent_abc.py</code> <pre><code>@abstractmethod\ndef summarise_results(\n    self,\n    question: str,\n    conversation_factory: Callable,\n    response_text: str,\n) -&gt; str:\n    \"\"\"Summarise an answer based on the given parameters.\n\n    Args:\n    ----\n        question (str): The question that was asked.\n\n        conversation_factory (Callable): A function that creates a\n            BioChatter conversation.\n\n        response_text (str): The response.text returned from the request.\n\n    Returns:\n    -------\n        A summary of the answer.\n\n    Todo:\n    ----\n        Genericise (remove file path and n_lines parameters, and use a\n        generic way to get the results). The child classes should manage the\n        specifics of the results.\n\n    \"\"\"\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseQueryBuilder","title":"<code>BaseQueryBuilder</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for query builders.</p> Source code in <code>biochatter/api_agent/base/agent_abc.py</code> <pre><code>class BaseQueryBuilder(ABC):\n    \"\"\"An abstract base class for query builders.\"\"\"\n\n    @property\n    def structured_output_prompt(self) -&gt; ChatPromptTemplate:\n        \"\"\"Define a structured output prompt template.\n\n        This provides a default implementation for an API agent that can be\n        overridden by subclasses to return a ChatPromptTemplate-compatible\n        object.\n        \"\"\"\n        return ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are a world class algorithm for extracting information in structured formats.\",\n                ),\n                (\n                    \"human\",\n                    \"Use the given format to extract information from the following input: {input}\",\n                ),\n                (\"human\", \"Tip: Make sure to answer in the correct format\"),\n            ],\n        )\n\n    @abstractmethod\n    def create_runnable(\n        self,\n        query_parameters: \"BaseModel\",\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Must be implemented by subclasses. Should use the LangChain\n        `create_structured_output_runnable` method to generate the Callable.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n\n    @abstractmethod\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[BaseModel]:\n        \"\"\"Parameterise a query object.\n\n        Parameterises a Pydantic model with the fields of the API based on the\n        given question using a BioChatter conversation instance. Must be\n        implemented by subclasses.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The BioChatter conversation object containing the LLM\n                that should parameterise the query.\n\n        Returns:\n        -------\n            A list containing one or more parameterised instance(s) of the query\n            object (Pydantic BaseModel).\n\n        \"\"\"\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseQueryBuilder.structured_output_prompt","title":"<code>structured_output_prompt</code>  <code>property</code>","text":"<p>Define a structured output prompt template.</p> <p>This provides a default implementation for an API agent that can be overridden by subclasses to return a ChatPromptTemplate-compatible object.</p>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>  <code>abstractmethod</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Must be implemented by subclasses. Should use the LangChain <code>create_structured_output_runnable</code> method to generate the Callable.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/api_agent/base/agent_abc.py</code> <pre><code>@abstractmethod\ndef create_runnable(\n    self,\n    query_parameters: \"BaseModel\",\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Must be implemented by subclasses. Should use the LangChain\n    `create_structured_output_runnable` method to generate the Callable.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>  <code>abstractmethod</code>","text":"<p>Parameterise a query object.</p> <p>Parameterises a Pydantic model with the fields of the API based on the given question using a BioChatter conversation instance. Must be implemented by subclasses.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The BioChatter conversation object containing the LLM\n    that should parameterise the query.\n</code></pre> <pre><code>A list containing one or more parameterised instance(s) of the query\nobject (Pydantic BaseModel).\n</code></pre> Source code in <code>biochatter/api_agent/base/agent_abc.py</code> <pre><code>@abstractmethod\ndef parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[BaseModel]:\n    \"\"\"Parameterise a query object.\n\n    Parameterises a Pydantic model with the fields of the API based on the\n    given question using a BioChatter conversation instance. Must be\n    implemented by subclasses.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The BioChatter conversation object containing the LLM\n            that should parameterise the query.\n\n    Returns:\n    -------\n        A list containing one or more parameterised instance(s) of the query\n        object (Pydantic BaseModel).\n\n    \"\"\"\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseTools","title":"<code>BaseTools</code>","text":"<p>Abstract base class for tools.</p> Source code in <code>biochatter/api_agent/base/agent_abc.py</code> <pre><code>class BaseTools:\n    \"\"\"Abstract base class for tools.\"\"\"\n\n    def make_pydantic_tools(self) -&gt; list[BaseAPIModel]:\n        \"\"\"Uses pydantics create_model to create a list of pydantic tools from a dictionary of parameters\"\"\"\n        tools = []\n        for func_name, tool_params in self.tools_params.items():\n            tools.append(create_model(func_name, **tool_params, __base__=BaseAPIModel))\n        return tools\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.agent_abc.BaseTools.make_pydantic_tools","title":"<code>make_pydantic_tools()</code>","text":"<p>Uses pydantics create_model to create a list of pydantic tools from a dictionary of parameters</p> Source code in <code>biochatter/api_agent/base/agent_abc.py</code> <pre><code>def make_pydantic_tools(self) -&gt; list[BaseAPIModel]:\n    \"\"\"Uses pydantics create_model to create a list of pydantic tools from a dictionary of parameters\"\"\"\n    tools = []\n    for func_name, tool_params in self.tools_params.items():\n        tools.append(create_model(func_name, **tool_params, __base__=BaseAPIModel))\n    return tools\n</code></pre>"},{"location":"api-docs/api-calling-base/#the-api-agent","title":"The API Agent","text":"<p>Base API agent module.</p>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent","title":"<code>APIAgent</code>","text":"Source code in <code>biochatter/api_agent/base/api_agent.py</code> <pre><code>class APIAgent:\n    def __init__(\n        self,\n        conversation_factory: Callable,\n        query_builder: \"BaseQueryBuilder\",\n        fetcher: \"BaseFetcher\",\n        interpreter: \"BaseInterpreter\",\n    ):\n        \"\"\"API agent class to interact with a tool's API for querying and fetching\n        results.  The query fields have to be defined in a Pydantic model\n        (`BaseModel`) and used (i.e., parameterised by the LLM) in the query\n        builder. Specific API agents are defined in submodules of this directory\n        (`api_agent`). The agent's logic is implemented in the `execute` method.\n\n        Attributes\n        ----------\n            conversation_factory (Callable): A function used to create a\n                BioChatter conversation, providing LLM access.\n\n            query_builder (BaseQueryBuilder): An instance of a child of the\n                BaseQueryBuilder class.\n\n            result_fetcher (BaseFetcher): An instance of a child of the\n                BaseFetcher class.\n\n            result_interpreter (BaseInterpreter): An instance of a child of the\n                BaseInterpreter class.\n\n        \"\"\"\n        self.conversation_factory = conversation_factory\n        self.query_builder = query_builder\n        self.fetcher = fetcher\n        self.interpreter = interpreter\n        self.final_answer = None\n\n    def parameterise_query(self, question: str) -&gt; list[BaseModel] | None:\n        \"\"\"Use LLM to parameterise a query (a list of Pydantic models) based on the given\n        question using a BioChatter conversation instance.\n        \"\"\"\n        try:\n            conversation = self.conversation_factory()\n            return self.query_builder.parameterise_query(question, conversation)\n        except Exception as e:\n            print(f\"Error generating query: {e}\")\n            return None\n\n    def fetch_results(self, query_models: list[BaseModel]) -&gt; str | None:\n        \"\"\"Fetch the results of the query using the individual API's implementation\n        (either single-step or submit-retrieve).\n\n        Args:\n        ----\n            query_models: list of parameterised query Pydantic models\n\n        \"\"\"\n        try:\n            return self.fetcher.fetch_results(query_models, 100)\n        except Exception as e:\n            print(f\"Error fetching results: {e}\")\n            return None\n\n    def summarise_results(\n        self,\n        question: str,\n        response_text: str,\n    ) -&gt; str | None:\n        \"\"\"Summarise the retrieved results to extract the answer to the question.\"\"\"\n        try:\n            return self.interpreter.summarise_results(\n                question=question,\n                conversation_factory=self.conversation_factory,\n                response_text=response_text,\n            )\n        except Exception as e:\n            print(f\"Error extracting answer: {e}\")\n            return None\n\n    def execute(self, question: str) -&gt; str | None:\n        \"\"\"Wrapper that uses class methods to execute the API agent logic. Consists\n        of 1) query generation, 2) query submission, 3) results fetching, and\n        4) answer extraction. The final answer is stored in the final_answer\n        attribute.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n        \"\"\"\n        # Generate query\n        try:\n            query_models = self.parameterise_query(question)\n            if not query_models:\n                raise ValueError(\"Failed to generate query.\")\n        except ValueError as e:\n            print(e)\n\n        # Fetch results\n        try:\n            response_text = self.fetch_results(\n                query_models=query_models,\n            )\n            if not response_text:\n                raise ValueError(\"Failed to fetch results.\")\n        except ValueError as e:\n            print(e)\n\n        # Extract answer from results\n        try:\n            final_answer = self.summarise_results(question, response_text)\n            if not final_answer:\n                raise ValueError(\"Failed to extract answer from results.\")\n        except ValueError as e:\n            print(e)\n\n        self.final_answer = final_answer\n        return final_answer\n\n    def get_description(self, tool_name: str, tool_desc: str):\n        return f\"This API agent interacts with {tool_name}'s API for querying and fetching results. {tool_desc}\"\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent.__init__","title":"<code>__init__(conversation_factory, query_builder, fetcher, interpreter)</code>","text":"<p>API agent class to interact with a tool's API for querying and fetching results.  The query fields have to be defined in a Pydantic model (<code>BaseModel</code>) and used (i.e., parameterised by the LLM) in the query builder. Specific API agents are defined in submodules of this directory (<code>api_agent</code>). The agent's logic is implemented in the <code>execute</code> method.</p>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent.__init__--attributes","title":"Attributes","text":"<pre><code>conversation_factory (Callable): A function used to create a\n    BioChatter conversation, providing LLM access.\n\nquery_builder (BaseQueryBuilder): An instance of a child of the\n    BaseQueryBuilder class.\n\nresult_fetcher (BaseFetcher): An instance of a child of the\n    BaseFetcher class.\n\nresult_interpreter (BaseInterpreter): An instance of a child of the\n    BaseInterpreter class.\n</code></pre> Source code in <code>biochatter/api_agent/base/api_agent.py</code> <pre><code>def __init__(\n    self,\n    conversation_factory: Callable,\n    query_builder: \"BaseQueryBuilder\",\n    fetcher: \"BaseFetcher\",\n    interpreter: \"BaseInterpreter\",\n):\n    \"\"\"API agent class to interact with a tool's API for querying and fetching\n    results.  The query fields have to be defined in a Pydantic model\n    (`BaseModel`) and used (i.e., parameterised by the LLM) in the query\n    builder. Specific API agents are defined in submodules of this directory\n    (`api_agent`). The agent's logic is implemented in the `execute` method.\n\n    Attributes\n    ----------\n        conversation_factory (Callable): A function used to create a\n            BioChatter conversation, providing LLM access.\n\n        query_builder (BaseQueryBuilder): An instance of a child of the\n            BaseQueryBuilder class.\n\n        result_fetcher (BaseFetcher): An instance of a child of the\n            BaseFetcher class.\n\n        result_interpreter (BaseInterpreter): An instance of a child of the\n            BaseInterpreter class.\n\n    \"\"\"\n    self.conversation_factory = conversation_factory\n    self.query_builder = query_builder\n    self.fetcher = fetcher\n    self.interpreter = interpreter\n    self.final_answer = None\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent.execute","title":"<code>execute(question)</code>","text":"<p>Wrapper that uses class methods to execute the API agent logic. Consists of 1) query generation, 2) query submission, 3) results fetching, and 4) answer extraction. The final answer is stored in the final_answer attribute.</p> <pre><code>question (str): The question to be answered.\n</code></pre> Source code in <code>biochatter/api_agent/base/api_agent.py</code> <pre><code>def execute(self, question: str) -&gt; str | None:\n    \"\"\"Wrapper that uses class methods to execute the API agent logic. Consists\n    of 1) query generation, 2) query submission, 3) results fetching, and\n    4) answer extraction. The final answer is stored in the final_answer\n    attribute.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n    \"\"\"\n    # Generate query\n    try:\n        query_models = self.parameterise_query(question)\n        if not query_models:\n            raise ValueError(\"Failed to generate query.\")\n    except ValueError as e:\n        print(e)\n\n    # Fetch results\n    try:\n        response_text = self.fetch_results(\n            query_models=query_models,\n        )\n        if not response_text:\n            raise ValueError(\"Failed to fetch results.\")\n    except ValueError as e:\n        print(e)\n\n    # Extract answer from results\n    try:\n        final_answer = self.summarise_results(question, response_text)\n        if not final_answer:\n            raise ValueError(\"Failed to extract answer from results.\")\n    except ValueError as e:\n        print(e)\n\n    self.final_answer = final_answer\n    return final_answer\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent.fetch_results","title":"<code>fetch_results(query_models)</code>","text":"<p>Fetch the results of the query using the individual API's implementation (either single-step or submit-retrieve).</p> <pre><code>query_models: list of parameterised query Pydantic models\n</code></pre> Source code in <code>biochatter/api_agent/base/api_agent.py</code> <pre><code>def fetch_results(self, query_models: list[BaseModel]) -&gt; str | None:\n    \"\"\"Fetch the results of the query using the individual API's implementation\n    (either single-step or submit-retrieve).\n\n    Args:\n    ----\n        query_models: list of parameterised query Pydantic models\n\n    \"\"\"\n    try:\n        return self.fetcher.fetch_results(query_models, 100)\n    except Exception as e:\n        print(f\"Error fetching results: {e}\")\n        return None\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent.parameterise_query","title":"<code>parameterise_query(question)</code>","text":"<p>Use LLM to parameterise a query (a list of Pydantic models) based on the given question using a BioChatter conversation instance.</p> Source code in <code>biochatter/api_agent/base/api_agent.py</code> <pre><code>def parameterise_query(self, question: str) -&gt; list[BaseModel] | None:\n    \"\"\"Use LLM to parameterise a query (a list of Pydantic models) based on the given\n    question using a BioChatter conversation instance.\n    \"\"\"\n    try:\n        conversation = self.conversation_factory()\n        return self.query_builder.parameterise_query(question, conversation)\n    except Exception as e:\n        print(f\"Error generating query: {e}\")\n        return None\n</code></pre>"},{"location":"api-docs/api-calling-base/#biochatter.api_agent.base.api_agent.APIAgent.summarise_results","title":"<code>summarise_results(question, response_text)</code>","text":"<p>Summarise the retrieved results to extract the answer to the question.</p> Source code in <code>biochatter/api_agent/base/api_agent.py</code> <pre><code>def summarise_results(\n    self,\n    question: str,\n    response_text: str,\n) -&gt; str | None:\n    \"\"\"Summarise the retrieved results to extract the answer to the question.\"\"\"\n    try:\n        return self.interpreter.summarise_results(\n            question=question,\n            conversation_factory=self.conversation_factory,\n            response_text=response_text,\n        )\n    except Exception as e:\n        print(f\"Error extracting answer: {e}\")\n        return None\n</code></pre>"},{"location":"api-docs/api-calling-python/","title":"API Calling: Python APIs","text":"<p>API calling for large Python APIs is currently experimental. In particular, we observe a decrease in stability with increasing number of total parameters offered to the LLM. Due to this limitation, we recommend benchmarking the stability of the calls using our benchmarking framework. If you're interested in the performance of a specific API / LLM combination, don't hesitate to get in touch.</p>"},{"location":"api-docs/api-calling-python/#generic-python-api-ingestion","title":"Generic Python API ingestion","text":"<p>Using Pydantic parsing, we autogenerate API descriptions for tool bindings. While this allows better scaling (given suitable structure of the ingested code, particularly with respect to the docstrings), it offers less control than the manual implementation of API descriptions. For instance, it is much harder to reduce the set of parameters to the essentials.</p> <p>Module for ingesting any Python module and generating a query builder.</p> <p>AutoGenerate Pydantic classes for each callable.</p> <p>This module provides a function to generate Pydantic classes for each callable (function/method) in a given module. It extracts parameters from docstrings using docstring-parser and creates Pydantic models with fields corresponding to the parameters. If a parameter name conflicts with BaseModel attributes, it is aliased.</p>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.generic_agent.GenericQueryBuilder","title":"<code>GenericQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building a generic query using LLM tools.</p> <p>The query builder works by ingesting a Python module and generating a list of Pydantic classes for each callable in the module. It then uses these classes to parameterise a query using LLM tool binding.</p> Source code in <code>biochatter/api_agent/python/generic_agent.py</code> <pre><code>class GenericQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building a generic query using LLM tools.\n\n    The query builder works by ingesting a Python module and generating a list\n    of Pydantic classes for each callable in the module. It then uses these\n    classes to parameterise a query using LLM tool binding.\n    \"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: list[\"BaseAPIModel\"],\n        conversation: Conversation,\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for the query builder.\n\n        Args:\n        ----\n            query_parameters: The list of Pydantic classes to be used for the\n                query.\n\n            conversation: The conversation object used for parameterising the\n                query.\n\n        Returns:\n        -------\n            The runnable object for the query builder.\n\n        \"\"\"\n        runnable = conversation.chat.bind_tools(query_parameters, tool_choice=\"required\")\n        return runnable | PydanticToolsParser(tools=query_parameters)\n\n    def parameterise_query(\n        self,\n        question: str,\n        prompt: str,\n        conversation: \"Conversation\",\n        module: ModuleType,\n        generated_classes: list[BaseAPIModel] | None = None,\n    ) -&gt; list[BaseAPIModel]:\n        \"\"\"Parameterise tool calls for any Python module.\n\n        Generate a list of parameterised BaseModel instances based on the given\n        question, prompt, and BioChatter conversation. Uses a Pydantic model\n        to define the API fields.\n\n        Using langchain's `bind_tools` method to allow the LLM to parameterise\n        the function call, based on the functions available in the module.\n\n        Relies on defined structure and annotation of the passed module.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            prompt (str): The prompt to be used for the query, instructing the\n                LLM of its task and the module context.\n\n            conversation: The conversation object used for parameterising the\n                query.\n\n            module: The Python module to be used for the query.\n\n            generated_classes: The list of Pydantic classes to be used for the\n                query. If not provided, the classes will be generated from the\n                module. Allows for external injection of classes for testing\n                purposes.\n\n        Returns:\n        -------\n            list[BaseAPIModel]: the parameterised query object (Pydantic\n                model)\n\n        \"\"\"\n        if generated_classes is None:\n            tools = generate_pydantic_classes(module)\n\n        runnable = self.create_runnable(\n            conversation=conversation,\n            query_parameters=tools,\n        )\n\n        query = [\n            (\"system\", prompt),\n            (\"human\", question),\n        ]\n\n        return runnable.invoke(query)\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.generic_agent.GenericQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for the query builder.</p> <pre><code>query_parameters: The list of Pydantic classes to be used for the\n    query.\n\nconversation: The conversation object used for parameterising the\n    query.\n</code></pre> <pre><code>The runnable object for the query builder.\n</code></pre> Source code in <code>biochatter/api_agent/python/generic_agent.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: list[\"BaseAPIModel\"],\n    conversation: Conversation,\n) -&gt; Callable:\n    \"\"\"Create a runnable object for the query builder.\n\n    Args:\n    ----\n        query_parameters: The list of Pydantic classes to be used for the\n            query.\n\n        conversation: The conversation object used for parameterising the\n            query.\n\n    Returns:\n    -------\n        The runnable object for the query builder.\n\n    \"\"\"\n    runnable = conversation.chat.bind_tools(query_parameters, tool_choice=\"required\")\n    return runnable | PydanticToolsParser(tools=query_parameters)\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.generic_agent.GenericQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, prompt, conversation, module, generated_classes=None)</code>","text":"<p>Parameterise tool calls for any Python module.</p> <p>Generate a list of parameterised BaseModel instances based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields.</p> <p>Using langchain's <code>bind_tools</code> method to allow the LLM to parameterise the function call, based on the functions available in the module.</p> <p>Relies on defined structure and annotation of the passed module.</p> <pre><code>question (str): The question to be answered.\n\nprompt (str): The prompt to be used for the query, instructing the\n    LLM of its task and the module context.\n\nconversation: The conversation object used for parameterising the\n    query.\n\nmodule: The Python module to be used for the query.\n\ngenerated_classes: The list of Pydantic classes to be used for the\n    query. If not provided, the classes will be generated from the\n    module. Allows for external injection of classes for testing\n    purposes.\n</code></pre> <pre><code>list[BaseAPIModel]: the parameterised query object (Pydantic\n    model)\n</code></pre> Source code in <code>biochatter/api_agent/python/generic_agent.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    prompt: str,\n    conversation: \"Conversation\",\n    module: ModuleType,\n    generated_classes: list[BaseAPIModel] | None = None,\n) -&gt; list[BaseAPIModel]:\n    \"\"\"Parameterise tool calls for any Python module.\n\n    Generate a list of parameterised BaseModel instances based on the given\n    question, prompt, and BioChatter conversation. Uses a Pydantic model\n    to define the API fields.\n\n    Using langchain's `bind_tools` method to allow the LLM to parameterise\n    the function call, based on the functions available in the module.\n\n    Relies on defined structure and annotation of the passed module.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        prompt (str): The prompt to be used for the query, instructing the\n            LLM of its task and the module context.\n\n        conversation: The conversation object used for parameterising the\n            query.\n\n        module: The Python module to be used for the query.\n\n        generated_classes: The list of Pydantic classes to be used for the\n            query. If not provided, the classes will be generated from the\n            module. Allows for external injection of classes for testing\n            purposes.\n\n    Returns:\n    -------\n        list[BaseAPIModel]: the parameterised query object (Pydantic\n            model)\n\n    \"\"\"\n    if generated_classes is None:\n        tools = generate_pydantic_classes(module)\n\n    runnable = self.create_runnable(\n        conversation=conversation,\n        query_parameters=tools,\n    )\n\n    query = [\n        (\"system\", prompt),\n        (\"human\", question),\n    ]\n\n    return runnable.invoke(query)\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.autogenerate_model--examples","title":"Examples","text":"<p>import scanpy as sc generated_classes = generate_pydantic_classes(sc.tl) for model in generated_classes: ...     print(model.schema())</p>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.autogenerate_model.generate_pydantic_classes","title":"<code>generate_pydantic_classes(module)</code>","text":"<p>Generate Pydantic classes for each callable.</p> <p>For each callable (function/method) in a given module. Extracts parameters from docstrings using docstring-parser. Each generated class has fields corresponding to the parameters of the function. If a parameter name conflicts with BaseModel attributes, it is aliased.</p>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.autogenerate_model.generate_pydantic_classes--params","title":"Params:","text":"<p>module : ModuleType     The Python module from which to extract functions and generate models.</p>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.autogenerate_model.generate_pydantic_classes--returns","title":"Returns","text":"<p>list[Type[BaseModel]]     A list of Pydantic model classes corresponding to each function found in         <code>module</code>.</p>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.autogenerate_model.generate_pydantic_classes--notes","title":"Notes","text":"<ul> <li>For now, all parameter types are set to <code>Any</code> to avoid complications with   complex or external classes that are not easily JSON-serializable.</li> <li>Optional parameters (those with a None default) are represented as   <code>Optional[Any]</code>.</li> <li>Required parameters (no default) use <code>...</code> to indicate that the field is   required.</li> </ul> Source code in <code>biochatter/api_agent/python/autogenerate_model.py</code> <pre><code>def generate_pydantic_classes(module: ModuleType) -&gt; list[type[BaseAPIModel]]:\n    \"\"\"Generate Pydantic classes for each callable.\n\n    For each callable (function/method) in a given module. Extracts parameters\n    from docstrings using docstring-parser. Each generated class has fields\n    corresponding to the parameters of the function. If a parameter name\n    conflicts with BaseModel attributes, it is aliased.\n\n    Params:\n    -------\n    module : ModuleType\n        The Python module from which to extract functions and generate models.\n\n    Returns\n    -------\n    list[Type[BaseModel]]\n        A list of Pydantic model classes corresponding to each function found in\n            `module`.\n\n    Notes\n    -----\n    - For now, all parameter types are set to `Any` to avoid complications with\n      complex or external classes that are not easily JSON-serializable.\n    - Optional parameters (those with a None default) are represented as\n      `Optional[Any]`.\n    - Required parameters (no default) use `...` to indicate that the field is\n      required.\n\n    \"\"\"\n    base_attributes = set(dir(BaseAPIModel))\n    classes_list = []\n\n    for name, func in inspect.getmembers(module, inspect.isfunction):\n        # Skip private/internal functions (e.g., _something)\n        if name.startswith(\"_\"):\n            continue\n\n        # Parse docstring for parameter descriptions\n        doc = inspect.getdoc(func) or \"\"\n        parsed_doc = parse(doc)\n        doc_params = {p.arg_name: p.description or \"No description available.\" for p in parsed_doc.params}\n\n        sig = inspect.signature(func)\n        fields = {}\n\n        for param_name, param in sig.parameters.items():\n            # Skip *args and **kwargs for now\n            if param_name in (\"args\", \"kwargs\"):\n                continue\n\n            # Fetch docstring description or fallback\n            description = doc_params.get(param_name, \"No description available.\")\n\n            # Determine default value\n            # If no default, we use `...` indicating a required field\n            if param.default is not inspect.Parameter.empty:\n                default_value = param.default\n\n                # Convert MappingProxyType to a dict for JSON compatibility\n                if isinstance(default_value, MappingProxyType):\n                    default_value = dict(default_value)\n\n                # Handle non-JSON-compliant float values by converting to string\n                if default_value in [float(\"inf\"), float(\"-inf\"), float(\"nan\"), float(\"-nan\")]:\n                    default_value = str(default_value)\n            else:\n                default_value = ...  # No default means required\n\n            # For now, all parameter types are Any\n            annotation = Any\n\n            # Append the original annotation as a note in the description if\n            # available\n            if param.annotation is not inspect.Parameter.empty:\n                description += f\"\\nOriginal type annotation: {param.annotation}\"\n\n            # If default_value is None, parameter can be Optional\n            # If not required, mark as Optional[Any]\n            if default_value is None:\n                annotation = Any | None\n\n            # Prepare field kwargs\n            field_kwargs = {\"description\": description, \"default\": default_value}\n\n            # If field name conflicts with BaseModel attributes, alias it\n            field_name = param_name\n            if param_name in base_attributes:\n                alias_name = param_name + \"_param\"\n                field_kwargs[\"alias\"] = param_name\n                field_name = alias_name\n\n            fields[field_name] = (annotation, Field(**field_kwargs))\n\n        # Create the Pydantic model\n\n        tl_parameters_model = create_model(\n            name,\n            **fields,\n            __base__=BaseAPIModel,\n        )\n        classes_list.append(tl_parameters_model)\n    return classes_list\n</code></pre>"},{"location":"api-docs/api-calling-python/#scanpy-modules","title":"Scanpy modules","text":"<p>We manually define the API descriptions for select Scanpy modules.</p> <p>Module for generating anndata queries using LLM tools.</p> <p>Module for interacting with the <code>scanpy</code> API for plotting (<code>pl</code>).</p> <p>Module for interacting with the <code>scanpy</code> API for plotting (<code>pl</code>).</p> <p>Scanpy Preprocessing (scanpy.pp) Query Builder.</p> <p>TODO: not sure if the approach below is functional yet.</p>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.AnnDataIOQueryBuilder","title":"<code>AnnDataIOQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building a AnndataIO query object.</p> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class AnnDataIOQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building a AnndataIO query object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: list[\"BaseAPIModel\"],\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Create runnable using the LangChain `create_structured_output_runnable`\n        method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        runnable = conversation.chat.bind_tools(query_parameters, tool_choice=\"required\")\n        return runnable | PydanticToolsParser(tools=query_parameters)\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[\"BaseModel\"]:\n        \"\"\"Generate a AnnDataIOQuery object.\n\n        Generates the object based on the given question, prompt, and\n        BioChatter conversation. Uses a Pydantic model to define the API fields.\n        Creates a runnable that can be invoked on LLMs that are qualified to\n        parameterise functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                AnnDataIOQuery.\n\n        Returns:\n        -------\n            AnnDataIOQuery: the parameterised query object (Pydantic model)\n\n        \"\"\"\n        tools = [\n            ReadCSV,\n            ReadExcel,\n            ReadH5AD,\n            ReadHDF,\n            ReadLoom,\n            ReadMTX,\n            ReadText,\n            ReadZarr,\n            ConcatenateAnnData,\n            MapAnnData,\n        ]\n        runnable = self.create_runnable(\n            conversation=conversation,\n            query_parameters=tools,\n        )\n        query = [\n            (\"system\", ANNDATA_IO_QUERY_PROMPT),\n            (\"human\", f\"{question}\"),\n        ]\n        return runnable.invoke(\n            query,\n        )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.AnnDataIOQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Create runnable using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: list[\"BaseAPIModel\"],\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Create runnable using the LangChain `create_structured_output_runnable`\n    method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    runnable = conversation.chat.bind_tools(query_parameters, tool_choice=\"required\")\n    return runnable | PydanticToolsParser(tools=query_parameters)\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.AnnDataIOQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate a AnnDataIOQuery object.</p> <p>Generates the object based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields. Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    AnnDataIOQuery.\n</code></pre> <pre><code>AnnDataIOQuery: the parameterised query object (Pydantic model)\n</code></pre> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[\"BaseModel\"]:\n    \"\"\"Generate a AnnDataIOQuery object.\n\n    Generates the object based on the given question, prompt, and\n    BioChatter conversation. Uses a Pydantic model to define the API fields.\n    Creates a runnable that can be invoked on LLMs that are qualified to\n    parameterise functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            AnnDataIOQuery.\n\n    Returns:\n    -------\n        AnnDataIOQuery: the parameterised query object (Pydantic model)\n\n    \"\"\"\n    tools = [\n        ReadCSV,\n        ReadExcel,\n        ReadH5AD,\n        ReadHDF,\n        ReadLoom,\n        ReadMTX,\n        ReadText,\n        ReadZarr,\n        ConcatenateAnnData,\n        MapAnnData,\n    ]\n    runnable = self.create_runnable(\n        conversation=conversation,\n        query_parameters=tools,\n    )\n    query = [\n        (\"system\", ANNDATA_IO_QUERY_PROMPT),\n        (\"human\", f\"{question}\"),\n    ]\n    return runnable.invoke(\n        query,\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ConcatenateAnnData","title":"<code>ConcatenateAnnData</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Concatenate AnnData objects along an axis.</p> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ConcatenateAnnData(BaseAPIModel):\n    \"\"\"Concatenate AnnData objects along an axis.\"\"\"\n\n    method_name: str = Field(default=\"anndata.concat\", description=\"NEVER CHANGE\")\n    adatas: list | dict = Field(\n        ...,\n        description=(\n            \"The objects to be concatenated. \"\n            \"Either a list of AnnData objects or a mapping of keys to AnnData objects.\"\n        ),\n    )\n    axis: str = Field(\n        default=\"obs\",\n        description=\"Axis to concatenate along. Can be 'obs' (0) or 'var' (1). Default is 'obs'.\",\n    )\n    join: str = Field(\n        default=\"inner\",\n        description=\"How to align values when concatenating. Options: 'inner' or 'outer'. Default is 'inner'.\",\n    )\n    merge: str | Callable | None = Field(\n        default=None,\n        description=(\n            \"How to merge elements not aligned to the concatenated axis. \"\n            \"Strategies include 'same', 'unique', 'first', 'only', or a callable function.\"\n        ),\n    )\n    uns_merge: str | Callable | None = Field(\n        default=None,\n        description=\"How to merge the .uns elements. Uses the same strategies as 'merge'.\",\n    )\n    label: str | None = Field(\n        default=None,\n        description=\"Column in axis annotation (.obs or .var) to place batch information. Default is None.\",\n    )\n    keys: list | None = Field(\n        default=None,\n        description=(\n            \"Names for each object being concatenated. \"\n            \"Used for column values or appended to the index if 'index_unique' is not None. \"\n            \"Default is None.\"\n        ),\n    )\n    index_unique: str | None = Field(\n        default=None,\n        description=\"Delimiter for making the index unique. When None, original indices are kept.\",\n    )\n    fill_value: Any | None = Field(\n        default=None,\n        description=\"Value used to fill missing indices when join='outer'. Default behavior depends on array type.\",\n    )\n    pairwise: bool = Field(\n        default=False,\n        description=\"Include pairwise elements along the concatenated dimension. Default is False.\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.MapAnnData","title":"<code>MapAnnData</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Apply mapping functions to elements of AnnData.</p> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class MapAnnData(BaseAPIModel):\n    \"\"\"Apply mapping functions to elements of AnnData.\"\"\"\n\n    method_name: str = Field(\n        default=\"anndata.obs|var['annotation_name'].map\",\n        description=(\n            \"ALWAYS ALWAYS ALWAYS REPLACE THE anndata BY THE ONE GIVEN BY THE INPUT\"\n            \"Specifies the AnnData attribute and operation being performed. \"\n            \"For example, 'obs.map' applies a mapping function or dictionary to the specified column in `adata.obs`. \"\n            \"This must always include the AnnData component and the `.map` operation. \"\n            \"Adapt the component (e.g., 'obs', 'var', etc.) to the specific use case.\"\n        ),\n    )\n    dics: dict | None = Field(default=None, description=\"Dictionary to map over.\")\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadCSV","title":"<code>ReadCSV</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .csv file.</p> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadCSV(BaseAPIModel):\n    \"\"\"Read .csv file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_csv\", description=\"NEVER CHANGE\")\n    filename: str = Field(\n        default=\"placeholder.csv\",\n        description=\"Path to the .csv file\",\n    )\n    delimiter: str | None = Field(\n        None,\n        description=\"Delimiter used in the .csv file\",\n    )\n    first_column_names: bool | None = Field(\n        None,\n        description=\"Whether the first column contains names\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadExcel","title":"<code>ReadExcel</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .xlsx (Excel) file.</p> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadExcel(BaseAPIModel):\n    \"\"\"Read .xlsx (Excel) file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_excel\", description=\"NEVER CHANGE\")\n    filename: str = Field(\n        default=\"placeholder.xlsx\",\n        description=\"Path to the .xlsx file\",\n    )\n    sheet: str | None = Field(None, description=\"Sheet name or index to read from\")\n    dtype: str | None = Field(\n        None,\n        description=\"Data type for the resulting dataframe\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadH5AD","title":"<code>ReadH5AD</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .h5ad-formatted hdf5 file.</p> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadH5AD(BaseAPIModel):\n    \"\"\"Read .h5ad-formatted hdf5 file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_h5ad\", description=\"NEVER CHANGE\")\n    filename: str = Field(default=\"dummy.h5ad\", description=\"Path to the .h5ad file\")\n    backed: str | None = Field(\n        default=None,\n        description=\"Mode to access file: None, 'r' for read-only\",\n    )\n    as_sparse: str | None = Field(\n        default=None,\n        description=\"Convert to sparse format: 'csr', 'csc', or None\",\n    )\n    as_sparse_fmt: str | None = Field(\n        default=None,\n        description=\"Sparse format if converting, e.g., 'csr'\",\n    )\n    index_unique: str | None = Field(\n        default=None,\n        description=\"Make index unique by appending suffix if needed\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadHDF","title":"<code>ReadHDF</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .h5 (hdf5) file.</p> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadHDF(BaseAPIModel):\n    \"\"\"Read .h5 (hdf5) file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_hdf\", description=\"NEVER CHANGE\")\n    filename: str = Field(default=\"placeholder.h5\", description=\"Path to the .h5 file\")\n    key: str | None = Field(None, description=\"Group key within the .h5 file\")\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadLoom","title":"<code>ReadLoom</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .loom-formatted hdf5 file.</p> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadLoom(BaseAPIModel):\n    \"\"\"Read .loom-formatted hdf5 file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_loom\", description=\"NEVER CHANGE\")\n    filename: str = Field(\n        default=\"placeholder.loom\",\n        description=\"Path to the .loom file\",\n    )\n    sparse: bool | None = Field(None, description=\"Whether to read data as sparse\")\n    cleanup: bool | None = Field(None, description=\"Clean up invalid entries\")\n    X_name: str | None = Field(None, description=\"Name to use for X matrix\")\n    obs_names: str | None = Field(\n        None,\n        description=\"Column to use for observation names\",\n    )\n    var_names: str | None = Field(\n        None,\n        description=\"Column to use for variable names\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadMTX","title":"<code>ReadMTX</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .mtx file.</p> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadMTX(BaseAPIModel):\n    \"\"\"Read .mtx file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_mtx\", description=\"NEVER CHANGE\")\n    filename: str = Field(\n        default=\"placeholder.mtx\",\n        description=\"Path to the .mtx file\",\n    )\n    dtype: str | None = Field(None, description=\"Data type for the matrix\")\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadText","title":"<code>ReadText</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read .txt, .tab, .data (text) file.</p> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadText(BaseAPIModel):\n    \"\"\"Read .txt, .tab, .data (text) file.\"\"\"\n\n    method_name: str = Field(default=\"io.read_text\", description=\"NEVER CHANGE\")\n    filename: str = Field(\n        default=\"placeholder.txt\",\n        description=\"Path to the text file\",\n    )\n    delimiter: str | None = Field(None, description=\"Delimiter used in the file\")\n    first_column_names: bool | None = Field(\n        None,\n        description=\"Whether the first column contains names\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.anndata_agent.ReadZarr","title":"<code>ReadZarr</code>","text":"<p>               Bases: <code>BaseAPIModel</code></p> <p>Read from a hierarchical Zarr array store.</p> Source code in <code>biochatter/api_agent/python/anndata_agent.py</code> <pre><code>class ReadZarr(BaseAPIModel):\n    \"\"\"Read from a hierarchical Zarr array store.\"\"\"\n\n    method_name: str = Field(default=\"io.read_zarr\", description=\"NEVER CHANGE\")\n    filename: str = Field(\n        default=\"placeholder.zarr\",\n        description=\"Path or URL to the Zarr store\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlDrawGraphQueryParameters","title":"<code>ScanpyPlDrawGraphQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.draw_graph</code> API.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlDrawGraphQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.draw_graph` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.draw_graph\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    use_raw: bool | None = Field(\n        default=None,\n        description=\"Use `.raw` attribute of `adata` for coloring with gene expression.\",\n    )\n    sort_order: bool = Field(\n        default=True,\n        description=(\n            \"For continuous annotations used as color parameter, \"\n            \"plot data points with higher values on top of others.\"\n        ),\n    )\n    edges: bool = Field(\n        default=False,\n        description=\"Show edges.\",\n    )\n    edges_width: float = Field(\n        default=0.1,\n        description=\"Width of edges.\",\n    )\n    edges_color: str | list[float] | list[str] = Field(\n        default=\"grey\",\n        description=\"Color of edges.\",\n    )\n    neighbors_key: str | None = Field(\n        default=None,\n        description=\"Where to look for neighbors connectivities.\",\n    )\n    arrows: bool = Field(\n        default=False,\n        description=\"Show arrows (deprecated in favor of `scvelo.pl.velocity_embedding`).\",\n    )\n    arrows_kwds: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Arguments passed to `quiver()`.\",\n    )\n    groups: str | list[str] | None = Field(\n        default=None,\n        description=\"Restrict to a few categories in categorical observation annotation.\",\n    )\n    components: str | list[str] | None = Field(\n        default=None,\n        description=\"For instance, ['1,2', '2,3']. To plot all available components use components='all'.\",\n    )\n    projection: str = Field(\n        default=\"2d\",\n        description=\"Projection of plot.\",\n    )\n    legend_loc: str = Field(\n        default=\"right margin\",\n        description=\"Location of legend.\",\n    )\n    legend_fontsize: int | float | str | None = Field(\n        default=None,\n        description=\"Numeric size in pt or string describing the size.\",\n    )\n    legend_fontweight: int | str = Field(\n        default=\"bold\",\n        description=\"Legend font weight.\",\n    )\n    legend_fontoutline: int | None = Field(\n        default=None,\n        description=\"Line width of the legend font outline in pt.\",\n    )\n    colorbar_loc: str | None = Field(\n        default=\"right\",\n        description=\"Where to place the colorbar for continuous variables.\",\n    )\n    size: float | list[float] | None = Field(\n        default=None,\n        description=\"Point size. If None, is automatically computed as 120000 / n_cells.\",\n    )\n    color_map: str | Any | None = Field(\n        default=None,\n        description=\"Color map to use for continuous variables.\",\n    )\n    palette: str | list[str] | Any | None = Field(\n        default=None,\n        description=\"Colors to use for plotting categorical annotation groups.\",\n    )\n    na_color: str | tuple[float, ...] = Field(\n        default=\"lightgray\",\n        description=\"Color to use for null or masked values.\",\n    )\n    na_in_legend: bool = Field(\n        default=True,\n        description=\"If there are missing values, whether they get an entry in the legend.\",\n    )\n    frameon: bool | None = Field(\n        default=None,\n        description=\"Draw a frame around the scatter plot.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the lower limit of the color scale.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the upper limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the center of the color scale.\",\n    )\n    norm: Any | None = Field(\n        default=None,\n        description=\"Normalization for the colormap.\",\n    )\n    add_outline: bool = Field(\n        default=False,\n        description=\"Add a thin border around groups of dots.\",\n    )\n    outline_width: tuple[float, ...] = Field(\n        default=(0.3, 0.05),\n        description=\"Width of the outline as a fraction of the scatter dot size.\",\n    )\n    outline_color: tuple[str, ...] = Field(\n        default=(\"black\", \"white\"),\n        description=\"Colors for the outline: border color and gap color.\",\n    )\n    ncols: int = Field(\n        default=4,\n        description=\"Number of panels per row.\",\n    )\n    hspace: float = Field(\n        default=0.25,\n        description=\"Height of the space between multiple panels.\",\n    )\n    wspace: float | None = Field(\n        default=None,\n        description=\"Width of the space between multiple panels.\",\n    )\n    return_fig: bool | None = Field(\n        default=None,\n        description=\"Return the matplotlib figure.\",\n    )\n    show: bool | None = Field(\n        default=None,\n        description=\"Show the plot; do not return axis.\",\n    )\n    save: str | bool | None = Field(\n        default=None,\n        description=\"If `True` or a `str`, save the figure.\",\n    )\n    ax: Any | None = Field(\n        default=None,\n        description=\"A matplotlib axes object.\",\n    )\n    layout: str | None = Field(\n        default=None,\n        description=\"One of the `draw_graph()` layouts.\",\n    )\n    kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Additional arguments passed to `matplotlib.pyplot.scatter()`.\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlPcaQueryParameters","title":"<code>ScanpyPlPcaQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the scanpy <code>pl.pca</code> API.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlPcaQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the scanpy `pl.pca` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.pca\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    components: str | list[str] = Field(\n        default=\"1,2\",\n        description=\"For example, ['1,2', '2,3']. To plot all available components use 'all'.\",\n    )\n    projection: str = Field(\n        default=\"2d\",\n        description=\"Projection of plot.\",\n    )\n    legend_loc: str = Field(\n        default=\"right margin\",\n        description=\"Location of legend.\",\n    )\n    legend_fontsize: int | float | str | None = Field(\n        default=None,\n        description=\"Font size for legend.\",\n    )\n    legend_fontweight: int | str | None = Field(\n        default=None,\n        description=\"Font weight for legend.\",\n    )\n    color_map: str | None = Field(\n        default=None,\n        description=\"String denoting matplotlib color map.\",\n    )\n    palette: str | list[str] | dict | None = Field(\n        default=None,\n        description=\"Colors to use for plotting categorical annotation groups.\",\n    )\n    frameon: bool | None = Field(\n        default=None,\n        description=\"Draw a frame around the scatter plot.\",\n    )\n    size: int | float | None = Field(\n        default=None,\n        description=\"Point size. If `None`, is automatically computed as 120000 / n_cells.\",\n    )\n    show: bool | None = Field(\n        default=None,\n        description=\"Show the plot, do not return axis.\",\n    )\n    save: str | bool | None = Field(\n        default=None,\n        description=\"If `True` or a `str`, save the figure.\",\n    )\n    ax: str | None = Field(\n        default=None,\n        description=\"A matplotlib axes object.\",\n    )\n    return_fig: bool = Field(\n        default=False,\n        description=\"Return the matplotlib figure object.\",\n    )\n    marker: str | None = Field(\n        default=\".\",\n        description=\"Marker symbol.\",\n    )\n    annotate_var_explained: bool = Field(\n        default=False,\n        description=\"Annotate the percentage of explained variance.\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlQueryBuilder","title":"<code>ScanpyPlQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building a AnndataIO query object.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building a AnndataIO query object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: list[\"BaseAPIModel\"],\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Create runnable using the LangChain `create_structured_output_runnable`\n        method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        runnable = conversation.chat.bind_tools(query_parameters)\n        return runnable | PydanticToolsParser(tools=query_parameters)\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[\"BaseModel\"]:\n        \"\"\"Generate a AnnDataIOQuery object.\n\n        Generates the object based on the given question, prompt, and\n        BioChatter conversation. Uses a Pydantic model to define the API fields.\n        Creates a runnable that can be invoked on LLMs that are qualified to\n        parameterise functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                ScanpyPlQuery.\n\n        Returns:\n        -------\n            ScanpyPlQuery: the parameterised query object (Pydantic model)\n\n        \"\"\"\n        tools = [\n            ScanpyPlScatterQueryParameters,\n            ScanpyPlPcaQueryParameters,\n            ScanpyPlTsneQueryParameters,\n            ScanpyPlUmapQueryParameters,\n            ScanpyPlDrawGraphQueryParameters,\n            ScanpyPlSpatialQueryParameters,\n        ]\n        runnable = self.create_runnable(conversation=conversation, query_parameters=tools)\n        return runnable.invoke(question)\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Create runnable using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: list[\"BaseAPIModel\"],\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Create runnable using the LangChain `create_structured_output_runnable`\n    method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    runnable = conversation.chat.bind_tools(query_parameters)\n    return runnable | PydanticToolsParser(tools=query_parameters)\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate a AnnDataIOQuery object.</p> <p>Generates the object based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields. Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    ScanpyPlQuery.\n</code></pre> <pre><code>ScanpyPlQuery: the parameterised query object (Pydantic model)\n</code></pre> Source code in <code>biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[\"BaseModel\"]:\n    \"\"\"Generate a AnnDataIOQuery object.\n\n    Generates the object based on the given question, prompt, and\n    BioChatter conversation. Uses a Pydantic model to define the API fields.\n    Creates a runnable that can be invoked on LLMs that are qualified to\n    parameterise functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            ScanpyPlQuery.\n\n    Returns:\n    -------\n        ScanpyPlQuery: the parameterised query object (Pydantic model)\n\n    \"\"\"\n    tools = [\n        ScanpyPlScatterQueryParameters,\n        ScanpyPlPcaQueryParameters,\n        ScanpyPlTsneQueryParameters,\n        ScanpyPlUmapQueryParameters,\n        ScanpyPlDrawGraphQueryParameters,\n        ScanpyPlSpatialQueryParameters,\n    ]\n    runnable = self.create_runnable(conversation=conversation, query_parameters=tools)\n    return runnable.invoke(question)\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlScatterQueryParameters","title":"<code>ScanpyPlScatterQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the scanpy <code>pl.scatter</code> API.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlScatterQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the scanpy `pl.scatter` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.scatter\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(description=\"Annotated data matrix.\")\n    x: str | None = Field(default=None, description=\"x coordinate.\")\n    y: str | None = Field(default=None, description=\"y coordinate.\")\n    color: str | tuple[float, ...] | list[str | tuple[float, ...]] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes, or a hex color specification.\",\n    )\n    use_raw: bool | None = Field(\n        default=None,\n        description=\"Whether to use raw attribute of adata. Defaults to True if .raw is present.\",\n    )\n    layers: str | list[str] | None = Field(\n        default=None,\n        description=\"Layer(s) to use from adata's layers attribute.\",\n    )\n    basis: str | None = Field(\n        default=None,\n        description=\"String that denotes a plotting tool that computed coordinates (e.g., 'pca', 'tsne', 'umap').\",\n    )\n    sort_order: bool = Field(\n        default=True,\n        description=\"For continuous annotations used as color parameter, plot data points with higher values on top.\",\n    )\n    groups: str | list[str] | None = Field(\n        default=None,\n        description=\"Restrict to specific categories in categorical observation annotation.\",\n    )\n    projection: str = Field(\n        default=\"2d\",\n        description=\"Projection of plot ('2d' or '3d').\",\n    )\n    legend_loc: str | None = Field(\n        default=\"right margin\",\n        description=\"Location of legend ('none', 'right margin', 'on data', etc.).\",\n    )\n    size: int | float | None = Field(\n        default=None,\n        description=\"Point size. If None, automatically computed as 120000 / n_cells.\",\n    )\n    color_map: str | None = Field(\n        default=None,\n        description=\"Color map to use for continuous variables (e.g., 'magma', 'viridis').\",\n    )\n    show: bool | None = Field(\n        default=None,\n        description=\"Show the plot, do not return axis.\",\n    )\n    save: str | bool | None = Field(\n        default=None,\n        description=\"If True or a str, save the figure. String is appended to default filename.\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlSpatialQueryParameters","title":"<code>ScanpyPlSpatialQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.spatial</code> API.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlSpatialQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.spatial` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.spatial\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    use_raw: bool | None = Field(\n        default=None,\n        description=\"Use `.raw` attribute of `adata` for coloring with gene expression.\",\n    )\n    layer: str | None = Field(\n        default=None,\n        description=\"Name of the AnnData object layer to plot.\",\n    )\n    library_id: str | None = Field(\n        default=None,\n        description=\"Library ID for Visium data, e.g., key in `adata.uns['spatial']`.\",\n    )\n    img_key: str | None = Field(\n        default=None,\n        description=(\n            \"Key for image data, used to get `img` and `scale_factor` from \"\n            \"'images' and 'scalefactors' entries for this library.\"\n        ),\n    )\n    img: Any | None = Field(\n        default=None,\n        description=\"Image data to plot, overrides `img_key`.\",\n    )\n    scale_factor: float | None = Field(\n        default=None,\n        description=\"Scaling factor used to map from coordinate space to pixel space.\",\n    )\n    spot_size: float | None = Field(\n        default=None,\n        description=\"Diameter of spot (in coordinate space) for each point.\",\n    )\n    crop_coord: tuple[int, ...] | None = Field(\n        default=None,\n        description=\"Coordinates to use for cropping the image (left, right, top, bottom).\",\n    )\n    alpha_img: float = Field(\n        default=1.0,\n        description=\"Alpha value for image.\",\n    )\n    bw: bool = Field(\n        default=False,\n        description=\"Plot image data in grayscale.\",\n    )\n    sort_order: bool = Field(\n        default=True,\n        description=(\n            \"For continuous annotations used as color parameter, plot data points \"\n            \"with higher values on top of others.\"\n        ),\n    )\n    groups: str | list[str] | None = Field(\n        default=None,\n        description=\"Restrict to specific categories in categorical observation annotation.\",\n    )\n    components: str | list[str] | None = Field(\n        default=None,\n        description=\"For example, ['1,2', '2,3']. To plot all available components, use 'all'.\",\n    )\n    projection: str = Field(\n        default=\"2d\",\n        description=\"Projection of plot.\",\n    )\n    legend_loc: str = Field(\n        default=\"right margin\",\n        description=\"Location of legend.\",\n    )\n    legend_fontsize: int | float | str | None = Field(\n        default=None,\n        description=\"Numeric size in pt or string describing the size.\",\n    )\n    legend_fontweight: int | str = Field(\n        default=\"bold\",\n        description=\"Legend font weight.\",\n    )\n    legend_fontoutline: int | None = Field(\n        default=None,\n        description=\"Line width of the legend font outline in pt.\",\n    )\n    colorbar_loc: str | None = Field(\n        default=\"right\",\n        description=\"Where to place the colorbar for continuous variables.\",\n    )\n    size: float = Field(\n        default=1.0,\n        description=\"Point size. If None, automatically computed as 120000 / n_cells.\",\n    )\n    color_map: str | Any | None = Field(\n        default=None,\n        description=\"Color map to use for continuous variables.\",\n    )\n    palette: str | list[str] | Any | None = Field(\n        default=None,\n        description=\"Colors to use for plotting categorical annotation groups.\",\n    )\n    na_color: str | tuple[float, ...] | None = Field(\n        default=None,\n        description=\"Color to use for null or masked values.\",\n    )\n    na_in_legend: bool = Field(\n        default=True,\n        description=\"If there are missing values, whether they get an entry in the legend.\",\n    )\n    frameon: bool | None = Field(\n        default=None,\n        description=\"Draw a frame around the scatter plot.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the lower limit of the color scale.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the upper limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the center of the color scale.\",\n    )\n    norm: Any | None = Field(\n        default=None,\n        description=\"Normalization for the colormap.\",\n    )\n    add_outline: bool = Field(\n        default=False,\n        description=\"Add a thin border around groups of dots.\",\n    )\n    outline_width: tuple[float, ...] = Field(\n        default=(0.3, 0.05),\n        description=\"Width of the outline as a fraction of the scatter dot size.\",\n    )\n    outline_color: tuple[str, ...] = Field(\n        default=(\"black\", \"white\"),\n        description=\"Colors for the outline: border color and gap color.\",\n    )\n    ncols: int = Field(\n        default=4,\n        description=\"Number of panels per row.\",\n    )\n    hspace: float = Field(\n        default=0.25,\n        description=\"Height of the space between multiple panels.\",\n    )\n    wspace: float | None = Field(\n        default=None,\n        description=\"Width of the space between multiple panels.\",\n    )\n    return_fig: bool | None = Field(\n        default=None,\n        description=\"Return the matplotlib figure.\",\n    )\n    show: bool | None = Field(\n        default=None,\n        description=\"Show the plot; do not return axis.\",\n    )\n    save: str | bool | None = Field(\n        default=None,\n        description=\"If `True` or a `str`, save the figure.\",\n    )\n    ax: Any | None = Field(\n        default=None,\n        description=\"A matplotlib axes object.\",\n    )\n    kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Additional arguments passed to `matplotlib.pyplot.scatter()`.\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlTsneQueryParameters","title":"<code>ScanpyPlTsneQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.tsne</code> API.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlTsneQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.tsne` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.tsne\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    use_raw: bool | None = Field(\n        default=None,\n        description=\"Use `.raw` attribute of `adata` for coloring with gene expression.\",\n    )\n    sort_order: bool = Field(\n        default=True,\n        description=\"Plot data points with higher values on top for continuous annotations.\",\n    )\n    edges: bool = Field(\n        default=False,\n        description=\"Show edges.\",\n    )\n    edges_width: float = Field(\n        default=0.1,\n        description=\"Width of edges.\",\n    )\n    edges_color: str | list[float] | list[str] = Field(\n        default=\"grey\",\n        description=\"Color of edges.\",\n    )\n    neighbors_key: str | None = Field(\n        default=None,\n        description=\"Key for neighbors connectivities.\",\n    )\n    arrows: bool = Field(\n        default=False,\n        description=\"Show arrows (deprecated in favor of `scvelo.pl.velocity_embedding`).\",\n    )\n    arrows_kwds: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Arguments passed to `quiver()`.\",\n    )\n    groups: str | None = Field(\n        default=None,\n        description=\"Restrict to specific categories in categorical observation annotation.\",\n    )\n    components: str | list[str] | None = Field(\n        default=None,\n        description=\"Components to plot, e.g., ['1,2', '2,3']. Use 'all' to plot all available components.\",\n    )\n    projection: str = Field(\n        default=\"2d\",\n        description=\"Projection of plot ('2d' or '3d').\",\n    )\n    legend_loc: str = Field(\n        default=\"right margin\",\n        description=\"Location of legend.\",\n    )\n    legend_fontsize: int | float | str | None = Field(\n        default=None,\n        description=\"Font size for legend.\",\n    )\n    legend_fontweight: int | str = Field(\n        default=\"bold\",\n        description=\"Font weight for legend.\",\n    )\n    legend_fontoutline: int | None = Field(\n        default=None,\n        description=\"Line width of the legend font outline in pt.\",\n    )\n    size: float | list[float] | None = Field(\n        default=None,\n        description=\"Point size. If `None`, computed as 120000 / n_cells.\",\n    )\n    color_map: str | Any | None = Field(\n        default=None,\n        description=\"Color map for continuous variables.\",\n    )\n    palette: str | list[str] | Any | None = Field(\n        default=None,\n        description=\"Colors for plotting categorical annotation groups.\",\n    )\n    na_color: str | tuple[float, ...] = Field(\n        default=\"lightgray\",\n        description=\"Color for null or masked values.\",\n    )\n    na_in_legend: bool = Field(\n        default=True,\n        description=\"Include missing values in the legend.\",\n    )\n    frameon: bool | None = Field(\n        default=None,\n        description=\"Draw a frame around the scatter plot.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Lower limit of the color scale.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Upper limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Center of the color scale, useful for diverging colormaps.\",\n    )\n    norm: Any | None = Field(\n        default=None,\n        description=\"Normalization for the colormap.\",\n    )\n    add_outline: bool = Field(\n        default=False,\n        description=\"Add a thin border around groups of dots.\",\n    )\n    outline_width: tuple[float, ...] = Field(\n        default=(0.3, 0.05),\n        description=\"Width of the outline as a fraction of the scatter dot size.\",\n    )\n    outline_color: tuple[str, ...] = Field(\n        default=(\"black\", \"white\"),\n        description=\"Colors for the outline: border color and gap color.\",\n    )\n    ncols: int = Field(\n        default=4,\n        description=\"Number of panels per row.\",\n    )\n    hspace: float = Field(\n        default=0.25,\n        description=\"Height of the space between multiple panels.\",\n    )\n    wspace: float | None = Field(\n        default=None,\n        description=\"Width of the space between multiple panels.\",\n    )\n    return_fig: bool | None = Field(\n        default=None,\n        description=\"Return the matplotlib figure.\",\n    )\n    show: bool | None = Field(\n        default=None,\n        description=\"Show the plot; do not return axis.\",\n    )\n    save: str | bool | None = Field(\n        default=None,\n        description=\"If `True` or a `str`, save the figure.\",\n    )\n    ax: Any | None = Field(\n        default=None,\n        description=\"A matplotlib axes object.\",\n    )\n    kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Additional arguments passed to `matplotlib.pyplot.scatter()`.\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_full.ScanpyPlUmapQueryParameters","title":"<code>ScanpyPlUmapQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.umap</code> API.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_full.py</code> <pre><code>class ScanpyPlUmapQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.umap` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.umap\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    mask_obs: str | None = Field(\n        default=None,\n        description=\"Mask for observations.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    use_raw: bool | None = Field(\n        default=None,\n        description=\"Use `.raw` attribute of `adata` for coloring with gene expression.\",\n    )\n    sort_order: bool = Field(\n        default=True,\n        description=\"Plot data points with higher values on top for continuous annotations.\",\n    )\n    edges: bool = Field(\n        default=False,\n        description=\"Show edges.\",\n    )\n    edges_width: float = Field(\n        default=0.1,\n        description=\"Width of edges.\",\n    )\n    edges_color: str | list[float] | list[str] = Field(\n        default=\"grey\",\n        description=\"Color of edges.\",\n    )\n    neighbors_key: str | None = Field(\n        default=None,\n        description=\"Key for neighbors connectivities.\",\n    )\n    arrows: bool = Field(\n        default=False,\n        description=\"Show arrows (deprecated in favor of `scvelo.pl.velocity_embedding`).\",\n    )\n    arrows_kwds: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Arguments passed to `quiver()`.\",\n    )\n    groups: str | None = Field(\n        default=None,\n        description=\"Restrict to specific categories in categorical observation annotation.\",\n    )\n    components: str | list[str] | None = Field(\n        default=None,\n        description=\"Components to plot, e.g., ['1,2', '2,3']. Use 'all' to plot all available components.\",\n    )\n    dimensions: int | None = Field(\n        default=None,\n        description=\"Number of dimensions to plot.\",\n    )\n    layer: str | None = Field(\n        default=None,\n        description=\"Name of the AnnData object layer to plot.\",\n    )\n    projection: str = Field(\n        default=\"2d\",\n        description=\"Projection of plot ('2d' or '3d').\",\n    )\n    scale_factor: float | None = Field(\n        default=None,\n        description=\"Scale factor for the plot.\",\n    )\n    color_map: str | Any | None = Field(\n        default=None,\n        description=\"Color map for continuous variables.\",\n    )\n    cmap: str | Any | None = Field(\n        default=None,\n        description=\"Alias for `color_map`.\",\n    )\n    palette: str | list[str] | Any | None = Field(\n        default=None,\n        description=\"Colors for plotting categorical annotation groups.\",\n    )\n    na_color: str | tuple[float, ...] = Field(\n        default=\"lightgray\",\n        description=\"Color for null or masked values.\",\n    )\n    na_in_legend: bool = Field(\n        default=True,\n        description=\"Include missing values in the legend.\",\n    )\n    size: float | list[float] | None = Field(\n        default=None,\n        description=\"Point size. If `None`, computed as 120000 / n_cells.\",\n    )\n    frameon: bool | None = Field(\n        default=None,\n        description=\"Draw a frame around the scatter plot.\",\n    )\n    legend_fontsize: int | float | str | None = Field(\n        default=None,\n        description=\"Font size for legend.\",\n    )\n    legend_fontweight: int | str = Field(\n        default=\"bold\",\n        description=\"Font weight for legend.\",\n    )\n    legend_loc: str = Field(\n        default=\"right margin\",\n        description=\"Location of legend.\",\n    )\n    legend_fontoutline: int | None = Field(\n        default=None,\n        description=\"Line width of the legend font outline in pt.\",\n    )\n    colorbar_loc: str = Field(\n        default=\"right\",\n        description=\"Location of the colorbar.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Upper limit of the color scale.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Lower limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Center of the color scale, useful for diverging colormaps.\",\n    )\n    norm: Any | None = Field(\n        default=None,\n        description=\"Normalization for the colormap.\",\n    )\n    add_outline: bool = Field(\n        default=False,\n        description=\"Add a thin border around groups of dots.\",\n    )\n    outline_width: tuple[float, ...] = Field(\n        default=(0.3, 0.05),\n        description=\"Width of the outline as a fraction of the scatter dot size.\",\n    )\n    outline_color: tuple[str, ...] = Field(\n        default=(\"black\", \"white\"),\n        description=\"Colors for the outline: border color and gap color.\",\n    )\n    ncols: int = Field(\n        default=4,\n        description=\"Number of panels per row.\",\n    )\n    hspace: float = Field(\n        default=0.25,\n        description=\"Height of the space between multiple panels.\",\n    )\n    wspace: float | None = Field(\n        default=None,\n        description=\"Width of the space between multiple panels.\",\n    )\n    show: bool | None = Field(\n        default=None,\n        description=\"Show the plot; do not return axis.\",\n    )\n    save: str | bool | None = Field(\n        default=None,\n        description=\"If `True` or a `str`, save the figure.\",\n    )\n    ax: Any | None = Field(\n        default=None,\n        description=\"A matplotlib axes object.\",\n    )\n    return_fig: bool | None = Field(\n        default=None,\n        description=\"Return the matplotlib figure.\",\n    )\n    marker: str = Field(\n        default=\".\",\n        description=\"Marker symbol.\",\n    )\n    kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Additional arguments passed to `matplotlib.pyplot.scatter()`.\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlDrawGraphQueryParameters","title":"<code>ScanpyPlDrawGraphQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.draw_graph</code> API.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlDrawGraphQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.draw_graph` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.draw_graph\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    color_map: str | Any | None = Field(\n        default=None,\n        description=\"Color map to use for continuous variables.\",\n    )\n    palette: str | list[str] | Any | None = Field(\n        default=None,\n        description=\"Colors to use for plotting categorical annotation groups.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the lower limit of the color scale.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the upper limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the center of the color scale.\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlPcaQueryParameters","title":"<code>ScanpyPlPcaQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the scanpy <code>pl.pca</code> API.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlPcaQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the scanpy `pl.pca` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.pca\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    color_map: str | None = Field(\n        default=None,\n        description=\"String denoting matplotlib color map.\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlQueryBuilder","title":"<code>ScanpyPlQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building a AnndataIO query object.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building a AnndataIO query object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: list[\"BaseAPIModel\"],\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Create runnable using the LangChain `create_structured_output_runnable`\n        method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        runnable = conversation.chat.bind_tools(query_parameters, tool_choice=\"required\")\n        return runnable | PydanticToolsParser(tools=query_parameters)\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[\"BaseModel\"]:\n        \"\"\"Generate a AnnDataIOQuery object.\n\n        Generates the object based on the given question, prompt, and\n        BioChatter conversation. Uses a Pydantic model to define the API fields.\n        Creates a runnable that can be invoked on LLMs that are qualified to\n        parameterise functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                ScanpyPlQuery.\n\n        Returns:\n        -------\n            ScanpyPlQuery: the parameterised query object (Pydantic model)\n\n        \"\"\"\n        tools = [\n            ScanpyPlScatterQueryParameters,\n            ScanpyPlPcaQueryParameters,\n            ScanpyPlTsneQueryParameters,\n            ScanpyPlUmapQueryParameters,\n            ScanpyPlDrawGraphQueryParameters,\n            ScanpyPlSpatialQueryParameters,\n        ]\n        runnable = self.create_runnable(conversation=conversation, query_parameters=tools)\n        return runnable.invoke(question)\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Create runnable using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: list[\"BaseAPIModel\"],\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Create runnable using the LangChain `create_structured_output_runnable`\n    method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    runnable = conversation.chat.bind_tools(query_parameters, tool_choice=\"required\")\n    return runnable | PydanticToolsParser(tools=query_parameters)\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate a AnnDataIOQuery object.</p> <p>Generates the object based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields. Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    ScanpyPlQuery.\n</code></pre> <pre><code>ScanpyPlQuery: the parameterised query object (Pydantic model)\n</code></pre> Source code in <code>biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[\"BaseModel\"]:\n    \"\"\"Generate a AnnDataIOQuery object.\n\n    Generates the object based on the given question, prompt, and\n    BioChatter conversation. Uses a Pydantic model to define the API fields.\n    Creates a runnable that can be invoked on LLMs that are qualified to\n    parameterise functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            ScanpyPlQuery.\n\n    Returns:\n    -------\n        ScanpyPlQuery: the parameterised query object (Pydantic model)\n\n    \"\"\"\n    tools = [\n        ScanpyPlScatterQueryParameters,\n        ScanpyPlPcaQueryParameters,\n        ScanpyPlTsneQueryParameters,\n        ScanpyPlUmapQueryParameters,\n        ScanpyPlDrawGraphQueryParameters,\n        ScanpyPlSpatialQueryParameters,\n    ]\n    runnable = self.create_runnable(conversation=conversation, query_parameters=tools)\n    return runnable.invoke(question)\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlScatterQueryParameters","title":"<code>ScanpyPlScatterQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the scanpy <code>pl.scatter</code> API.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlScatterQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the scanpy `pl.scatter` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.scatter\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(description=\"Annotated data matrix.\")\n    x: str | None = Field(default=None, description=\"x coordinate.\")\n    y: str | None = Field(default=None, description=\"y coordinate.\")\n    color: str | tuple[float, ...] | list[str | tuple[float, ...]] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes, or a hex color specification.\",\n    )\n    use_raw: bool | None = Field(\n        default=None,\n        description=\"Whether to use raw attribute of adata. Defaults to True if .raw is present.\",\n    )\n    layers: str | list[str] | None = Field(\n        default=None,\n        description=\"Layer(s) to use from adata's layers attribute.\",\n    )\n    basis: str | None = Field(\n        default=None,\n        description=\"String that denotes a plotting tool that computed coordinates (e.g., 'pca', 'tsne', 'umap').\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlSpatialQueryParameters","title":"<code>ScanpyPlSpatialQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.spatial</code> API.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlSpatialQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.spatial` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.spatial\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    layer: str | None = Field(\n        default=None,\n        description=\"Name of the AnnData object layer to plot.\",\n    )\n    library_id: str | None = Field(\n        default=None,\n        description=\"Library ID for Visium data, e.g., key in `adata.uns['spatial']`.\",\n    )\n    img_key: str | None = Field(\n        default=None,\n        description=(\n            \"Key for image data, used to get `img` and `scale_factor` from \"\n            \"'images' and 'scalefactors' entries for this library.\"\n        ),\n    )\n    img: Any | None = Field(\n        default=None,\n        description=\"Image data to plot, overrides `img_key`.\",\n    )\n    scale_factor: float | None = Field(\n        default=None,\n        description=\"Scaling factor used to map from coordinate space to pixel space.\",\n    )\n    spot_size: float | None = Field(\n        default=None,\n        description=\"Diameter of spot (in coordinate space) for each point.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the lower limit of the color scale.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the upper limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"The value representing the center of the color scale.\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlTsneQueryParameters","title":"<code>ScanpyPlTsneQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.tsne</code> API.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlTsneQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.tsne` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.tsne\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    groups: str | None = Field(\n        default=None,\n        description=\"Restrict to specific categories in categorical observation annotation.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Lower limit of the color scale.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Upper limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Center of the color scale, useful for diverging colormaps.\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pl_reduced.ScanpyPlUmapQueryParameters","title":"<code>ScanpyPlUmapQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the Scanpy <code>pl.umap</code> API.</p> Source code in <code>biochatter/api_agent/python/scanpy_pl_reduced.py</code> <pre><code>class ScanpyPlUmapQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the Scanpy `pl.umap` API.\"\"\"\n\n    method_name: str = Field(\n        default=\"sc.pl.umap\",\n        description=\"The name of the method to call.\",\n    )\n    question_uuid: str | None = Field(\n        default=None,\n        description=\"Unique identifier for the question.\",\n    )\n    adata: str = Field(\n        ...,\n        description=\"Annotated data matrix.\",\n    )\n    color: str | list[str] | None = Field(\n        default=None,\n        description=\"Keys for annotations of observations/cells or variables/genes.\",\n    )\n    gene_symbols: str | None = Field(\n        default=None,\n        description=\"Column name in `.var` DataFrame that stores gene symbols.\",\n    )\n    layer: str | None = Field(\n        default=None,\n        description=\"Name of the AnnData object layer to plot.\",\n    )\n    vmax: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Upper limit of the color scale.\",\n    )\n    vmin: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Lower limit of the color scale.\",\n    )\n    vcenter: str | float | Any | list[str | float | Any] | None = Field(\n        default=None,\n        description=\"Center of the color scale, useful for diverging colormaps.\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pp_reduced.ScanpyPpFuncs","title":"<code>ScanpyPpFuncs</code>","text":"<p>               Bases: <code>BaseTools</code></p> <p>Scanpy Preprocessing (scanpy.pp) Query Builder.</p> Source code in <code>biochatter/api_agent/python/scanpy_pp_reduced.py</code> <pre><code>class ScanpyPpFuncs(BaseTools):\n    \"\"\"Scanpy Preprocessing (scanpy.pp) Query Builder.\"\"\"\n\n    tools_params = {}\n\n    tools_params[\"filter_cells\"] = {\n        \"data\": (str, Field(..., description=\"The (annotated) data matrix.\")),\n        \"min_counts\": (Optional[int], Field(None, description=\"Minimum counts per cell.\")),\n        \"min_genes\": (Optional[int], Field(None, description=\"Minimum genes expressed in a cell.\")),\n        \"max_counts\": (Optional[int], Field(None, description=\"Maximum counts per cell.\")),\n        \"max_genes\": (Optional[int], Field(None, description=\"Maximum genes expressed in a cell.\")),\n        \"inplace\": (bool, Field(True, description=\"Whether to modify the data in place.\")),\n    }\n\n    tools_params[\"filter_genes\"] = {\n        \"data\": (str, Field(..., description=\"The (annotated) data matrix.\")),\n        \"min_counts\": (Optional[int], Field(None, description=\"Minimum counts per gene.\")),\n        \"min_cells\": (Optional[int], Field(None, description=\"Minimum number of cells expressing the gene.\")),\n        \"max_counts\": (Optional[int], Field(None, description=\"Maximum counts per gene.\")),\n        \"max_cells\": (Optional[int], Field(None, description=\"Maximum number of cells expressing the gene.\")),\n        \"inplace\": (bool, Field(True, description=\"Whether to modify the data in place.\")),\n    }\n\n    tools_params[\"highly_variable_genes\"] = {\n        \"adata\": (str, Field(..., description=\"Annotated data matrix.\")),\n        \"n_top_genes\": (Optional[int], Field(None, description=\"Number of highly-variable genes to keep.\")),\n        \"min_mean\": (float, Field(0.0125, description=\"Minimum mean expression for highly-variable genes.\")),\n        \"max_mean\": (float, Field(3, description=\"Maximum mean expression for highly-variable genes.\")),\n        \"flavor\": (str, Field(\"seurat\", description=\"Method for identifying highly-variable genes.\")),\n        \"inplace\": (bool, Field(True, description=\"Whether to place metrics in .var or return them.\")),\n    }\n\n    tools_params[\"log1p\"] = {\n        \"data\": (str, Field(..., description=\"The data matrix.\")),\n        \"base\": (Optional[float], Field(None, description=\"Base of the logarithm.\")),\n        \"copy\": (bool, Field(False, description=\"If True, return a copy_param of the data.\")),\n        \"chunked\": (Optional[bool], Field(None, description=\"Process data in chunks.\")),\n    }\n\n    tools_params[\"pca\"] = {\n        \"data\": (str, Field(..., description=\"The (annotated) data matrix.\")),\n        \"n_comps\": (Optional[int], Field(None, description=\"Number of principal components to compute.\")),\n        \"layer\": (Optional[str], Field(None, description=\"Element of layers to use for PCA.\")),\n        \"zero_center\": (bool, Field(True, description=\"Whether to zero-center the data.\")),\n        \"svd_solver\": (Optional[str], Field(None, description=\"SVD solver to use.\")),\n        \"copy\": (bool, Field(False, description=\"If True, return a copy_param of the data.\")),\n    }\n\n    tools_params[\"normalize_total\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"target_sum\": (Optional[float], Field(None, description=\"Target sum after normalization.\")),\n        \"exclude_highly_expressed\": (bool, Field(False, description=\"Whether to exclude highly expressed genes.\")),\n        \"inplace\": (bool, Field(True, description=\"Whether to update adata or return normalized data.\")),\n    }\n\n    tools_params[\"regress_out\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"keys\": (Union[str, Collection[str]], Field(..., description=\"Keys for regression.\")),\n        \"copy\": (bool, Field(False, description=\"If True, return a copy_param of the data.\")),\n    }\n\n    tools_params[\"scale\"] = {\n        \"data\": (str, Field(..., description=\"The data matrix.\")),\n        \"zero_center\": (bool, Field(True, description=\"Whether to zero-center the data.\")),\n        \"copy\": (bool, Field(False, description=\"Whether to perform operation inplace.\")),\n    }\n\n    tools_params[\"subsample\"] = {\n        \"data\": (str, Field(..., description=\"The data matrix.\")),\n        \"fraction\": (Optional[float], Field(None, description=\"Fraction of observations to subsample.\")),\n        \"n_obs\": (Optional[int], Field(None, description=\"Number of observations to subsample.\")),\n        \"copy\": (bool, Field(False, description=\"If True, return a copy_param of the data.\")),\n    }\n\n    tools_params[\"downsample_counts\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"counts_per_cell\": (Optional[int | str], Field(None, description=\"Target total counts per cell.\")),\n        \"replace\": (bool, Field(False, description=\"Whether to sample with replacement.\")),\n        \"copy\": (bool, Field(False, description=\"If True, return a copy_param of the data.\")),\n    }\n\n    tools_params[\"combat\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"key\": (str, Field(\"batch\", description=\"Key for batch effect removal.\")),\n        \"inplace\": (bool, Field(True, description=\"Whether to replace the data inplace.\")),\n    }\n\n    tools_params[\"scrublet\"] = {\n        \"adata\": (str, Field(..., description=\"Annotated data matrix.\")),\n        \"sim_doublet_ratio\": (float, Field(2.0, description=\"Number of doublets to simulate.\")),\n        \"threshold\": (Optional[float], Field(None, description=\"Doublet score threshold.\")),\n        \"copy\": (bool, Field(False, description=\"If True, return a copy_param of the data.\")),\n    }\n\n    tools_params[\"scrublet_simulate_doublets\"] = {\n        \"adata\": (str, Field(..., description=\"Annotated data matrix.\")),\n        \"sim_doublet_ratio\": (float, Field(2.0, description=\"Number of doublets to simulate.\")),\n        \"random_seed\": (int, Field(0, description=\"Random seed for reproducibility.\")),\n    }\n    tools_params[\"calculate_qc_metrics\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"expr_type\": (str, Field(\"counts\", description=\"Name of kind of values in X.\")),\n        \"var_type\": (str, Field(\"genes\", description=\"The kind of thing the variables are.\")),\n        \"qc_vars\": (\n            Collection[str],\n            Field(\n                (),\n                description=\"Keys for boolean columns of .var which identify variables you could want to control for (e.g., \u201cERCC\u201d or \u201cmito\u201d).\",\n            ),\n        ),\n        \"percent_top\": (\n            Collection[int],\n            Field(\n                (50, 100, 200, 500),\n                description=\"List of ranks at which cumulative proportion of expression will be reported as a percentage.\",\n            ),\n        ),\n        \"layer\": (\n            Optional[str],\n            Field(None, description=\"If provided, use adata.layers[layer] for expression values instead of adata.X.\"),\n        ),\n        \"use_raw\": (\n            bool,\n            Field(False, description=\"If True, use adata.raw.X for expression values instead of adata.X.\"),\n        ),\n        \"inplace\": (bool, Field(False, description=\"Whether to place calculated metrics in adata\u2019s .obs and .var.\")),\n        \"log1p\": (bool, Field(True, description=\"Set to False to skip computing log1p transformed annotations.\")),\n    }\n\n    tools_params[\"recipe_zheng17\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"n_top_genes\": (int, Field(1000, description=\"Number of genes to keep.\")),\n        \"log\": (bool, Field(True, description=\"Take logarithm of the data.\")),\n        \"plot\": (bool, Field(False, description=\"Show a plot of the gene dispersion vs. mean relation.\")),\n        \"copy\": (bool, Field(False, description=\"Return a copy of adata instead of updating it.\")),\n    }\n\n    tools_params[\"recipe_weinreb17\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"log\": (bool, Field(True, description=\"Logarithmize the data?\")),\n        \"mean_threshold\": (float, Field(0.01, description=\"Mean expression threshold for gene selection.\")),\n        \"cv_threshold\": (float, Field(2, description=\"Coefficient of variation threshold for gene selection.\")),\n        \"n_pcs\": (int, Field(50, description=\"Number of principal components to compute.\")),\n        \"svd_solver\": (str, Field(\"randomized\", description=\"SVD solver to use for PCA.\")),\n        \"random_state\": (int, Field(0, description=\"Random seed for reproducibility.\")),\n        \"copy\": (\n            bool,\n            Field(False, description=\"Return a copy if true, otherwise modifies the original adata object.\"),\n        ),\n    }\n\n    tools_params[\"recipe_seurat\"] = {\n        \"adata\": (str, Field(..., description=\"The annotated data matrix.\")),\n        \"log\": (bool, Field(True, description=\"Logarithmize the data?\")),\n        \"plot\": (bool, Field(False, description=\"Show a plot of the gene dispersion vs. mean relation.\")),\n        \"copy\": (\n            bool,\n            Field(False, description=\"Return a copy if true, otherwise modifies the original adata object.\"),\n        ),\n    }\n\n    def __init__(self, tools_params: dict = tools_params) -&gt; None:\n        \"\"\"Initialise the ScanpyPpFuncs class.\"\"\"\n        super().__init__()\n        self.tools_params = tools_params\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pp_reduced.ScanpyPpFuncs.__init__","title":"<code>__init__(tools_params=tools_params)</code>","text":"<p>Initialise the ScanpyPpFuncs class.</p> Source code in <code>biochatter/api_agent/python/scanpy_pp_reduced.py</code> <pre><code>def __init__(self, tools_params: dict = tools_params) -&gt; None:\n    \"\"\"Initialise the ScanpyPpFuncs class.\"\"\"\n    super().__init__()\n    self.tools_params = tools_params\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pp_reduced.ScanpyPpQueryBuilder","title":"<code>ScanpyPpQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building a ScanpyPp query object.</p> Source code in <code>biochatter/api_agent/python/scanpy_pp_reduced.py</code> <pre><code>class ScanpyPpQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building a ScanpyPp query object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: list[\"BaseAPIModel\"],\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Create runnable using the LangChain `create_structured_output_runnable`\n        method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        runnable = conversation.chat.bind_tools(query_parameters)\n        return runnable | PydanticToolsParser(tools=query_parameters)\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[\"BaseModel\"]:\n        \"\"\"Generate a ScanpyPp query object.\n\n        Generates the object based on the given question, prompt, and\n        BioChatter conversation. Uses a Pydantic model to define the API fields.\n        Creates a runnable that can be invoked on LLMs that are qualified to\n        parameterise functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                ScanpyPpQuery.\n\n        Returns:\n        -------\n            ScanpyPpQuery: the parameterised query object (Pydantic model)\n\n        \"\"\"\n        tool_maker = ScanpyPpFuncs()\n        tools = tool_maker.make_pydantic_tools()\n        runnable = self.create_runnable(conversation=conversation, query_parameters=tools)\n        return runnable.invoke(\n            question,\n        )\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pp_reduced.ScanpyPpQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Create runnable using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/api_agent/python/scanpy_pp_reduced.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: list[\"BaseAPIModel\"],\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Create runnable using the LangChain `create_structured_output_runnable`\n    method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    runnable = conversation.chat.bind_tools(query_parameters)\n    return runnable | PydanticToolsParser(tools=query_parameters)\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.python.scanpy_pp_reduced.ScanpyPpQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate a ScanpyPp query object.</p> <p>Generates the object based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields. Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    ScanpyPpQuery.\n</code></pre> <pre><code>ScanpyPpQuery: the parameterised query object (Pydantic model)\n</code></pre> Source code in <code>biochatter/api_agent/python/scanpy_pp_reduced.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[\"BaseModel\"]:\n    \"\"\"Generate a ScanpyPp query object.\n\n    Generates the object based on the given question, prompt, and\n    BioChatter conversation. Uses a Pydantic model to define the API fields.\n    Creates a runnable that can be invoked on LLMs that are qualified to\n    parameterise functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            ScanpyPpQuery.\n\n    Returns:\n    -------\n        ScanpyPpQuery: the parameterised query object (Pydantic model)\n\n    \"\"\"\n    tool_maker = ScanpyPpFuncs()\n    tools = tool_maker.make_pydantic_tools()\n    runnable = self.create_runnable(conversation=conversation, query_parameters=tools)\n    return runnable.invoke(\n        question,\n    )\n</code></pre>"},{"location":"api-docs/api-calling-python/#api-calling-utility-functions","title":"API Calling: Utility functions","text":""},{"location":"api-docs/api-calling-python/#formatters-to-parse-the-calls","title":"Formatters to parse the calls","text":"<p>Formatters for API calls (Pydantic models to strings).</p>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.base.formatters.format_as_python_call","title":"<code>format_as_python_call(model)</code>","text":"<p>Convert a parameter model into a Python method call string.</p> <pre><code>model: Pydantic model containing method parameters\n</code></pre> <pre><code>String representation of the Python method call\n</code></pre> Source code in <code>biochatter/api_agent/base/formatters.py</code> <pre><code>def format_as_python_call(model: BaseAPIModel) -&gt; str:\n    \"\"\"Convert a parameter model into a Python method call string.\n\n    Args:\n    ----\n        model: Pydantic model containing method parameters\n\n    Returns:\n    -------\n        String representation of the Python method call\n\n    \"\"\"\n    params = model.dict(exclude_none=True)\n    method_name = params.pop(\"method_name\", None)\n    params.pop(\"question_uuid\", None)\n    if isinstance(model, MapAnnData):\n        param_str = params.pop(\"dics\", {})\n    else:\n        param_str = \", \".join(f\"{k}={v!r}\" for k, v in params.items())\n\n    return f\"{method_name}({param_str})\"\n</code></pre>"},{"location":"api-docs/api-calling-python/#biochatter.api_agent.base.formatters.format_as_rest_call","title":"<code>format_as_rest_call(model)</code>","text":"<p>Convert a parameter model (BaseModel) into a REST API call string.</p> <pre><code>model: Pydantic model containing API call parameters\n</code></pre> <pre><code>String representation of the REST API call\n</code></pre> Source code in <code>biochatter/api_agent/base/formatters.py</code> <pre><code>def format_as_rest_call(model: BaseModel) -&gt; str:\n    \"\"\"Convert a parameter model (BaseModel) into a REST API call string.\n\n    Args:\n    ----\n        model: Pydantic model containing API call parameters\n\n    Returns:\n    -------\n        String representation of the REST API call\n\n    \"\"\"\n    params = model.dict(exclude_none=True)\n    endpoint = params.pop(\"endpoint\")\n    base_url = params.pop(\"base_url\")\n    params.pop(\"question_uuid\", None)\n\n    full_url = f\"{base_url.rstrip('/')}/{endpoint.strip('/')}\"\n    return f\"{full_url}?{urlencode(params)}\"\n</code></pre>"},{"location":"api-docs/api-calling-web/","title":"API Calling: Web APIs","text":""},{"location":"api-docs/api-calling-web/#blast","title":"BLAST","text":"<p>Module for handling BLAST API interactions.</p> <p>Provides functionality for building queries, fetching results, and interpreting BLAST (Basic Local Alignment Search Tool) sequence alignment data.</p>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastFetcher","title":"<code>BlastFetcher</code>","text":"<p>               Bases: <code>BaseFetcher</code></p> <p>A class for retrieving API results from BLAST.</p> <p>Retrieves results from BLAST given a parameterised BlastQuery.</p> <p>TODO add a limit of characters to be returned from the response.text?</p> Source code in <code>biochatter/api_agent/web/blast.py</code> <pre><code>class BlastFetcher(BaseFetcher):\n    \"\"\"A class for retrieving API results from BLAST.\n\n    Retrieves results from BLAST given a parameterised BlastQuery.\n\n    TODO add a limit of characters to be returned from the response.text?\n    \"\"\"\n\n    def _submit_query(self, request_data: BlastQueryParameters) -&gt; str:\n        \"\"\"POST the BLAST query and retrieve the RID.\n\n        The method submits the structured BlastQuery object and returns the RID.\n\n        Args:\n        ----\n            request_data: BlastQuery object containing the BLAST query\n                parameters.\n\n        Returns:\n        -------\n            str: The Request ID (RID) for the submitted BLAST query.\n\n        \"\"\"\n        data = {\n            \"CMD\": request_data.cmd,\n            \"PROGRAM\": request_data.program,\n            \"DATABASE\": request_data.database,\n            \"QUERY\": request_data.query,\n            \"FORMAT_TYPE\": request_data.format_type,\n            \"MEGABLAST\": request_data.megablast,\n            \"HITLIST_SIZE\": request_data.max_hits,\n        }\n        # Include any other_params if provided\n        if request_data.other_params:\n            data.update(request_data.other_params)\n        # Make the API call\n        query_string = urlencode(data)\n        # Combine base URL with the query string\n        full_url = f\"{request_data.url}?{query_string}\"\n        # Print the full URL\n        request_data.full_url = full_url\n        print(\"Full URL built by retriever:\\n\", request_data.full_url)\n        response = requests.post(request_data.url, data=data, timeout=10)\n        response.raise_for_status()\n        # Extract RID from response\n        print(response)\n        match = re.search(r\"RID = (\\w+)\", response.text)\n        if match:\n            return match.group(1)\n\n        msg = \"RID not found in BLAST submission response.\"\n        raise ValueError(msg)\n\n    def _fetch_results(\n        self,\n        rid: str,\n        question_uuid: str,\n        retries: int = 10000,\n    ) -&gt; str:\n        \"\"\"Fetch BLAST query data given RID.\n\n        The second function to be called for a BLAST query.\n        \"\"\"\n        base_url = \"https://blast.ncbi.nlm.nih.gov/Blast.cgi\"\n        check_status_params = {\n            \"CMD\": \"Get\",\n            \"FORMAT_OBJECT\": \"SearchInfo\",\n            \"RID\": rid,\n        }\n        get_results_params = {\n            \"CMD\": \"Get\",\n            \"FORMAT_TYPE\": \"XML\",\n            \"RID\": rid,\n        }\n\n        # Check the status of the BLAST job\n        for attempt in range(retries):\n            status_response = requests.get(base_url, params=check_status_params, timeout=10)\n            status_response.raise_for_status()\n            status_text = status_response.text\n            print(\"evaluating status\")\n            if \"Status=WAITING\" in status_text:\n                print(f\"{question_uuid} results not ready, waiting...\")\n                time.sleep(15)\n            elif \"Status=FAILED\" in status_text:\n                msg = \"BLAST query FAILED.\"\n                raise RuntimeError(msg)\n            elif \"Status=UNKNOWN\" in status_text:\n                msg = \"BLAST query expired or does not exist.\"\n                raise RuntimeError(msg)\n            elif \"Status=READY\" in status_text:\n                if \"ThereAreHits=yes\" in status_text:\n                    print(f\"{question_uuid} results are ready, retrieving.\")\n                    results_response = requests.get(\n                        base_url,\n                        params=get_results_params,\n                        timeout=10,\n                    )\n                    results_response.raise_for_status()\n                    return results_response.text\n                return \"No hits found\"\n            if attempt == retries - 1:\n                msg = \"Maximum attempts reached. Results may not be ready.\"\n                raise TimeoutError(msg)\n        return None\n\n    def fetch_results(\n        self,\n        query_models: list[BlastQueryParameters],\n        retries: int = 20,\n    ) -&gt; str:\n        \"\"\"Submit request and fetch results from BLAST API.\n\n        Wraps individual submission and retrieval of results.\n\n        Args:\n        ----\n            query_models: list of Pydantic models of the queries\n            retries: the number of maximum retries\n\n        Returns:\n        -------\n            str: the result from the BLAST API\n\n        \"\"\"\n        # For now, we only use the first query in the list\n        query = query_models[0]\n        rid = self._submit_query(request_data=query)\n        return self._fetch_results(\n            rid=rid,\n            question_uuid=query.question_uuid,\n            retries=retries,\n        )\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastFetcher.fetch_results","title":"<code>fetch_results(query_models, retries=20)</code>","text":"<p>Submit request and fetch results from BLAST API.</p> <p>Wraps individual submission and retrieval of results.</p> <pre><code>query_models: list of Pydantic models of the queries\nretries: the number of maximum retries\n</code></pre> <pre><code>str: the result from the BLAST API\n</code></pre> Source code in <code>biochatter/api_agent/web/blast.py</code> <pre><code>def fetch_results(\n    self,\n    query_models: list[BlastQueryParameters],\n    retries: int = 20,\n) -&gt; str:\n    \"\"\"Submit request and fetch results from BLAST API.\n\n    Wraps individual submission and retrieval of results.\n\n    Args:\n    ----\n        query_models: list of Pydantic models of the queries\n        retries: the number of maximum retries\n\n    Returns:\n    -------\n        str: the result from the BLAST API\n\n    \"\"\"\n    # For now, we only use the first query in the list\n    query = query_models[0]\n    rid = self._submit_query(request_data=query)\n    return self._fetch_results(\n        rid=rid,\n        question_uuid=query.question_uuid,\n        retries=retries,\n    )\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastInterpreter","title":"<code>BlastInterpreter</code>","text":"<p>               Bases: <code>BaseInterpreter</code></p> <p>A class for interpreting BLAST results.</p> Source code in <code>biochatter/api_agent/web/blast.py</code> <pre><code>class BlastInterpreter(BaseInterpreter):\n    \"\"\"A class for interpreting BLAST results.\"\"\"\n\n    def summarise_results(\n        self,\n        question: str,\n        conversation_factory: Callable,\n        response_text: str,\n    ) -&gt; str:\n        \"\"\"Extract the answer from the BLAST results.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n            conversation_factory: A BioChatter conversation object.\n            response_text (str): The response.text returned by NCBI.\n\n        Returns:\n        -------\n            str: The extracted answer from the BLAST results.\n\n        \"\"\"\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are a world class molecular biologist who knows everything about NCBI and BLAST results.\",\n                ),\n                (\"user\", \"{input}\"),\n            ],\n        )\n        summary_prompt = BLAST_SUMMARY_PROMPT.format(\n            question=question,\n            context=response_text,\n        )\n        output_parser = StrOutputParser()\n        conversation = conversation_factory()\n        chain = prompt | conversation.chat | output_parser\n        return chain.invoke({\"input\": {summary_prompt}})\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastInterpreter.summarise_results","title":"<code>summarise_results(question, conversation_factory, response_text)</code>","text":"<p>Extract the answer from the BLAST results.</p> <pre><code>question (str): The question to be answered.\nconversation_factory: A BioChatter conversation object.\nresponse_text (str): The response.text returned by NCBI.\n</code></pre> <pre><code>str: The extracted answer from the BLAST results.\n</code></pre> Source code in <code>biochatter/api_agent/web/blast.py</code> <pre><code>def summarise_results(\n    self,\n    question: str,\n    conversation_factory: Callable,\n    response_text: str,\n) -&gt; str:\n    \"\"\"Extract the answer from the BLAST results.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n        conversation_factory: A BioChatter conversation object.\n        response_text (str): The response.text returned by NCBI.\n\n    Returns:\n    -------\n        str: The extracted answer from the BLAST results.\n\n    \"\"\"\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"You are a world class molecular biologist who knows everything about NCBI and BLAST results.\",\n            ),\n            (\"user\", \"{input}\"),\n        ],\n    )\n    summary_prompt = BLAST_SUMMARY_PROMPT.format(\n        question=question,\n        context=response_text,\n    )\n    output_parser = StrOutputParser()\n    conversation = conversation_factory()\n    chain = prompt | conversation.chat | output_parser\n    return chain.invoke({\"input\": {summary_prompt}})\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastQueryBuilder","title":"<code>BlastQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building a BlastQuery object.</p> Source code in <code>biochatter/api_agent/web/blast.py</code> <pre><code>class BlastQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building a BlastQuery object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: \"BlastQueryParameters\",\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Creates a runnable using the LangChain\n        `create_structured_output_runnable` method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        return create_structured_output_runnable(\n            output_schema=query_parameters,\n            llm=conversation.chat,\n            prompt=self.structured_output_prompt,\n        )\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[BlastQueryParameters]:\n        \"\"\"Generate a BlastQuery object.\n\n        Generates the object based on the given question, prompt, and\n        BioChatter conversation. Uses a Pydantic model to define the API fields.\n        Creates a runnable that can be invoked on LLMs that are qualified to\n        parameterise functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                BlastQuery.\n\n        Returns:\n        -------\n            BlastQuery: the parameterised query object (Pydantic model)\n\n        \"\"\"\n        runnable = self.create_runnable(\n            query_parameters=BlastQueryParameters,\n            conversation=conversation,\n        )\n        blast_call_obj = runnable.invoke(\n            {\"input\": f\"Answer:\\n{question} based on:\\n {BLAST_QUERY_PROMPT}\"},\n        )\n        blast_call_obj.question_uuid = str(uuid.uuid4())\n        return [blast_call_obj]\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Creates a runnable using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/api_agent/web/blast.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: \"BlastQueryParameters\",\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Creates a runnable using the LangChain\n    `create_structured_output_runnable` method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    return create_structured_output_runnable(\n        output_schema=query_parameters,\n        llm=conversation.chat,\n        prompt=self.structured_output_prompt,\n    )\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate a BlastQuery object.</p> <p>Generates the object based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields. Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    BlastQuery.\n</code></pre> <pre><code>BlastQuery: the parameterised query object (Pydantic model)\n</code></pre> Source code in <code>biochatter/api_agent/web/blast.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[BlastQueryParameters]:\n    \"\"\"Generate a BlastQuery object.\n\n    Generates the object based on the given question, prompt, and\n    BioChatter conversation. Uses a Pydantic model to define the API fields.\n    Creates a runnable that can be invoked on LLMs that are qualified to\n    parameterise functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            BlastQuery.\n\n    Returns:\n    -------\n        BlastQuery: the parameterised query object (Pydantic model)\n\n    \"\"\"\n    runnable = self.create_runnable(\n        query_parameters=BlastQueryParameters,\n        conversation=conversation,\n    )\n    blast_call_obj = runnable.invoke(\n        {\"input\": f\"Answer:\\n{question} based on:\\n {BLAST_QUERY_PROMPT}\"},\n    )\n    blast_call_obj.question_uuid = str(uuid.uuid4())\n    return [blast_call_obj]\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.blast.BlastQueryParameters","title":"<code>BlastQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model for the parameters of a BLAST query request.</p> <p>The class is used for configuring and sending a request to the NCBI BLAST query API. The fields are dynamically configured by the LLM based on the user's question.</p> Source code in <code>biochatter/api_agent/web/blast.py</code> <pre><code>class BlastQueryParameters(BaseModel):\n    \"\"\"Pydantic model for the parameters of a BLAST query request.\n\n    The class is used for configuring and sending a request to the NCBI BLAST\n    query API. The fields are dynamically configured by the LLM based on the\n    user's question.\n\n    \"\"\"\n\n    url: str | None = Field(\n        default=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi?\",\n        description=\"ALWAYS USE DEFAULT, DO NOT CHANGE\",\n    )\n    cmd: str | None = Field(\n        default=\"Put\",\n        description=\"Command to execute, 'Put' for submitting query, 'Get' for retrieving results.\",\n    )\n    program: str | None = Field(\n        default=\"blastn\",\n        description=(\n            \"BLAST program to use, e.g., 'blastn' for nucleotide-nucleotide BLAST, \"\n            \"'blastp' for protein-protein BLAST.\"\n        ),\n    )\n    database: str | None = Field(\n        default=\"nt\",\n        description=(\n            \"Database to search, e.g., 'nt' for nucleotide database, 'nr' for \"\n            \"non redundant protein database, 'pdb' the Protein Data Bank \"\n            \"database, which is used specifically for protein structures, \"\n            \"'refseq_rna' and 'refseq_genomic': specialized databases for \"\n            \"RNA sequences and genomic sequences\"\n        ),\n    )\n    query: str | None = Field(\n        None,\n        description=(\n            \"Nucleotide or protein sequence for the BLAST or blat query, \"\n            \"make sure to always keep the entire sequence given.\"\n        ),\n    )\n    format_type: str | None = Field(\n        default=\"Text\",\n        description=\"Format of the BLAST results, e.g., 'Text', 'XML'.\",\n    )\n    rid: str | None = Field(\n        None,\n        description=\"Request ID for retrieving BLAST results.\",\n    )\n    other_params: dict | None = Field(\n        default={\"email\": \"user@example.com\"},\n        description=\"Other optional BLAST parameters, including user email.\",\n    )\n    max_hits: int | None = Field(\n        default=15,\n        description=\"Maximum number of hits to return in the BLAST results.\",\n    )\n    sort_by: str | None = Field(\n        default=\"score\",\n        description=\"Criterion to sort BLAST results by, e.g., 'score', 'evalue'.\",\n    )\n    megablast: str | None = Field(\n        default=\"on\",\n        description=\"Set to 'on' for human genome alignemnts\",\n    )\n    question_uuid: str | None = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"Unique identifier for the question.\",\n    )\n    full_url: str | None = Field(\n        default=\"TBF\",\n        description=\"Full URL to be used to submit the BLAST query\",\n    )\n</code></pre>"},{"location":"api-docs/api-calling-web/#oncokb","title":"OncoKB","text":"<p>OncoKB API agent.</p>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBFetcher","title":"<code>OncoKBFetcher</code>","text":"<p>               Bases: <code>BaseFetcher</code></p> <p>A class for retrieving API results.</p> <p>Retrieve from OncoKB given a parameterized OncoKBQuery.</p> Source code in <code>biochatter/api_agent/web/oncokb.py</code> <pre><code>class OncoKBFetcher(BaseFetcher):\n    \"\"\"A class for retrieving API results.\n\n    Retrieve from OncoKB given a parameterized OncoKBQuery.\n    \"\"\"\n\n    def __init__(self, api_token=\"demo\"):\n        self.headers = {\n            \"Authorization\": f\"Bearer {api_token}\",\n            \"Accept\": \"application/json\",\n        }\n        self.base_url = \"https://demo.oncokb.org/api/v1\"\n\n    def fetch_results(\n        self,\n        request_data: list[OncoKBQueryParameters],\n        retries: int | None = 3,\n    ) -&gt; str:\n        \"\"\"Submit the OncoKB query and fetch the results directly.\n\n        No multi-step procedure, thus no wrapping of submission and retrieval in\n        this case.\n\n        Args:\n        ----\n            request_data: List of OncoKBQuery objects (Pydantic models)\n                containing the OncoKB query parameters.\n\n            retries: The number of retries to fetch the results.\n\n        Returns:\n        -------\n            str: The results of the OncoKB query.\n\n        \"\"\"\n        # For now, we only use the first query in the list\n        query = request_data[0]\n\n        # Submit the query and get the URL\n        params = query.dict(exclude_unset=True)\n        endpoint = params.pop(\"endpoint\")\n        params.pop(\"question_uuid\")\n        full_url = f\"{self.base_url}/{endpoint}\"\n        response = requests.get(full_url, headers=self.headers, params=params)\n        response.raise_for_status()\n\n        # Fetch the results from the URL\n        results_response = requests.get(response.url, headers=self.headers)\n        results_response.raise_for_status()\n\n        return results_response.text\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBFetcher.fetch_results","title":"<code>fetch_results(request_data, retries=3)</code>","text":"<p>Submit the OncoKB query and fetch the results directly.</p> <p>No multi-step procedure, thus no wrapping of submission and retrieval in this case.</p> <pre><code>request_data: List of OncoKBQuery objects (Pydantic models)\n    containing the OncoKB query parameters.\n\nretries: The number of retries to fetch the results.\n</code></pre> <pre><code>str: The results of the OncoKB query.\n</code></pre> Source code in <code>biochatter/api_agent/web/oncokb.py</code> <pre><code>def fetch_results(\n    self,\n    request_data: list[OncoKBQueryParameters],\n    retries: int | None = 3,\n) -&gt; str:\n    \"\"\"Submit the OncoKB query and fetch the results directly.\n\n    No multi-step procedure, thus no wrapping of submission and retrieval in\n    this case.\n\n    Args:\n    ----\n        request_data: List of OncoKBQuery objects (Pydantic models)\n            containing the OncoKB query parameters.\n\n        retries: The number of retries to fetch the results.\n\n    Returns:\n    -------\n        str: The results of the OncoKB query.\n\n    \"\"\"\n    # For now, we only use the first query in the list\n    query = request_data[0]\n\n    # Submit the query and get the URL\n    params = query.dict(exclude_unset=True)\n    endpoint = params.pop(\"endpoint\")\n    params.pop(\"question_uuid\")\n    full_url = f\"{self.base_url}/{endpoint}\"\n    response = requests.get(full_url, headers=self.headers, params=params)\n    response.raise_for_status()\n\n    # Fetch the results from the URL\n    results_response = requests.get(response.url, headers=self.headers)\n    results_response.raise_for_status()\n\n    return results_response.text\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBInterpreter","title":"<code>OncoKBInterpreter</code>","text":"<p>               Bases: <code>BaseInterpreter</code></p> Source code in <code>biochatter/api_agent/web/oncokb.py</code> <pre><code>class OncoKBInterpreter(BaseInterpreter):\n    def summarise_results(\n        self,\n        question: str,\n        conversation_factory: Callable,\n        response_text: str,\n    ) -&gt; str:\n        \"\"\"Extract the answer from the BLAST results.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n            conversation_factory: A BioChatter conversation object.\n            response_text (str): The response.text returned by OncoKB.\n\n        Returns:\n        -------\n            str: The extracted answer from the BLAST results.\n\n        \"\"\"\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are a world class molecular biologist who knows \"\n                    \"everything about OncoKB and cancer genomics. Your task is \"\n                    \"to interpret results from OncoKB API calls and summarise \"\n                    \"them for the user.\",\n                ),\n                (\"user\", \"{input}\"),\n            ],\n        )\n        summary_prompt = ONCOKB_SUMMARY_PROMPT.format(\n            question=question,\n            context=response_text,\n        )\n        output_parser = StrOutputParser()\n        conversation = conversation_factory()\n        chain = prompt | conversation.chat | output_parser\n        answer = chain.invoke({\"input\": {summary_prompt}})\n        return answer\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBInterpreter.summarise_results","title":"<code>summarise_results(question, conversation_factory, response_text)</code>","text":"<p>Extract the answer from the BLAST results.</p> <pre><code>question (str): The question to be answered.\nconversation_factory: A BioChatter conversation object.\nresponse_text (str): The response.text returned by OncoKB.\n</code></pre> <pre><code>str: The extracted answer from the BLAST results.\n</code></pre> Source code in <code>biochatter/api_agent/web/oncokb.py</code> <pre><code>def summarise_results(\n    self,\n    question: str,\n    conversation_factory: Callable,\n    response_text: str,\n) -&gt; str:\n    \"\"\"Extract the answer from the BLAST results.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n        conversation_factory: A BioChatter conversation object.\n        response_text (str): The response.text returned by OncoKB.\n\n    Returns:\n    -------\n        str: The extracted answer from the BLAST results.\n\n    \"\"\"\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"You are a world class molecular biologist who knows \"\n                \"everything about OncoKB and cancer genomics. Your task is \"\n                \"to interpret results from OncoKB API calls and summarise \"\n                \"them for the user.\",\n            ),\n            (\"user\", \"{input}\"),\n        ],\n    )\n    summary_prompt = ONCOKB_SUMMARY_PROMPT.format(\n        question=question,\n        context=response_text,\n    )\n    output_parser = StrOutputParser()\n    conversation = conversation_factory()\n    chain = prompt | conversation.chat | output_parser\n    answer = chain.invoke({\"input\": {summary_prompt}})\n    return answer\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBQueryBuilder","title":"<code>OncoKBQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building an OncoKBQuery object.</p> Source code in <code>biochatter/api_agent/web/oncokb.py</code> <pre><code>class OncoKBQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building an OncoKBQuery object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: \"OncoKBQueryParameters\",\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Creates a runnable object for executing queries using the LangChain\n        `create_structured_output_runnable` method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        return create_structured_output_runnable(\n            output_schema=query_parameters,\n            llm=conversation.chat,\n            prompt=self.structured_output_prompt,\n        )\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[OncoKBQueryParameters]:\n        \"\"\"Generate an OncoKBQuery object.\n\n        Generate based on the given question, prompt, and BioChatter\n        conversation. Uses a Pydantic model to define the API fields. Creates a\n        runnable that can be invoked on LLMs that are qualified to parameterise\n        functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                OncoKBQuery.\n\n        Returns:\n        -------\n            OncoKBQueryParameters: the parameterised query object (Pydantic model)\n\n        \"\"\"\n        runnable = self.create_runnable(\n            query_parameters=OncoKBQueryParameters,\n            conversation=conversation,\n        )\n        oncokb_call_obj = runnable.invoke(\n            {\"input\": f\"Answer:\\n{question} based on:\\n {ONCOKB_QUERY_PROMPT}\"},\n        )\n        oncokb_call_obj.question_uuid = str(uuid.uuid4())\n        return [oncokb_call_obj]\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Creates a runnable object for executing queries using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/api_agent/web/oncokb.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: \"OncoKBQueryParameters\",\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Creates a runnable object for executing queries using the LangChain\n    `create_structured_output_runnable` method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    return create_structured_output_runnable(\n        output_schema=query_parameters,\n        llm=conversation.chat,\n        prompt=self.structured_output_prompt,\n    )\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.oncokb.OncoKBQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate an OncoKBQuery object.</p> <p>Generate based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields. Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    OncoKBQuery.\n</code></pre> <pre><code>OncoKBQueryParameters: the parameterised query object (Pydantic model)\n</code></pre> Source code in <code>biochatter/api_agent/web/oncokb.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[OncoKBQueryParameters]:\n    \"\"\"Generate an OncoKBQuery object.\n\n    Generate based on the given question, prompt, and BioChatter\n    conversation. Uses a Pydantic model to define the API fields. Creates a\n    runnable that can be invoked on LLMs that are qualified to parameterise\n    functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            OncoKBQuery.\n\n    Returns:\n    -------\n        OncoKBQueryParameters: the parameterised query object (Pydantic model)\n\n    \"\"\"\n    runnable = self.create_runnable(\n        query_parameters=OncoKBQueryParameters,\n        conversation=conversation,\n    )\n    oncokb_call_obj = runnable.invoke(\n        {\"input\": f\"Answer:\\n{question} based on:\\n {ONCOKB_QUERY_PROMPT}\"},\n    )\n    oncokb_call_obj.question_uuid = str(uuid.uuid4())\n    return [oncokb_call_obj]\n</code></pre>"},{"location":"api-docs/api-calling-web/#biotools","title":"bio.tools","text":"<p>Module for interacting with the bio.tools API.</p>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsFetcher","title":"<code>BioToolsFetcher</code>","text":"<p>               Bases: <code>BaseFetcher</code></p> <p>A class for retrieving API results from BioTools.</p> <p>Retrieves API results given a parameterized BioToolsQuery.</p> Source code in <code>biochatter/api_agent/web/bio_tools.py</code> <pre><code>class BioToolsFetcher(BaseFetcher):\n    \"\"\"A class for retrieving API results from BioTools.\n\n    Retrieves API results given a parameterized BioToolsQuery.\n    \"\"\"\n\n    def __init__(self, api_token: str = \"demo\") -&gt; None:  # noqa: S107\n        \"\"\"Initialise the BioToolsFetcher.\n\n        Args:\n        ----\n            api_token: The API token for the BioTools API.\n\n        \"\"\"\n        self.headers = {\n            \"Authorization\": f\"Bearer {api_token}\",\n            \"Accept\": \"application/json\",\n        }\n        self.base_url = \"https://bio.tools/api\"\n\n    def fetch_results(\n        self,\n        request_data: list[BioToolsQueryParameters],\n        retries: int | None = 3,  # noqa: ARG002\n    ) -&gt; str:\n        \"\"\"Submit the BioTools query and fetch the results directly.\n\n        No multi-step procedure, thus no wrapping of submission and retrieval in\n        this case.\n\n        Args:\n        ----\n            request_data: List of BioToolsQuery objects (Pydantic models)\n                containing the BioTools query parameters.\n\n            retries: The number of retries to fetch the results.\n\n        Returns:\n        -------\n            str: The results of the BioTools query.\n\n        \"\"\"\n        # For now, we only use the first query in the list\n        query = request_data[0]\n\n        # Submit the query and get the URL\n        params = query.dict(exclude_unset=True)\n        endpoint = params.pop(\"endpoint\")\n        params.pop(\"question_uuid\")\n        full_url = f\"{self.base_url}/{endpoint}\"\n        response = requests.get(full_url, headers=self.headers, params=params, timeout=30)\n        response.raise_for_status()\n\n        # Fetch the results from the URL\n        results_response = requests.get(response.url, headers=self.headers, timeout=30)\n        results_response.raise_for_status()\n\n        return results_response.text\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsFetcher.__init__","title":"<code>__init__(api_token='demo')</code>","text":"<p>Initialise the BioToolsFetcher.</p> <pre><code>api_token: The API token for the BioTools API.\n</code></pre> Source code in <code>biochatter/api_agent/web/bio_tools.py</code> <pre><code>def __init__(self, api_token: str = \"demo\") -&gt; None:  # noqa: S107\n    \"\"\"Initialise the BioToolsFetcher.\n\n    Args:\n    ----\n        api_token: The API token for the BioTools API.\n\n    \"\"\"\n    self.headers = {\n        \"Authorization\": f\"Bearer {api_token}\",\n        \"Accept\": \"application/json\",\n    }\n    self.base_url = \"https://bio.tools/api\"\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsFetcher.fetch_results","title":"<code>fetch_results(request_data, retries=3)</code>","text":"<p>Submit the BioTools query and fetch the results directly.</p> <p>No multi-step procedure, thus no wrapping of submission and retrieval in this case.</p> <pre><code>request_data: List of BioToolsQuery objects (Pydantic models)\n    containing the BioTools query parameters.\n\nretries: The number of retries to fetch the results.\n</code></pre> <pre><code>str: The results of the BioTools query.\n</code></pre> Source code in <code>biochatter/api_agent/web/bio_tools.py</code> <pre><code>def fetch_results(\n    self,\n    request_data: list[BioToolsQueryParameters],\n    retries: int | None = 3,  # noqa: ARG002\n) -&gt; str:\n    \"\"\"Submit the BioTools query and fetch the results directly.\n\n    No multi-step procedure, thus no wrapping of submission and retrieval in\n    this case.\n\n    Args:\n    ----\n        request_data: List of BioToolsQuery objects (Pydantic models)\n            containing the BioTools query parameters.\n\n        retries: The number of retries to fetch the results.\n\n    Returns:\n    -------\n        str: The results of the BioTools query.\n\n    \"\"\"\n    # For now, we only use the first query in the list\n    query = request_data[0]\n\n    # Submit the query and get the URL\n    params = query.dict(exclude_unset=True)\n    endpoint = params.pop(\"endpoint\")\n    params.pop(\"question_uuid\")\n    full_url = f\"{self.base_url}/{endpoint}\"\n    response = requests.get(full_url, headers=self.headers, params=params, timeout=30)\n    response.raise_for_status()\n\n    # Fetch the results from the URL\n    results_response = requests.get(response.url, headers=self.headers, timeout=30)\n    results_response.raise_for_status()\n\n    return results_response.text\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsInterpreter","title":"<code>BioToolsInterpreter</code>","text":"<p>               Bases: <code>BaseInterpreter</code></p> <p>A class for interpreting BioTools results.</p> Source code in <code>biochatter/api_agent/web/bio_tools.py</code> <pre><code>class BioToolsInterpreter(BaseInterpreter):\n    \"\"\"A class for interpreting BioTools results.\"\"\"\n\n    def summarise_results(\n        self,\n        question: str,\n        conversation_factory: Callable,\n        response_text: str,\n    ) -&gt; str:\n        \"\"\"Extract the answer from the BLAST results.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n            conversation_factory: A BioChatter conversation object.\n            response_text (str): The response.text returned by bio.tools.\n\n        Returns:\n        -------\n            str: The extracted answer from the BLAST results.\n\n        \"\"\"\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are a world class bioinformatician who knows \"\n                    \"everything about bio.tools packages and the \"\n                    \"bioinformatics ecosystem. Your task is to interpret \"\n                    \"results from BioTools API calls and summarise \"\n                    \"them for the user.\",\n                ),\n                (\"user\", \"{input}\"),\n            ],\n        )\n        summary_prompt = BIOTOOLS_SUMMARY_PROMPT.format(\n            question=question,\n            context=response_text,\n        )\n        output_parser = StrOutputParser()\n        conversation = conversation_factory()\n        chain = prompt | conversation.chat | output_parser\n        return chain.invoke({\"input\": {summary_prompt}})\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsInterpreter.summarise_results","title":"<code>summarise_results(question, conversation_factory, response_text)</code>","text":"<p>Extract the answer from the BLAST results.</p> <pre><code>question (str): The question to be answered.\nconversation_factory: A BioChatter conversation object.\nresponse_text (str): The response.text returned by bio.tools.\n</code></pre> <pre><code>str: The extracted answer from the BLAST results.\n</code></pre> Source code in <code>biochatter/api_agent/web/bio_tools.py</code> <pre><code>def summarise_results(\n    self,\n    question: str,\n    conversation_factory: Callable,\n    response_text: str,\n) -&gt; str:\n    \"\"\"Extract the answer from the BLAST results.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n        conversation_factory: A BioChatter conversation object.\n        response_text (str): The response.text returned by bio.tools.\n\n    Returns:\n    -------\n        str: The extracted answer from the BLAST results.\n\n    \"\"\"\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"You are a world class bioinformatician who knows \"\n                \"everything about bio.tools packages and the \"\n                \"bioinformatics ecosystem. Your task is to interpret \"\n                \"results from BioTools API calls and summarise \"\n                \"them for the user.\",\n            ),\n            (\"user\", \"{input}\"),\n        ],\n    )\n    summary_prompt = BIOTOOLS_SUMMARY_PROMPT.format(\n        question=question,\n        context=response_text,\n    )\n    output_parser = StrOutputParser()\n    conversation = conversation_factory()\n    chain = prompt | conversation.chat | output_parser\n    return chain.invoke({\"input\": {summary_prompt}})\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsQueryBuilder","title":"<code>BioToolsQueryBuilder</code>","text":"<p>               Bases: <code>BaseQueryBuilder</code></p> <p>A class for building an BioToolsQuery object.</p> Source code in <code>biochatter/api_agent/web/bio_tools.py</code> <pre><code>class BioToolsQueryBuilder(BaseQueryBuilder):\n    \"\"\"A class for building an BioToolsQuery object.\"\"\"\n\n    def create_runnable(\n        self,\n        query_parameters: \"BioToolsQueryParameters\",\n        conversation: \"Conversation\",\n    ) -&gt; Callable:\n        \"\"\"Create a runnable object for executing queries.\n\n        Create runnable using the LangChain `create_structured_output_runnable`\n        method.\n\n        Args:\n        ----\n            query_parameters: A Pydantic data model that specifies the fields of\n                the API that should be queried.\n\n            conversation: A BioChatter conversation object.\n\n        Returns:\n        -------\n            A Callable object that can execute the query.\n\n        \"\"\"\n        return create_structured_output_runnable(\n            output_schema=query_parameters,\n            llm=conversation.chat,\n            prompt=self.structured_output_prompt,\n        )\n\n    def parameterise_query(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; list[BioToolsQueryParameters]:\n        \"\"\"Generate an BioToolsQuery object.\n\n        Generate a BioToolsQuery object based on the given question, prompt,\n        and BioChatter conversation. Uses a Pydantic model to define the API\n        fields.  Creates a runnable that can be invoked on LLMs that are\n        qualified to parameterise functions.\n\n        Args:\n        ----\n            question (str): The question to be answered.\n\n            conversation: The conversation object used for parameterising the\n                BioToolsQuery.\n\n        Returns:\n        -------\n            BioToolsQueryParameters: the parameterised query object (Pydantic\n                model)\n\n        \"\"\"\n        runnable = self.create_runnable(\n            query_parameters=BioToolsQueryParameters,\n            conversation=conversation,\n        )\n        oncokb_call_obj = runnable.invoke(\n            {\n                \"input\": f\"Answer:\\n{question} based on:\\n {BIOTOOLS_QUERY_PROMPT}\",\n            },\n        )\n        oncokb_call_obj.question_uuid = str(uuid.uuid4())\n        return [oncokb_call_obj]\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsQueryBuilder.create_runnable","title":"<code>create_runnable(query_parameters, conversation)</code>","text":"<p>Create a runnable object for executing queries.</p> <p>Create runnable using the LangChain <code>create_structured_output_runnable</code> method.</p> <pre><code>query_parameters: A Pydantic data model that specifies the fields of\n    the API that should be queried.\n\nconversation: A BioChatter conversation object.\n</code></pre> <pre><code>A Callable object that can execute the query.\n</code></pre> Source code in <code>biochatter/api_agent/web/bio_tools.py</code> <pre><code>def create_runnable(\n    self,\n    query_parameters: \"BioToolsQueryParameters\",\n    conversation: \"Conversation\",\n) -&gt; Callable:\n    \"\"\"Create a runnable object for executing queries.\n\n    Create runnable using the LangChain `create_structured_output_runnable`\n    method.\n\n    Args:\n    ----\n        query_parameters: A Pydantic data model that specifies the fields of\n            the API that should be queried.\n\n        conversation: A BioChatter conversation object.\n\n    Returns:\n    -------\n        A Callable object that can execute the query.\n\n    \"\"\"\n    return create_structured_output_runnable(\n        output_schema=query_parameters,\n        llm=conversation.chat,\n        prompt=self.structured_output_prompt,\n    )\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsQueryBuilder.parameterise_query","title":"<code>parameterise_query(question, conversation)</code>","text":"<p>Generate an BioToolsQuery object.</p> <p>Generate a BioToolsQuery object based on the given question, prompt, and BioChatter conversation. Uses a Pydantic model to define the API fields.  Creates a runnable that can be invoked on LLMs that are qualified to parameterise functions.</p> <pre><code>question (str): The question to be answered.\n\nconversation: The conversation object used for parameterising the\n    BioToolsQuery.\n</code></pre> <pre><code>BioToolsQueryParameters: the parameterised query object (Pydantic\n    model)\n</code></pre> Source code in <code>biochatter/api_agent/web/bio_tools.py</code> <pre><code>def parameterise_query(\n    self,\n    question: str,\n    conversation: \"Conversation\",\n) -&gt; list[BioToolsQueryParameters]:\n    \"\"\"Generate an BioToolsQuery object.\n\n    Generate a BioToolsQuery object based on the given question, prompt,\n    and BioChatter conversation. Uses a Pydantic model to define the API\n    fields.  Creates a runnable that can be invoked on LLMs that are\n    qualified to parameterise functions.\n\n    Args:\n    ----\n        question (str): The question to be answered.\n\n        conversation: The conversation object used for parameterising the\n            BioToolsQuery.\n\n    Returns:\n    -------\n        BioToolsQueryParameters: the parameterised query object (Pydantic\n            model)\n\n    \"\"\"\n    runnable = self.create_runnable(\n        query_parameters=BioToolsQueryParameters,\n        conversation=conversation,\n    )\n    oncokb_call_obj = runnable.invoke(\n        {\n            \"input\": f\"Answer:\\n{question} based on:\\n {BIOTOOLS_QUERY_PROMPT}\",\n        },\n    )\n    oncokb_call_obj.question_uuid = str(uuid.uuid4())\n    return [oncokb_call_obj]\n</code></pre>"},{"location":"api-docs/api-calling-web/#biochatter.api_agent.web.bio_tools.BioToolsQueryParameters","title":"<code>BioToolsQueryParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for querying the bio.tools API.</p> Source code in <code>biochatter/api_agent/web/bio_tools.py</code> <pre><code>class BioToolsQueryParameters(BaseModel):\n    \"\"\"Parameters for querying the bio.tools API.\"\"\"\n\n    base_url: str = Field(\n        default=\"https://bio.tools/api/\",\n        description=\"Base URL for the BioTools API.\",\n    )\n    endpoint: str = Field(\n        ...,\n        description=\"Specific API endpoint to hit. Example: 't/' for listing tools.\",\n    )\n    biotoolsID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Search for bio.tools tool ID (usually quoted - to get exact match)\",\n    )\n    name: str | None = Field(\n        None,\n        description=\"Search for tool name (quoted as needed: quoted for exact match, unquoted for fuzzy search)\",\n    )\n    homepage: str | None = Field(\n        None,\n        description=\"Exact search for tool homepage URL (**must** be quoted)\",\n    )\n    description: str | None = Field(\n        None,\n        description=\"Search over tool description (quoted as needed)\",\n    )\n    version: str | None = Field(\n        None,\n        description=\"Exact search for tool version (**must** be quoted)\",\n    )\n    topic: str | None = Field(\n        None,\n        description=\"Search for EDAM Topic (term) (quoted as needed)\",\n    )\n    topicID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for EDAM Topic (URI): **must** be quoted\",\n    )\n    function: str | None = Field(\n        None,\n        description=\"Fuzzy search over function (input, operation, output, note and command)\",\n    )\n    operation: str | None = Field(\n        None,\n        description=\"Fuzzy search for EDAM Operation (term) (quoted as needed)\",\n    )\n    operationID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for EDAM Operation (ID) (**must** be quoted)\",\n    )\n    dataType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over input and output for EDAM Data (term) (quoted as needed)\",\n    )\n    dataTypeID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over input and output for EDAM Data (ID) (**must** be quoted)\",\n    )\n    dataFormat: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over input and output for EDAM Format (term) (quoted as needed)\",\n    )\n    dataFormatID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over input and output for EDAM Format (ID) (**must** be quoted)\",\n    )\n    input: str | None = Field(\n        None,\n        description=\"Fuzzy search over input for EDAM Data and Format (term) (quoted as needed)\",\n    )\n    inputID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over input for EDAM Data and Format (ID) (**must** be quoted)\",\n    )\n    inputDataType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over input for EDAM Data (term) (quoted as needed)\",\n    )\n    inputDataTypeID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over input for EDAM Data (ID) (**must** be quoted)\",\n    )\n    inputDataFormat: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over input for EDAM Format (term) (quoted as needed)\",\n    )\n    inputDataFormatID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over input for EDAM Format (ID) (**must** be quoted)\",\n    )\n    output: str | None = Field(\n        None,\n        description=\"Fuzzy search over output for EDAM Data and Format (term) (quoted as needed)\",\n    )\n    outputID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over output for EDAM Data and Format (ID) (**must** be quoted)\",\n    )\n    outputDataType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over output for EDAM Data (term) (quoted as needed)\",\n    )\n    outputDataTypeID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over output for EDAM Data (ID) (**must** be quoted)\",\n    )\n    outputDataFormat: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over output for EDAM Format (term) (quoted as needed)\",\n    )\n    outputDataFormatID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search over output for EDAM Format (ID) (**must** be quoted)\",\n    )\n    toolType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for tool type\",\n    )\n    collectionID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for tool collection (normally quoted)\",\n    )\n    maturity: str | None = Field(\n        None,\n        description=\"Exact search for tool maturity\",\n    )\n    operatingSystem: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for tool operating system\",\n    )\n    language: str | None = Field(\n        None,\n        description=\"Exact search for programming language\",\n    )\n    cost: str | None = Field(\n        None,\n        description=\"Exact search for cost\",\n    )\n    license: str | None = Field(\n        None,\n        description=\"Exact search for software or data usage license (quoted as needed)\",\n    )\n    accessibility: str | None = Field(\n        None,\n        description=\"Exact search for tool accessibility\",\n    )\n    credit: str | None = Field(\n        None,\n        description=\"Fuzzy search over credit (name, email, URL, ORCID iD, type of entity, type of role and note)\",\n    )\n    creditName: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for name of credited entity\",\n    )\n    creditTypeRole: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for role of credited entity\",\n    )\n    creditTypeEntity: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for type of credited entity\",\n    )\n    creditOrcidID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for ORCID iD of credited entity (**must** be quoted)\",\n    )\n    publication: str | None = Field(\n        None,\n        description=(\n            \"Fuzzy search over publication (DOI, PMID, PMCID, publication type and tool version) (quoted as needed)\"\n        ),\n    )\n    publicationID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for publication ID (DOI, PMID or PMCID) (**must** be quoted)\",\n    )\n    publicationType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for publication type\",\n    )\n    publicationVersion: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for tool version associated with a publication (**must** be quoted)\",\n    )\n    link: str | None = Field(\n        None,\n        description=\"Fuzzy search over general link (URL, type and note) (quote as needed)\",\n    )\n    linkType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for type of information found at a link\",\n    )\n    documentation: str | None = Field(\n        None,\n        description=\"Fuzzy search over documentation link (URL, type and note) (quote as needed)\",\n    )\n    documentationType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for type of documentation\",\n    )\n    download: str | None = Field(\n        None,\n        description=\"Fuzzy search over download link (URL, type, version and note) (quote as needed)\",\n    )\n    downloadType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for type of download\",\n    )\n    downloadVersion: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for tool version associated with a download (**must** be quoted)\",\n    )\n    otherID: str | None = Field(  # noqa: N815\n        None,\n        description=\"Fuzzy search over alternate tool IDs (ID value, type of ID and version)\",\n    )\n    otherIDValue: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for value of alternate tool ID (**must** be quoted)\",\n    )\n    otherIDType: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for type of alternate tool ID\",\n    )\n    otherIDVersion: str | None = Field(  # noqa: N815\n        None,\n        description=\"Exact search for tool version associated with an alternate ID (**must** be quoted)\",\n    )\n    question_uuid: str | None = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"Unique identifier for the question.\",\n    )\n</code></pre>"},{"location":"api-docs/kg/","title":"Knowledge Graph Agent Reference","text":"<p>Here we handle generation of use case-specific database prompts and their execution against a database using the database agent.</p>"},{"location":"api-docs/kg/#dynamic-prompt-generation-for-biocypher-knowledge-graphs","title":"Dynamic prompt generation for BioCypher knowledge graphs","text":""},{"location":"api-docs/kg/#biochatter.prompts.BioCypherPromptEngine","title":"<code>BioCypherPromptEngine</code>","text":"Source code in <code>biochatter/prompts.py</code> <pre><code>class BioCypherPromptEngine:\n    def __init__(\n        self,\n        schema_config_or_info_path: str | None = None,\n        schema_config_or_info_dict: dict | None = None,\n        model_provider: str = \"google_genai\",\n        model_name: str = \"gemini-2.0-flash\",\n        conversation_factory: Callable | None = None,\n    ) -&gt; None:\n        \"\"\"Given a biocypher schema configuration, extract the entities and\n        relationships, and for each extract their mode of representation (node\n        or edge), properties, and identifier namespace. Using these data, allow\n        the generation of prompts for a large language model, informing it of\n        the schema constituents and their properties, to enable the\n        parameterisation of function calls to a knowledge graph.\n\n        Args:\n        ----\n            schema_config_or_info_path: Path to a biocypher schema configuration\n                file or the extended schema information output generated by\n                BioCypher's `write_schema_info` function (preferred).\n\n            schema_config_or_info_dict: A dictionary containing the schema\n                configuration file or the extended schema information output\n                generated by BioCypher's `write_schema_info` function\n                (preferred).\n\n            model_provider: The provider of the model to use.\n\n            model_name: The name of the model to use for the conversation.\n                DEPRECATED: This should now be set in the conversation factory.\n\n            conversation_factory: A function used to create a conversation for\n                creating the KG query. If not provided, a default function is\n                used (creating an OpenAI conversation with the specified model,\n                see `_get_conversation`).\n\n        \"\"\"\n        if not schema_config_or_info_path and not schema_config_or_info_dict:\n            raise ValueError(\n                \"Please provide the schema configuration or schema info as a path to a file or as a dictionary.\",\n            )\n\n        if schema_config_or_info_path and schema_config_or_info_dict:\n            raise ValueError(\n                \"Please provide the schema configuration or schema info as a \"\n                \"path to a file or as a dictionary, not both.\",\n            )\n\n        # set conversation factory or use default\n        self.conversation_factory = conversation_factory if conversation_factory is not None else self._get_conversation\n\n        if schema_config_or_info_path:\n            # read the schema configuration\n            with open(schema_config_or_info_path) as f:\n                schema_config = yaml.safe_load(f)\n        elif schema_config_or_info_dict:\n            schema_config = schema_config_or_info_dict\n\n        # check whether it is the original schema config or the output of\n        # biocypher info\n        is_schema_info = schema_config.get(\"is_schema_info\", False)\n\n        # extract the entities and relationships: each top level key that has\n        # a 'represented_as' key\n        self.entities = {}\n        self.relationships = {}\n        if not is_schema_info:\n            for key, value in schema_config.items():\n                # hacky, better with biocypher output\n                name_indicates_relationship = \"interaction\" in key.lower() or \"association\" in key.lower()\n                if \"represented_as\" in value:\n                    if value[\"represented_as\"] == \"node\" and not name_indicates_relationship:\n                        self.entities[sentencecase_to_pascalcase(key)] = value\n                    elif (value[\"represented_as\"] == \"node\" and name_indicates_relationship) or value[\n                        \"represented_as\"\n                    ] == \"edge\":\n                        self.relationships[sentencecase_to_pascalcase(key)] = value\n        else:\n            for key, value in schema_config.items():\n                if not isinstance(value, dict):\n                    continue\n                if value.get(\"present_in_knowledge_graph\", None) == False:\n                    continue\n                if value.get(\"is_relationship\", None) == False:\n                    self.entities[sentencecase_to_pascalcase(key)] = value\n                elif value.get(\"is_relationship\", None) == True:\n                    value = self._capitalise_source_and_target(value)\n                    self.relationships[sentencecase_to_pascalcase(key)] = value\n\n        self.question = \"\"\n        self.selected_entities = []\n        self.selected_relationships = []  # used in property selection\n        self.selected_relationship_labels = {}  # copy to deal with labels that\n        # are not the same as the relationship name, used in query generation\n        # dictionary to also include source and target types\n        self.rel_directions = {}\n        self.model_provider = model_provider\n        self.model_name = model_name\n\n    def _capitalise_source_and_target(self, relationship: dict) -&gt; dict:\n        \"\"\"Make sources and targets PascalCase to match the entities. Sources and\n        targets can be strings or lists of strings.\n        \"\"\"\n        if \"source\" in relationship:\n            if isinstance(relationship[\"source\"], str):\n                relationship[\"source\"] = sentencecase_to_pascalcase(\n                    relationship[\"source\"],\n                )\n            elif isinstance(relationship[\"source\"], list):\n                relationship[\"source\"] = [sentencecase_to_pascalcase(s) for s in relationship[\"source\"]]\n        if \"target\" in relationship:\n            if isinstance(relationship[\"target\"], str):\n                relationship[\"target\"] = sentencecase_to_pascalcase(\n                    relationship[\"target\"],\n                )\n            elif isinstance(relationship[\"target\"], list):\n                relationship[\"target\"] = [sentencecase_to_pascalcase(t) for t in relationship[\"target\"]]\n        return relationship\n\n    def _select_graph_entities_from_question(\n        self,\n        question: str,\n        conversation: Conversation,\n    ) -&gt; str:\n        conversation.reset()\n        success1 = self._select_entities(\n            question=question,\n            conversation=conversation,\n        )\n        if not success1:\n            raise ValueError(\n                \"Entity selection failed. Please try again with a different question.\",\n            )\n        conversation.reset()\n        success2 = self._select_relationships(conversation=conversation)\n        if not success2:\n            raise ValueError(\n                \"Relationship selection failed. Please try again with a different question.\",\n            )\n        conversation.reset()\n        success3 = self._select_properties(conversation=conversation)\n        if not success3:\n            raise ValueError(\n                \"Property selection failed. Please try again with a different question.\",\n            )\n\n    def _generate_query_prompt(\n        self,\n        entities: list,\n        relationships: dict,\n        properties: dict,\n        query_language: str | None = \"Cypher\",\n    ) -&gt; str:\n        \"\"\"Generate a prompt for a large language model to generate a database\n        query based on the selected entities, relationships, and properties.\n\n        Args:\n        ----\n            entities: A list of entities that are relevant to the question.\n\n            relationships: A list of relationships that are relevant to the\n                question.\n\n            properties: A dictionary of properties that are relevant to the\n                question.\n\n            query_language: The language of the query to generate.\n\n        Returns:\n        -------\n            A prompt for a large language model to generate a database query.\n\n        \"\"\"\n        msg = (\n            f\"Generate a database query in {query_language} that answers \"\n            f\"the user's question. \"\n            f\"You can use the following entities: {entities}, \"\n            f\"relationships: {list(relationships.keys())}, and \"\n            f\"properties: {properties}. \"\n        )\n\n        for relationship, values in relationships.items():\n            self._expand_pairs(relationship, values)\n\n        if self.rel_directions:\n            msg += \"Given the following valid combinations of source, relationship, and target: \"\n            for key, value in self.rel_directions.items():\n                for pair in value:\n                    msg += f\"'(:{pair[0]})-(:{key})-&gt;(:{pair[1]})', \"\n            msg += f\"generate a {query_language} query using one of these combinations. \"\n\n        msg += \"Only return the query, without any additional text, symbols or characters --- just the query statement.\"\n        return msg\n\n    def generate_query_prompt(\n        self,\n        question: str,\n        query_language: str | None = \"Cypher\",\n    ) -&gt; str:\n        \"\"\"Generate a prompt for a large language model to generate a database\n        query based on the user's question and class attributes informing about\n        the schema.\n\n        Args:\n        ----\n            question: A user's question.\n\n            query_language: The language of the query to generate.\n\n        Returns:\n        -------\n            A prompt for a large language model to generate a database query.\n\n        \"\"\"\n        self._select_graph_entities_from_question(\n            question,\n            self.conversation_factory(),\n        )\n        msg = self._generate_query_prompt(\n            self.selected_entities,\n            self.selected_relationship_labels,\n            self.selected_properties,\n            query_language,\n        )\n        return msg\n\n    def generate_query(\n        self,\n        question: str,\n        query_language: str | None = \"Cypher\",\n    ) -&gt; str:\n        \"\"\"Wrap entity and property selection and query generation; return the\n        generated query.\n\n        Args:\n        ----\n            question: A user's question.\n\n            query_language: The language of the query to generate.\n\n        Returns:\n        -------\n            A database query that could answer the user's question.\n\n        \"\"\"\n        self._select_graph_entities_from_question(\n            question,\n            self.conversation_factory(),\n        )\n\n        return self._generate_query(\n            question=question,\n            entities=self.selected_entities,\n            relationships=self.selected_relationship_labels,\n            properties=self.selected_properties,\n            query_language=query_language,\n            conversation=self.conversation_factory(),\n        )\n\n    def _get_conversation(\n        self,\n        model_provider: str = \"google_genai\",\n        model_name: str = \"gemini-2.0-flash\",\n    ) -&gt; \"Conversation\":\n        \"\"\"Create a conversation object given a model name.\n\n        Args:\n        ----\n            model_provider: The provider of the model to use.\n\n            model_name: The name of the model to use for the conversation.\n\n        Returns:\n        -------\n            A BioChatter Conversation object for connecting to the LLM.\n\n        Todo:\n        ----\n            Genericise to models outside of OpenAI.\n\n        \"\"\"\n        conversation = LangChainConversation(\n            model_provider=model_provider,\n            model_name=model_name or self.model_name,\n            prompts={},\n            correct=False,\n        )\n        conversation.set_api_key(\n            api_key=os.getenv(\"OPENAI_API_KEY\"),\n            user=\"test_user\",\n        )\n        return conversation\n\n    def _select_entities(\n        self,\n        question: str,\n        conversation: \"Conversation\",\n    ) -&gt; bool:\n        \"\"\"Given a question, select the entities that are relevant to the question\n        and store them in `selected_entities` and `selected_relationships`. Use\n        LLM conversation to do this.\n\n        Args:\n        ----\n            question: A user's question.\n\n            conversation: A BioChatter Conversation object for connecting to the\n                LLM.\n\n        Returns:\n        -------\n            True if at least one entity was selected, False otherwise.\n\n        \"\"\"\n        self.question = question\n\n        conversation.append_system_message(\n            \"You have access to a knowledge graph that contains \"\n            f\"these entity types: {', '.join(self.entities)}. Your task is \"\n            \"to select the entity types that are relevant to the user's question \"\n            \"for subsequent use in a query. Only return the entity types, \"\n            \"comma-separated, without any additional text. Do not return \"\n            \"entity names, relationships, or properties.\",\n        )\n\n        msg, token_usage, correction = conversation.query(question)\n\n        result = msg.split(\",\") if msg else []\n        # TODO: do we go back and retry if no entities were selected? or ask for\n        # a reason? offer visual selection of entities and relationships by the\n        # user?\n\n        if result:\n            for entity in result:\n                entity = entity.strip()\n                if entity in self.entities:\n                    self.selected_entities.append(entity)\n\n        return bool(result)\n\n    def _select_relationships(self, conversation: \"Conversation\") -&gt; bool:\n        \"\"\"Given a question and the preselected entities, select relationships for\n        the query.\n\n        Args:\n        ----\n            conversation: A BioChatter Conversation object for connecting to the\n                LLM.\n\n        Returns:\n        -------\n            True if at least one relationship was selected, False otherwise.\n\n        Todo:\n        ----\n            Now we have the problem that we discard all relationships that do\n            not have a source and target, if at least one relationship has a\n            source and target. At least communicate this all-or-nothing\n            behaviour to the user.\n\n        \"\"\"\n        if not self.question:\n            raise ValueError(\n                \"No question found. Please make sure to run entity selection first.\",\n            )\n\n        if not self.selected_entities:\n            raise ValueError(\n                \"No entities found. Please run the entity selection step first.\",\n            )\n\n        rels = {}\n        source_and_target_present = False\n        for key, value in self.relationships.items():\n            if \"source\" in value and \"target\" in value:\n                # if source or target is a list, expand to single pairs\n                source = ensure_iterable(value[\"source\"])\n                target = ensure_iterable(value[\"target\"])\n                pairs = []\n                for s in source:\n                    for t in target:\n                        pairs.append(\n                            (\n                                sentencecase_to_pascalcase(s),\n                                sentencecase_to_pascalcase(t),\n                            ),\n                        )\n                rels[key] = pairs\n                source_and_target_present = True\n            else:\n                rels[key] = {}\n\n        # prioritise relationships that have source and target, and discard\n        # relationships that do not have both source and target, if at least one\n        # relationship has both source and target. keep relationships that have\n        # either source or target, if none of the relationships have both source\n        # and target.\n\n        if source_and_target_present:\n            # First, separate the relationships into two groups: those with both\n            # source and target in the selected entities, and those with either\n            # source or target but not both.\n\n            rels_with_both = {}\n            rels_with_either = {}\n            for key, value in rels.items():\n                for pair in value:\n                    if pair[0] in self.selected_entities:\n                        if pair[1] in self.selected_entities:\n                            rels_with_both[key] = value\n                        else:\n                            rels_with_either[key] = value\n                    elif pair[1] in self.selected_entities:\n                        rels_with_either[key] = value\n\n            # If there are any relationships with both source and target,\n            # discard the others.\n\n            if rels_with_both:\n                rels = rels_with_both\n            else:\n                rels = rels_with_either\n\n            selected_rels = []\n            for key, value in rels.items():\n                if not value:\n                    continue\n\n                for pair in value:\n                    if pair[0] in self.selected_entities or pair[1] in self.selected_entities:\n                        selected_rels.append((key, pair))\n\n            rels = json.dumps(selected_rels)\n        else:\n            rels = json.dumps(self.relationships)\n\n        msg = (\n            \"You have access to a knowledge graph that contains \"\n            f\"these entities: {', '.join(self.selected_entities)}. \"\n            \"Your task is to select the relationships that are relevant \"\n            \"to the user's question for subsequent use in a query. Only \"\n            \"return the relationships without their sources or targets, \"\n            \"comma-separated, and without any additional text. Here are the \"\n            \"possible relationships and their source and target entities: \"\n            f\"{rels}.\"\n        )\n\n        conversation.append_system_message(msg)\n\n        res, token_usage, correction = conversation.query(self.question)\n\n        result = res.split(\",\") if msg else []\n\n        if result:\n            for relationship in result:\n                relationship = relationship.strip()\n                if relationship in self.relationships:\n                    self.selected_relationships.append(relationship)\n                    rel_dict = self.relationships[relationship]\n                    label = rel_dict.get(\"label_as_edge\", relationship)\n                    if \"source\" in rel_dict and \"target\" in rel_dict:\n                        self.selected_relationship_labels[label] = {\n                            \"source\": rel_dict[\"source\"],\n                            \"target\": rel_dict[\"target\"],\n                        }\n                    else:\n                        self.selected_relationship_labels[label] = {\n                            \"source\": None,\n                            \"target\": None,\n                        }\n\n        # if we selected relationships that have either source or target which\n        # is not in the selected entities, we add those entities to the selected\n        # entities.\n\n        if self.selected_relationship_labels:\n            for key, value in self.selected_relationship_labels.items():\n                sources = ensure_iterable(value[\"source\"])\n                targets = ensure_iterable(value[\"target\"])\n                for source in sources:\n                    if source is None:\n                        continue\n                    if source not in self.selected_entities:\n                        self.selected_entities.append(\n                            sentencecase_to_pascalcase(source),\n                        )\n                for target in targets:\n                    if target is None:\n                        continue\n                    if target not in self.selected_entities:\n                        self.selected_entities.append(\n                            sentencecase_to_pascalcase(target),\n                        )\n\n        return bool(result)\n\n    @staticmethod\n    def _validate_json_str(json_str: str):\n        json_str = json_str.strip()\n        if json_str.startswith(\"```json\"):\n            json_str = json_str[7:]\n        if json_str.endswith(\"```\"):\n            json_str = json_str[:-3]\n        return json_str.strip()\n\n    def _select_properties(self, conversation: \"Conversation\") -&gt; bool:\n        \"\"\"Given a question (optionally provided, but in the standard use case\n        reused from the entity selection step) and the selected entities, select\n        the properties that are relevant to the question and store them in\n        the dictionary `selected_properties`.\n\n        Returns\n        -------\n            True if at least one property was selected, False otherwise.\n\n        \"\"\"\n        if not self.question:\n            raise ValueError(\n                \"No question found. Please make sure to run entity and relationship selection first.\",\n            )\n\n        if not self.selected_entities and not self.selected_relationships:\n            raise ValueError(\n                \"No entities or relationships provided, and none available \"\n                \"from entity selection step. Please provide \"\n                \"entities/relationships or run the entity selection \"\n                \"(`select_entities()`) step first.\",\n            )\n\n        e_props = {}\n        for entity in self.selected_entities:\n            if self.entities[entity].get(\"properties\"):\n                e_props[entity] = list(\n                    self.entities[entity][\"properties\"].keys(),\n                )\n\n        r_props = {}\n        for relationship in self.selected_relationships:\n            if self.relationships[relationship].get(\"properties\"):\n                r_props[relationship] = list(\n                    self.relationships[relationship][\"properties\"].keys(),\n                )\n\n        msg = (\n            \"You have access to a knowledge graph that contains entities and \"\n            \"relationships. They have the following properties. Entities:\"\n            f\"{e_props}, Relationships: {r_props}. \"\n            \"Your task is to select the properties that are relevant to the \"\n            \"user's question for subsequent use in a query. Only return the \"\n            \"entities and relationships with their relevant properties in compact \"\n            \"JSON format, without any additional text. Return the \"\n            \"entities/relationships as top-level dictionary keys, and their \"\n            \"properties as dictionary values. \"\n            \"Do not return properties that are not relevant to the question.\"\n        )\n\n        conversation.append_system_message(msg)\n\n        msg, token_usage, correction = conversation.query(self.question)\n        msg = BioCypherPromptEngine._validate_json_str(msg)\n\n        try:\n            self.selected_properties = json.loads(msg) if msg else {}\n        except json.decoder.JSONDecodeError:\n            self.selected_properties = {}\n\n        return bool(self.selected_properties)\n\n    def _generate_query(\n        self,\n        question: str,\n        entities: list,\n        relationships: dict,\n        properties: dict,\n        query_language: str,\n        conversation: \"Conversation\",\n    ) -&gt; str:\n        \"\"\"Generate a query in the specified query language that answers the user's\n        question.\n\n        Args:\n        ----\n            question: A user's question.\n\n            entities: A list of entities that are relevant to the question.\n\n            relationships: A list of relationships that are relevant to the\n                question.\n\n            properties: A dictionary of properties that are relevant to the\n                question.\n\n            query_language: The language of the query to generate.\n\n            conversation: A BioChatter Conversation object for connecting to the\n                LLM.\n\n        Returns:\n        -------\n            A database query that could answer the user's question.\n\n        \"\"\"\n        msg = self._generate_query_prompt(\n            entities,\n            relationships,\n            properties,\n            query_language,\n        )\n\n        conversation.append_system_message(msg)\n\n        out_msg, token_usage, correction = conversation.query(question)\n\n        return out_msg.strip()\n\n    def _expand_pairs(self, relationship, values) -&gt; None:\n        if not self.rel_directions.get(relationship):\n            self.rel_directions[relationship] = []\n        if isinstance(values[\"source\"], list):\n            for source in values[\"source\"]:\n                if isinstance(values[\"target\"], list):\n                    for target in values[\"target\"]:\n                        self.rel_directions[relationship].append(\n                            (source, target),\n                        )\n                else:\n                    self.rel_directions[relationship].append(\n                        (source, values[\"target\"]),\n                    )\n        elif isinstance(values[\"target\"], list):\n            for target in values[\"target\"]:\n                self.rel_directions[relationship].append(\n                    (values[\"source\"], target),\n                )\n        else:\n            self.rel_directions[relationship].append(\n                (values[\"source\"], values[\"target\"]),\n            )\n</code></pre>"},{"location":"api-docs/kg/#biochatter.prompts.BioCypherPromptEngine.__init__","title":"<code>__init__(schema_config_or_info_path=None, schema_config_or_info_dict=None, model_provider='google_genai', model_name='gemini-2.0-flash', conversation_factory=None)</code>","text":"<p>Given a biocypher schema configuration, extract the entities and relationships, and for each extract their mode of representation (node or edge), properties, and identifier namespace. Using these data, allow the generation of prompts for a large language model, informing it of the schema constituents and their properties, to enable the parameterisation of function calls to a knowledge graph.</p> <pre><code>schema_config_or_info_path: Path to a biocypher schema configuration\n    file or the extended schema information output generated by\n    BioCypher's `write_schema_info` function (preferred).\n\nschema_config_or_info_dict: A dictionary containing the schema\n    configuration file or the extended schema information output\n    generated by BioCypher's `write_schema_info` function\n    (preferred).\n\nmodel_provider: The provider of the model to use.\n\nmodel_name: The name of the model to use for the conversation.\n    DEPRECATED: This should now be set in the conversation factory.\n\nconversation_factory: A function used to create a conversation for\n    creating the KG query. If not provided, a default function is\n    used (creating an OpenAI conversation with the specified model,\n    see `_get_conversation`).\n</code></pre> Source code in <code>biochatter/prompts.py</code> <pre><code>def __init__(\n    self,\n    schema_config_or_info_path: str | None = None,\n    schema_config_or_info_dict: dict | None = None,\n    model_provider: str = \"google_genai\",\n    model_name: str = \"gemini-2.0-flash\",\n    conversation_factory: Callable | None = None,\n) -&gt; None:\n    \"\"\"Given a biocypher schema configuration, extract the entities and\n    relationships, and for each extract their mode of representation (node\n    or edge), properties, and identifier namespace. Using these data, allow\n    the generation of prompts for a large language model, informing it of\n    the schema constituents and their properties, to enable the\n    parameterisation of function calls to a knowledge graph.\n\n    Args:\n    ----\n        schema_config_or_info_path: Path to a biocypher schema configuration\n            file or the extended schema information output generated by\n            BioCypher's `write_schema_info` function (preferred).\n\n        schema_config_or_info_dict: A dictionary containing the schema\n            configuration file or the extended schema information output\n            generated by BioCypher's `write_schema_info` function\n            (preferred).\n\n        model_provider: The provider of the model to use.\n\n        model_name: The name of the model to use for the conversation.\n            DEPRECATED: This should now be set in the conversation factory.\n\n        conversation_factory: A function used to create a conversation for\n            creating the KG query. If not provided, a default function is\n            used (creating an OpenAI conversation with the specified model,\n            see `_get_conversation`).\n\n    \"\"\"\n    if not schema_config_or_info_path and not schema_config_or_info_dict:\n        raise ValueError(\n            \"Please provide the schema configuration or schema info as a path to a file or as a dictionary.\",\n        )\n\n    if schema_config_or_info_path and schema_config_or_info_dict:\n        raise ValueError(\n            \"Please provide the schema configuration or schema info as a \"\n            \"path to a file or as a dictionary, not both.\",\n        )\n\n    # set conversation factory or use default\n    self.conversation_factory = conversation_factory if conversation_factory is not None else self._get_conversation\n\n    if schema_config_or_info_path:\n        # read the schema configuration\n        with open(schema_config_or_info_path) as f:\n            schema_config = yaml.safe_load(f)\n    elif schema_config_or_info_dict:\n        schema_config = schema_config_or_info_dict\n\n    # check whether it is the original schema config or the output of\n    # biocypher info\n    is_schema_info = schema_config.get(\"is_schema_info\", False)\n\n    # extract the entities and relationships: each top level key that has\n    # a 'represented_as' key\n    self.entities = {}\n    self.relationships = {}\n    if not is_schema_info:\n        for key, value in schema_config.items():\n            # hacky, better with biocypher output\n            name_indicates_relationship = \"interaction\" in key.lower() or \"association\" in key.lower()\n            if \"represented_as\" in value:\n                if value[\"represented_as\"] == \"node\" and not name_indicates_relationship:\n                    self.entities[sentencecase_to_pascalcase(key)] = value\n                elif (value[\"represented_as\"] == \"node\" and name_indicates_relationship) or value[\n                    \"represented_as\"\n                ] == \"edge\":\n                    self.relationships[sentencecase_to_pascalcase(key)] = value\n    else:\n        for key, value in schema_config.items():\n            if not isinstance(value, dict):\n                continue\n            if value.get(\"present_in_knowledge_graph\", None) == False:\n                continue\n            if value.get(\"is_relationship\", None) == False:\n                self.entities[sentencecase_to_pascalcase(key)] = value\n            elif value.get(\"is_relationship\", None) == True:\n                value = self._capitalise_source_and_target(value)\n                self.relationships[sentencecase_to_pascalcase(key)] = value\n\n    self.question = \"\"\n    self.selected_entities = []\n    self.selected_relationships = []  # used in property selection\n    self.selected_relationship_labels = {}  # copy to deal with labels that\n    # are not the same as the relationship name, used in query generation\n    # dictionary to also include source and target types\n    self.rel_directions = {}\n    self.model_provider = model_provider\n    self.model_name = model_name\n</code></pre>"},{"location":"api-docs/kg/#biochatter.prompts.BioCypherPromptEngine.generate_query","title":"<code>generate_query(question, query_language='Cypher')</code>","text":"<p>Wrap entity and property selection and query generation; return the generated query.</p> <pre><code>question: A user's question.\n\nquery_language: The language of the query to generate.\n</code></pre> <pre><code>A database query that could answer the user's question.\n</code></pre> Source code in <code>biochatter/prompts.py</code> <pre><code>def generate_query(\n    self,\n    question: str,\n    query_language: str | None = \"Cypher\",\n) -&gt; str:\n    \"\"\"Wrap entity and property selection and query generation; return the\n    generated query.\n\n    Args:\n    ----\n        question: A user's question.\n\n        query_language: The language of the query to generate.\n\n    Returns:\n    -------\n        A database query that could answer the user's question.\n\n    \"\"\"\n    self._select_graph_entities_from_question(\n        question,\n        self.conversation_factory(),\n    )\n\n    return self._generate_query(\n        question=question,\n        entities=self.selected_entities,\n        relationships=self.selected_relationship_labels,\n        properties=self.selected_properties,\n        query_language=query_language,\n        conversation=self.conversation_factory(),\n    )\n</code></pre>"},{"location":"api-docs/kg/#biochatter.prompts.BioCypherPromptEngine.generate_query_prompt","title":"<code>generate_query_prompt(question, query_language='Cypher')</code>","text":"<p>Generate a prompt for a large language model to generate a database query based on the user's question and class attributes informing about the schema.</p> <pre><code>question: A user's question.\n\nquery_language: The language of the query to generate.\n</code></pre> <pre><code>A prompt for a large language model to generate a database query.\n</code></pre> Source code in <code>biochatter/prompts.py</code> <pre><code>def generate_query_prompt(\n    self,\n    question: str,\n    query_language: str | None = \"Cypher\",\n) -&gt; str:\n    \"\"\"Generate a prompt for a large language model to generate a database\n    query based on the user's question and class attributes informing about\n    the schema.\n\n    Args:\n    ----\n        question: A user's question.\n\n        query_language: The language of the query to generate.\n\n    Returns:\n    -------\n        A prompt for a large language model to generate a database query.\n\n    \"\"\"\n    self._select_graph_entities_from_question(\n        question,\n        self.conversation_factory(),\n    )\n    msg = self._generate_query_prompt(\n        self.selected_entities,\n        self.selected_relationship_labels,\n        self.selected_properties,\n        query_language,\n    )\n    return msg\n</code></pre>"},{"location":"api-docs/kg/#execution-of-prompts-against-the-database","title":"Execution of prompts against the database","text":""},{"location":"api-docs/kg/#biochatter.database_agent.DatabaseAgent","title":"<code>DatabaseAgent</code>","text":"Source code in <code>biochatter/database_agent.py</code> <pre><code>class DatabaseAgent:\n    def __init__(\n        self,\n        model_provider: str,\n        model_name: str,\n        connection_args: dict,\n        schema_config_or_info_dict: dict,\n        conversation_factory: Callable,\n        use_reflexion: bool,\n    ) -&gt; None:\n        \"\"\"Create a DatabaseAgent analogous to the VectorDatabaseAgentMilvus class,\n        which can return results from a database using a query engine. Currently\n        limited to Neo4j for development.\n\n        Args:\n        ----\n            connection_args (dict): A dictionary of arguments to connect to the\n                database. Contains database name, URI, user, and password.\n\n            conversation_factory (Callable): A function to create a conversation\n                for creating the KG query.\n\n            use_reflexion (bool): Whether to use the ReflexionAgent to generate\n                the query.\n\n        \"\"\"\n        self.conversation_factory = conversation_factory\n        self.prompt_engine = BioCypherPromptEngine(\n            model_provider=model_provider,\n            model_name=model_name,\n            schema_config_or_info_dict=schema_config_or_info_dict,\n            conversation_factory=conversation_factory,\n        )\n        self.connection_args = connection_args\n        self.driver = None\n        self.use_reflexion = use_reflexion\n\n    def connect(self) -&gt; None:\n        \"\"\"Connect to the database and authenticate.\"\"\"\n        db_name = self.connection_args.get(\"db_name\")\n        uri = f\"{self.connection_args.get('host')}:{self.connection_args.get('port')}\"\n        uri = uri if uri.startswith(\"bolt://\") else \"bolt://\" + uri\n        user = self.connection_args.get(\"user\")\n        password = self.connection_args.get(\"password\")\n        self.driver = nu.Driver(\n            db_name=db_name or \"neo4j\",\n            db_uri=uri,\n            user=user,\n            password=password,\n        )\n\n    def is_connected(self) -&gt; bool:\n        return self.driver is not None\n\n    def _generate_query(self, query: str):\n        if self.use_reflexion:\n            agent = KGQueryReflexionAgent(\n                self.conversation_factory,\n                self.connection_args,\n            )\n            query_prompt = self.prompt_engine.generate_query_prompt(query)\n            agent_result = agent.execute(query, query_prompt)\n            tool_result = [agent_result.tool_result] if agent_result.tool_result is not None else None\n            return agent_result.answer, tool_result\n        else:\n            query = self.prompt_engine.generate_query(query)\n            results = self.driver.query(query=query)\n            return query, results\n\n    def _build_response(\n        self,\n        results: list[dict],\n        cypher_query: str,\n        results_num: int | None = 3,\n    ) -&gt; list[Document]:\n        if len(results) == 0:\n            return [\n                Document(\n                    page_content=(\n                        \"I didn't find any result in knowledge graph, \"\n                        f\"but here is the query I used: {cypher_query}. \"\n                        \"You can ask user to refine the question. \"\n                        \"Note: please ensure to include the query in a code \"\n                        \"block in your response so that the user can refine \"\n                        \"their question effectively.\"\n                    ),\n                    metadata={\"cypher_query\": cypher_query},\n                ),\n            ]\n\n        clipped_results = results[:results_num] if results_num &gt; 0 else results\n        results_dump = json.dumps(clipped_results)\n\n        return [\n            Document(\n                page_content=(\n                    \"The results retrieved from knowledge graph are: \"\n                    f\"{results_dump}. \"\n                    f\"The query used is: {cypher_query}. \"\n                    \"Note: please ensure to include the query in a code block \"\n                    \"in your response so that the user can refine \"\n                    \"their question effectively.\"\n                ),\n                metadata={\"cypher_query\": cypher_query},\n            ),\n        ]\n\n    def get_query_results(self, query: str, k: int = 3) -&gt; list[Document]:\n        \"\"\"Generate a query using the prompt engine and return the results.\n        Replicates vector database similarity search API. Results are returned\n        as a list of Document objects to align with the vector database agent.\n\n        Args:\n        ----\n            query (str): A query string.\n\n            k (int): The number of results to return.\n\n        Returns:\n        -------\n            List[Document]: A list of Document objects. The page content values\n                are the literal dictionaries returned by the query, the metadata\n                values are the cypher query used to generate the results, for\n                now.\n\n        \"\"\"\n        (cypher_query, tool_result) = self._generate_query(\n            query,\n        )  # self.prompt_engine.generate_query(query)\n        # TODO some logic if it fails?\n        if tool_result is not None:\n            # If _generate_query() already returned tool_result, we won't connect\n            # to graph database to query result any more\n            results = tool_result\n        else:\n            results = self.driver.query(query=cypher_query)\n\n        # return first k results\n        # returned nodes can have any formatting, and can also be empty or fewer\n        # than k\n        if results is None or len(results) == 0 or results[0] is None:\n            return []\n        return self._build_response(\n            results=results[0],\n            cypher_query=cypher_query,\n            results_num=k,\n        )\n\n    def get_description(self):\n        result = self.driver.query(\"MATCH (n:Schema_info) RETURN n LIMIT 1\")\n\n        if result[0]:\n            schema_info_node = result[0][0][\"n\"]\n            schema_dict_content = schema_info_node[\"schema_info\"][:MAX_AGENT_DESC_LENGTH]  # limit to 1000 characters\n            return f\"the graph database contains the following nodes and edges: \\n\\n{schema_dict_content}\"\n\n        # schema_info is not found in database\n        nodes_query = \"MATCH (n) RETURN DISTINCT labels(n) LIMIT 300\"\n        node_results = self.driver.query(query=nodes_query)\n        edges_query = \"MATCH (n) RETURN DISTINCT type(n) LIMIT 300\"\n        edge_results = self.driver.query(query=edges_query)\n        desc = (\n            f\"The graph database contains the following nodes and edges: \\n\"\n            f\"nodes: \\n{node_results}\"\n            f\"edges: \\n{edge_results}\"\n        )\n        return desc[:MAX_AGENT_DESC_LENGTH]\n</code></pre>"},{"location":"api-docs/kg/#biochatter.database_agent.DatabaseAgent.__init__","title":"<code>__init__(model_provider, model_name, connection_args, schema_config_or_info_dict, conversation_factory, use_reflexion)</code>","text":"<p>Create a DatabaseAgent analogous to the VectorDatabaseAgentMilvus class, which can return results from a database using a query engine. Currently limited to Neo4j for development.</p> <pre><code>connection_args (dict): A dictionary of arguments to connect to the\n    database. Contains database name, URI, user, and password.\n\nconversation_factory (Callable): A function to create a conversation\n    for creating the KG query.\n\nuse_reflexion (bool): Whether to use the ReflexionAgent to generate\n    the query.\n</code></pre> Source code in <code>biochatter/database_agent.py</code> <pre><code>def __init__(\n    self,\n    model_provider: str,\n    model_name: str,\n    connection_args: dict,\n    schema_config_or_info_dict: dict,\n    conversation_factory: Callable,\n    use_reflexion: bool,\n) -&gt; None:\n    \"\"\"Create a DatabaseAgent analogous to the VectorDatabaseAgentMilvus class,\n    which can return results from a database using a query engine. Currently\n    limited to Neo4j for development.\n\n    Args:\n    ----\n        connection_args (dict): A dictionary of arguments to connect to the\n            database. Contains database name, URI, user, and password.\n\n        conversation_factory (Callable): A function to create a conversation\n            for creating the KG query.\n\n        use_reflexion (bool): Whether to use the ReflexionAgent to generate\n            the query.\n\n    \"\"\"\n    self.conversation_factory = conversation_factory\n    self.prompt_engine = BioCypherPromptEngine(\n        model_provider=model_provider,\n        model_name=model_name,\n        schema_config_or_info_dict=schema_config_or_info_dict,\n        conversation_factory=conversation_factory,\n    )\n    self.connection_args = connection_args\n    self.driver = None\n    self.use_reflexion = use_reflexion\n</code></pre>"},{"location":"api-docs/kg/#biochatter.database_agent.DatabaseAgent.connect","title":"<code>connect()</code>","text":"<p>Connect to the database and authenticate.</p> Source code in <code>biochatter/database_agent.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"Connect to the database and authenticate.\"\"\"\n    db_name = self.connection_args.get(\"db_name\")\n    uri = f\"{self.connection_args.get('host')}:{self.connection_args.get('port')}\"\n    uri = uri if uri.startswith(\"bolt://\") else \"bolt://\" + uri\n    user = self.connection_args.get(\"user\")\n    password = self.connection_args.get(\"password\")\n    self.driver = nu.Driver(\n        db_name=db_name or \"neo4j\",\n        db_uri=uri,\n        user=user,\n        password=password,\n    )\n</code></pre>"},{"location":"api-docs/kg/#biochatter.database_agent.DatabaseAgent.get_query_results","title":"<code>get_query_results(query, k=3)</code>","text":"<p>Generate a query using the prompt engine and return the results. Replicates vector database similarity search API. Results are returned as a list of Document objects to align with the vector database agent.</p> <pre><code>query (str): A query string.\n\nk (int): The number of results to return.\n</code></pre> <pre><code>List[Document]: A list of Document objects. The page content values\n    are the literal dictionaries returned by the query, the metadata\n    values are the cypher query used to generate the results, for\n    now.\n</code></pre> Source code in <code>biochatter/database_agent.py</code> <pre><code>def get_query_results(self, query: str, k: int = 3) -&gt; list[Document]:\n    \"\"\"Generate a query using the prompt engine and return the results.\n    Replicates vector database similarity search API. Results are returned\n    as a list of Document objects to align with the vector database agent.\n\n    Args:\n    ----\n        query (str): A query string.\n\n        k (int): The number of results to return.\n\n    Returns:\n    -------\n        List[Document]: A list of Document objects. The page content values\n            are the literal dictionaries returned by the query, the metadata\n            values are the cypher query used to generate the results, for\n            now.\n\n    \"\"\"\n    (cypher_query, tool_result) = self._generate_query(\n        query,\n    )  # self.prompt_engine.generate_query(query)\n    # TODO some logic if it fails?\n    if tool_result is not None:\n        # If _generate_query() already returned tool_result, we won't connect\n        # to graph database to query result any more\n        results = tool_result\n    else:\n        results = self.driver.query(query=cypher_query)\n\n    # return first k results\n    # returned nodes can have any formatting, and can also be empty or fewer\n    # than k\n    if results is None or len(results) == 0 or results[0] is None:\n        return []\n    return self._build_response(\n        results=results[0],\n        cypher_query=cypher_query,\n        results_num=k,\n    )\n</code></pre>"},{"location":"api-docs/llm_connect/","title":"LLM Connectivity Reference","text":"<p>Here we handle connections to various LLM services, proprietary and open source.</p> <p>Handle connections to different LLM providers.</p>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.AnthropicConversation","title":"<code>AnthropicConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the Anthropic model.</p> Source code in <code>biochatter/llm_connect/anthropic.py</code> <pre><code>class AnthropicConversation(Conversation):\n    \"\"\"Conversation class for the Anthropic model.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n    ) -&gt; None:\n        \"\"\"Connect to Anthropic's API and set up a conversation with the user.\n\n        Also initialise a second conversational agent to provide corrections to\n        the model output, if necessary.\n\n        Args:\n        ----\n            model_name (str): The name of the model to use.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n\n        self.ca_model_name = \"claude-3-5-sonnet-20240620\"\n        # TODO make accessible by drop-down\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the Anthropic API.\n\n        If the key is valid, initialise the conversational agent. Optionally set\n        the user for usage statistics.\n\n        Args:\n        ----\n            api_key (str): The API key for the Anthropic API.\n\n            user (str, optional): The user for usage statistics. If provided and\n                equals \"community\", will track usage stats.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        client = anthropic.Anthropic(\n            api_key=api_key,\n        )\n        self.user = user\n\n        try:\n            client.count_tokens(\"Test connection\")\n            self.chat = ChatAnthropic(\n                model_name=self.model_name,\n                temperature=0,\n                api_key=api_key,\n            )\n            self.ca_chat = ChatAnthropic(\n                model_name=self.ca_model_name,\n                temperature=0,\n                api_key=api_key,\n            )\n            if user == \"community\":\n                self.usage_stats = get_stats(user=user)\n\n            return True\n\n        except anthropic._exceptions.AuthenticationError:\n            self._chat = None\n            self._ca_chat = None\n            return False\n\n    def _primary_query(self, **kwargs) -&gt; tuple:\n        \"\"\"Query the Anthropic API with the user's message.\n\n        Return the response using the message history (flattery system messages,\n        prior conversation) as context. Correct the response if necessary.\n\n        Args:\n        ----\n            **kwargs: Keyword arguments (not used by this basic Anthropic implementation,\n                     but accepted for compatibility with the base Conversation interface)\n\n        Returns:\n        -------\n            tuple: A tuple containing the response from the Anthropic API and\n                the token usage.\n\n        \"\"\"\n        if kwargs:\n            warnings.warn(f\"Warning: {kwargs} are not used by this class\", UserWarning)\n\n        try:\n            history = self._create_history()\n            response = self.chat.generate([history])\n        except (\n            anthropic._exceptions.APIError,\n            anthropic._exceptions.AnthropicError,\n            anthropic._exceptions.ConflictError,\n            anthropic._exceptions.NotFoundError,\n            anthropic._exceptions.APIStatusError,\n            anthropic._exceptions.RateLimitError,\n            anthropic._exceptions.APITimeoutError,\n            anthropic._exceptions.BadRequestError,\n            anthropic._exceptions.APIConnectionError,\n            anthropic._exceptions.AuthenticationError,\n            anthropic._exceptions.InternalServerError,\n            anthropic._exceptions.PermissionDeniedError,\n            anthropic._exceptions.UnprocessableEntityError,\n            anthropic._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n\n        msg = response.generations[0][0].text\n        token_usage_raw = response.llm_output.get(\"token_usage\")\n        token_usage = self._extract_total_tokens(token_usage_raw)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _create_history(self) -&gt; list:\n        \"\"\"Create a history of messages for the Anthropic API.\n\n        Returns\n        -------\n            list: A list of messages, with the last message being the most\n                recent.\n\n        \"\"\"\n        history = []\n        # extract text components from message contents\n        msg_texts = [m.content[0][\"text\"] if isinstance(m.content, list) else m.content for m in self.messages]\n\n        # check if last message is an image message\n        is_image_message = False\n        if isinstance(self.messages[-1].content, list):\n            is_image_message = self.messages[-1].content[1][\"type\"] == \"image_url\"\n\n        # find location of last AI message (if any)\n        last_ai_message = None\n        for i, m in enumerate(self.messages):\n            if isinstance(m, AIMessage):\n                last_ai_message = i\n\n        # Aggregate system messages into one message at the beginning\n        system_messages = [m.content for m in self.messages if isinstance(m, SystemMessage)]\n        if system_messages:\n            history.append(\n                SystemMessage(content=\"\\n\".join(system_messages)),\n            )\n\n        # concatenate all messages before the last AI message into one message\n        if last_ai_message is not None:\n            history.append(\n                HumanMessage(\n                    content=\"\\n\".join([m for m in msg_texts[:last_ai_message]]),\n                ),\n            )\n            # then append the last AI message\n            history.append(\n                AIMessage(\n                    content=msg_texts[last_ai_message],\n                ),\n            )\n\n            # then concatenate all messages after that\n            # into one HumanMessage\n            history.append(\n                HumanMessage(\n                    content=\"\\n\".join(\n                        [m for m in msg_texts[last_ai_message + 1 :]],\n                    ),\n                ),\n            )\n\n        # else add human message to history (without system messages)\n        else:\n            last_system_message = None\n            for i, m in enumerate(self.messages):\n                if isinstance(m, SystemMessage):\n                    last_system_message = i\n            history.append(\n                HumanMessage(\n                    content=\"\\n\".join(\n                        [m for m in msg_texts[last_system_message + 1 :]],\n                    ),\n                ),\n            )\n\n        # if the last message is an image message, add the image to the history\n        if is_image_message:\n            history[-1].content = [\n                {\"type\": \"text\", \"text\": history[-1].content},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": self.messages[-1].content[1][\"image_url\"][\"url\"],\n                    },\n                },\n            ]\n        return history\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response from the Anthropic API.\n\n        Send the response to a secondary language model. Optionally split the\n        response into single sentences and correct each sentence individually.\n        Update usage stats.\n\n        Args:\n        ----\n            msg (str): The response from the Anthropic API.\n\n        Returns:\n        -------\n            str: The corrected response (or OK if no correction necessary).\n\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n            ),\n        )\n\n        response = self.ca_chat.generate([ca_messages])\n\n        correction = response.generations[0][0].text\n        token_usage = response.llm_output.get(\"token_usage\")\n\n        return correction\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.AnthropicConversation.__init__","title":"<code>__init__(model_name, prompts, correct=False, split_correction=False)</code>","text":"<p>Connect to Anthropic's API and set up a conversation with the user.</p> <p>Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <pre><code>model_name (str): The name of the model to use.\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\nsplit_correction (bool): Whether to correct the model output by\n    splitting the output into sentences and correcting each\n    sentence individually.\n</code></pre> Source code in <code>biochatter/llm_connect/anthropic.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = False,\n    split_correction: bool = False,\n) -&gt; None:\n    \"\"\"Connect to Anthropic's API and set up a conversation with the user.\n\n    Also initialise a second conversational agent to provide corrections to\n    the model output, if necessary.\n\n    Args:\n    ----\n        model_name (str): The name of the model to use.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n\n    self.ca_model_name = \"claude-3-5-sonnet-20240620\"\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.AnthropicConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the Anthropic API.</p> <p>If the key is valid, initialise the conversational agent. Optionally set the user for usage statistics.</p> <pre><code>api_key (str): The API key for the Anthropic API.\n\nuser (str, optional): The user for usage statistics. If provided and\n    equals \"community\", will track usage stats.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/llm_connect/anthropic.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the Anthropic API.\n\n    If the key is valid, initialise the conversational agent. Optionally set\n    the user for usage statistics.\n\n    Args:\n    ----\n        api_key (str): The API key for the Anthropic API.\n\n        user (str, optional): The user for usage statistics. If provided and\n            equals \"community\", will track usage stats.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    client = anthropic.Anthropic(\n        api_key=api_key,\n    )\n    self.user = user\n\n    try:\n        client.count_tokens(\"Test connection\")\n        self.chat = ChatAnthropic(\n            model_name=self.model_name,\n            temperature=0,\n            api_key=api_key,\n        )\n        self.ca_chat = ChatAnthropic(\n            model_name=self.ca_model_name,\n            temperature=0,\n            api_key=api_key,\n        )\n        if user == \"community\":\n            self.usage_stats = get_stats(user=user)\n\n        return True\n\n    except anthropic._exceptions.AuthenticationError:\n        self._chat = None\n        self._ca_chat = None\n        return False\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.AzureGptConversation","title":"<code>AzureGptConversation</code>","text":"<p>               Bases: <code>GptConversation</code></p> <p>Conversation class for the Azure GPT model.</p> Source code in <code>biochatter/llm_connect/azure.py</code> <pre><code>class AzureGptConversation(GptConversation):\n    \"\"\"Conversation class for the Azure GPT model.\"\"\"\n\n    def __init__(\n        self,\n        deployment_name: str,\n        model_name: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n        version: str | None = None,\n        base_url: str | None = None,\n        update_token_usage: Callable | None = None,\n    ) -&gt; None:\n        \"\"\"Connect to Azure's GPT API and set up a conversation with the user.\n\n        Extends GptConversation.\n\n        Args:\n        ----\n            deployment_name (str): The name of the Azure deployment to use.\n\n            model_name (str): The name of the model to use. This is distinct\n                from the deployment name.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            correct (bool): Whether to correct the model output.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n\n            version (str): The version of the Azure API to use.\n\n            base_url (str): The base URL of the Azure API to use.\n\n            update_token_usage (Callable): A function to update the token usage\n                statistics.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n            update_token_usage=update_token_usage,\n        )\n\n        self.version = version\n        self.base_url = base_url\n        self.deployment_name = deployment_name\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the Azure API.\n\n        If the key is valid, initialise the conversational agent. No user stats\n        on Azure.\n\n        Args:\n        ----\n            api_key (str): The API key for the Azure API.\n\n            user (str, optional): The user for usage statistics.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        try:\n            self.chat = AzureChatOpenAI(\n                deployment_name=self.deployment_name,\n                model_name=self.model_name,\n                openai_api_version=self.version,\n                azure_endpoint=self.base_url,\n                openai_api_key=api_key,\n                temperature=0,\n            )\n            self.ca_chat = AzureChatOpenAI(\n                deployment_name=self.deployment_name,\n                model_name=self.model_name,\n                openai_api_version=self.version,\n                azure_endpoint=self.base_url,\n                openai_api_key=api_key,\n                temperature=0,\n            )\n\n            self.chat.generate([[HumanMessage(content=\"Hello\")]])\n            self.user = user if user is not None else \"Azure Community\"\n\n            return True\n\n        except openai._exceptions.AuthenticationError:\n            self._chat = None\n            self._ca_chat = None\n            return False\n\n    def _update_usage_stats(self, model: str, token_usage: dict) -&gt; None:\n        if self._update_token_usage is not None:\n            self._update_token_usage(self.user, model, token_usage)\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.AzureGptConversation.__init__","title":"<code>__init__(deployment_name, model_name, prompts, correct=False, split_correction=False, version=None, base_url=None, update_token_usage=None)</code>","text":"<p>Connect to Azure's GPT API and set up a conversation with the user.</p> <p>Extends GptConversation.</p> <pre><code>deployment_name (str): The name of the Azure deployment to use.\n\nmodel_name (str): The name of the model to use. This is distinct\n    from the deployment name.\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\ncorrect (bool): Whether to correct the model output.\n\nsplit_correction (bool): Whether to correct the model output by\n    splitting the output into sentences and correcting each\n    sentence individually.\n\nversion (str): The version of the Azure API to use.\n\nbase_url (str): The base URL of the Azure API to use.\n\nupdate_token_usage (Callable): A function to update the token usage\n    statistics.\n</code></pre> Source code in <code>biochatter/llm_connect/azure.py</code> <pre><code>def __init__(\n    self,\n    deployment_name: str,\n    model_name: str,\n    prompts: dict,\n    correct: bool = False,\n    split_correction: bool = False,\n    version: str | None = None,\n    base_url: str | None = None,\n    update_token_usage: Callable | None = None,\n) -&gt; None:\n    \"\"\"Connect to Azure's GPT API and set up a conversation with the user.\n\n    Extends GptConversation.\n\n    Args:\n    ----\n        deployment_name (str): The name of the Azure deployment to use.\n\n        model_name (str): The name of the model to use. This is distinct\n            from the deployment name.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        correct (bool): Whether to correct the model output.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n\n        version (str): The version of the Azure API to use.\n\n        base_url (str): The base URL of the Azure API to use.\n\n        update_token_usage (Callable): A function to update the token usage\n            statistics.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n        update_token_usage=update_token_usage,\n    )\n\n    self.version = version\n    self.base_url = base_url\n    self.deployment_name = deployment_name\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.AzureGptConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the Azure API.</p> <p>If the key is valid, initialise the conversational agent. No user stats on Azure.</p> <pre><code>api_key (str): The API key for the Azure API.\n\nuser (str, optional): The user for usage statistics.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/llm_connect/azure.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the Azure API.\n\n    If the key is valid, initialise the conversational agent. No user stats\n    on Azure.\n\n    Args:\n    ----\n        api_key (str): The API key for the Azure API.\n\n        user (str, optional): The user for usage statistics.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    try:\n        self.chat = AzureChatOpenAI(\n            deployment_name=self.deployment_name,\n            model_name=self.model_name,\n            openai_api_version=self.version,\n            azure_endpoint=self.base_url,\n            openai_api_key=api_key,\n            temperature=0,\n        )\n        self.ca_chat = AzureChatOpenAI(\n            deployment_name=self.deployment_name,\n            model_name=self.model_name,\n            openai_api_version=self.version,\n            azure_endpoint=self.base_url,\n            openai_api_key=api_key,\n            temperature=0,\n        )\n\n        self.chat.generate([[HumanMessage(content=\"Hello\")]])\n        self.user = user if user is not None else \"Azure Community\"\n\n        return True\n\n    except openai._exceptions.AuthenticationError:\n        self._chat = None\n        self._ca_chat = None\n        return False\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.BloomConversation","title":"<code>BloomConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the Bloom model.</p> Source code in <code>biochatter/llm_connect/misc.py</code> <pre><code>class BloomConversation(Conversation):\n    \"\"\"Conversation class for the Bloom model.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        split_correction: bool,\n    ) -&gt; None:\n        \"\"\"Initialise the BloomConversation class.\n\n        DEPRECATED: Superceded by XinferenceConversation.\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            split_correction=split_correction,\n        )\n\n        self.messages = []\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the HuggingFace API.\n\n        If the key is valid, initialise the conversational agent.\n\n        Args:\n        ----\n            api_key (str): The API key for the HuggingFace API.\n\n            user (str): The user for usage statistics.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        self.chat = HuggingFaceHub(\n            repo_id=self.model_name,\n            model_kwargs={\"temperature\": 1.0},  # \"regular sampling\"\n            # as per https://huggingface.co/docs/api-inference/detailed_parameters\n            huggingfacehub_api_token=api_key,\n        )\n\n        try:\n            self.chat.generate([\"Hello, I am a biomedical researcher.\"])\n            return True\n        except ValueError:\n            return False\n\n    def _cast_messages(self, messages: list) -&gt; str:\n        \"\"\"Render the different roles of the chat-based conversation.\"\"\"\n        cast = \"\"\n        for m in messages:\n            if isinstance(m, SystemMessage):\n                cast += f\"System: {m.content}\\n\"\n            elif isinstance(m, HumanMessage):\n                cast += f\"Human: {m.content}\\n\"\n            elif isinstance(m, AIMessage):\n                cast += f\"AI: {m.content}\\n\"\n            else:\n                error_msg = f\"Unknown message type: {type(m)}\"\n                raise TypeError(error_msg)\n\n        return cast\n\n    def _primary_query(self) -&gt; tuple:\n        response = self.chat.generate([self._cast_messages(self.messages)])\n\n        msg = response.generations[0][0].text\n        token_usage_raw = {\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_tokens\": 0,\n        }\n        token_usage = self._extract_total_tokens(token_usage_raw)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str) -&gt; str:\n        return \"ok\"\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.BloomConversation.__init__","title":"<code>__init__(model_name, prompts, split_correction)</code>","text":"<p>Initialise the BloomConversation class.</p> <p>DEPRECATED: Superceded by XinferenceConversation.</p> Source code in <code>biochatter/llm_connect/misc.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    split_correction: bool,\n) -&gt; None:\n    \"\"\"Initialise the BloomConversation class.\n\n    DEPRECATED: Superceded by XinferenceConversation.\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        split_correction=split_correction,\n    )\n\n    self.messages = []\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.BloomConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the HuggingFace API.</p> <p>If the key is valid, initialise the conversational agent.</p> <pre><code>api_key (str): The API key for the HuggingFace API.\n\nuser (str): The user for usage statistics.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/llm_connect/misc.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the HuggingFace API.\n\n    If the key is valid, initialise the conversational agent.\n\n    Args:\n    ----\n        api_key (str): The API key for the HuggingFace API.\n\n        user (str): The user for usage statistics.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    self.chat = HuggingFaceHub(\n        repo_id=self.model_name,\n        model_kwargs={\"temperature\": 1.0},  # \"regular sampling\"\n        # as per https://huggingface.co/docs/api-inference/detailed_parameters\n        huggingfacehub_api_token=api_key,\n    )\n\n    try:\n        self.chat.generate([\"Hello, I am a biomedical researcher.\"])\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation","title":"<code>Conversation</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Use this class to set up a connection to an LLM API.</p> <p>Can be used to set the user name and API key, append specific messages for system, user, and AI roles (if available), set up the general context as well as manual and tool-based data inputs, and finally to query the API with prompts made by the user.</p> <p>The conversation class is expected to have a <code>messages</code> attribute to store the conversation, and a <code>history</code> attribute, which is a list of messages in a specific format for logging / printing.</p> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>class Conversation(ABC):\n    \"\"\"Use this class to set up a connection to an LLM API.\n\n    Can be used to set the user name and API key, append specific messages for\n    system, user, and AI roles (if available), set up the general context as\n    well as manual and tool-based data inputs, and finally to query the API\n    with prompts made by the user.\n\n    The conversation class is expected to have a `messages` attribute to store\n    the conversation, and a `history` attribute, which is a list of messages in\n    a specific format for logging / printing.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n        use_ragagent_selector: bool = False,\n        tools: list[Callable] = None,\n        tool_call_mode: Literal[\"auto\", \"text\"] = \"auto\",\n        mcp: bool = False,\n        additional_tools_instructions: str = None,\n        force_tool: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.prompts = prompts\n        self.correct = correct\n        self.split_correction = split_correction\n        self.rag_agents: list[RagAgent] = []\n        self.history = []\n        self.messages = []\n        self.ca_messages = []\n        self.tool_calls = deque()\n        self.current_statements = []\n        self._use_ragagent_selector = use_ragagent_selector\n        self._chat = None\n        self._ca_chat = None\n        self.tools = tools\n        self.tool_call_mode = tool_call_mode\n        self.tools_prompt = None\n        self.mcp = mcp\n        self.additional_tools_instructions = additional_tools_instructions if additional_tools_instructions else \"\"\n        self.force_tool = force_tool\n\n    @property\n    def chat(self):\n        \"\"\"Access the chat attribute with error handling.\"\"\"\n        if self._chat is None:\n            msg = \"Chat attribute not initialized. Did you call set_api_key()?\"\n            logger.error(msg)\n            raise AttributeError(msg)\n        return self._chat\n\n    @chat.setter\n    def chat(self, value):\n        \"\"\"Set the chat attribute.\"\"\"\n        self._chat = value\n\n    @property\n    def ca_chat(self):\n        \"\"\"Access the correcting agent chat attribute with error handling.\"\"\"\n        if self._ca_chat is None:\n            msg = \"Correcting agent chat attribute not initialized. Did you call set_api_key()?\"\n            logger.error(msg)\n            raise AttributeError(msg)\n        return self._ca_chat\n\n    @ca_chat.setter\n    def ca_chat(self, value):\n        \"\"\"Set the correcting agent chat attribute.\"\"\"\n        self._ca_chat = value\n\n    @property\n    def use_ragagent_selector(self) -&gt; bool:\n        \"\"\"Whether to use the ragagent selector.\"\"\"\n        return self._use_ragagent_selector\n\n    @use_ragagent_selector.setter\n    def use_ragagent_selector(self, val: bool) -&gt; None:\n        \"\"\"Set the use_ragagent_selector attribute.\"\"\"\n        self._use_ragagent_selector = val\n\n    def set_user_name(self, user_name: str) -&gt; None:\n        \"\"\"Set the user name.\"\"\"\n        self.user_name = user_name\n\n    def set_rag_agent(self, agent: RagAgent) -&gt; None:\n        \"\"\"Update or insert rag_agent.\n\n        If the rag_agent with the same mode already exists, it will be updated.\n        Otherwise, the new rag_agent will be inserted.\n        \"\"\"\n        i, _ = self.find_rag_agent(agent.mode)\n        if i &lt; 0:\n            # insert\n            self.rag_agents.append(agent)\n        else:\n            # update\n            self.rag_agents[i] = agent\n\n    def find_rag_agent(self, mode: str) -&gt; tuple[int, RagAgent]:\n        \"\"\"Find the rag_agent with the given mode.\"\"\"\n        for i, val in enumerate(self.rag_agents):\n            if val.mode == mode:\n                return i, val\n        return -1, None\n\n    def _extract_total_tokens(self, token_usage: dict | int | None) -&gt; int | None:\n        \"\"\"Extract total tokens from various token usage formats.\n\n        This method standardizes token counting across different providers:\n        - OpenAI/Azure: {\"prompt_tokens\": X, \"completion_tokens\": Y, \"total_tokens\": Z}\n        - Anthropic: {\"input_tokens\": X, \"output_tokens\": Y} -&gt; calculate total\n        - Gemini: {\"total_tokens\": Z} -&gt; extract total\n        - Ollama: integer (eval_count) -&gt; return as is\n        - LiteLLM: {\"input_tokens\": X, \"output_tokens\": Y, \"total_tokens\": Z}\n        - Others: try to extract or calculate total\n\n        Args:\n        ----\n            token_usage: Token usage in various formats (dict, int, or None)\n\n        Returns:\n        -------\n            int | None: Total token count, or None if not available\n\n        \"\"\"\n        if token_usage is None:\n            return None\n\n        # Handle integer token counts (Ollama, some others)\n        if isinstance(token_usage, int):\n            return token_usage\n\n        # Handle dictionary token counts\n        if isinstance(token_usage, dict):\n            # First try to get total_tokens directly\n            if \"total_tokens\" in token_usage:\n                return token_usage[\"total_tokens\"]\n\n            # Calculate from input/output tokens (Anthropic style)\n            if \"input_tokens\" in token_usage and \"output_tokens\" in token_usage:\n                return token_usage[\"input_tokens\"] + token_usage[\"output_tokens\"]\n\n            # Calculate from prompt/completion tokens (OpenAI style fallback)\n            if \"prompt_tokens\" in token_usage and \"completion_tokens\" in token_usage:\n                return token_usage[\"prompt_tokens\"] + token_usage[\"completion_tokens\"]\n\n            # If only one type of token count is available, use it\n            if \"input_tokens\" in token_usage:\n                return token_usage[\"input_tokens\"]\n            if \"output_tokens\" in token_usage:\n                return token_usage[\"output_tokens\"]\n            if \"prompt_tokens\" in token_usage:\n                return token_usage[\"prompt_tokens\"]\n            if \"completion_tokens\" in token_usage:\n                return token_usage[\"completion_tokens\"]\n\n        # If we can't extract meaningful token count, return None\n        return None\n\n    def _extract_input_tokens(self, token_usage: dict | int | None) -&gt; int | None:\n        \"\"\"Extract input tokens from various token usage formats.\n\n        This method standardizes input token counting across different providers:\n        - OpenAI/Azure: {\"prompt_tokens\": X, \"completion_tokens\": Y, \"total_tokens\": Z}\n        - Anthropic: {\"input_tokens\": X, \"output_tokens\": Y}\n        - Gemini: {\"prompt_tokens\": X, \"candidates_tokens\": Y, \"total_tokens\": Z}\n        - LiteLLM: {\"input_tokens\": X, \"output_tokens\": Y, \"total_tokens\": Z}\n        - Others: try to extract input/prompt tokens\n\n        Args:\n        ----\n            token_usage: Token usage in various formats (dict, int, or None)\n\n        Returns:\n        -------\n            int | None: Input token count, or None if not available\n\n        \"\"\"\n        if token_usage is None:\n            return None\n\n        # Handle integer token counts (cannot distinguish input vs output)\n        if isinstance(token_usage, int):\n            return None\n\n        # Handle dictionary token counts\n        if isinstance(token_usage, dict):\n            # First try to get input_tokens (Anthropic, LiteLLM style)\n            if \"input_tokens\" in token_usage:\n                return token_usage[\"input_tokens\"]\n\n            # Try prompt_tokens (OpenAI style)\n            if \"prompt_tokens\" in token_usage:\n                return token_usage[\"prompt_tokens\"]\n\n        # If we can't extract meaningful input token count, return None\n        return None\n\n    def _extract_output_tokens(self, token_usage: dict | int | None) -&gt; int | None:\n        \"\"\"Extract output tokens from various token usage formats.\n\n        This method standardizes output token counting across different providers:\n        - OpenAI/Azure: {\"prompt_tokens\": X, \"completion_tokens\": Y, \"total_tokens\": Z}\n        - Anthropic: {\"input_tokens\": X, \"output_tokens\": Y}\n        - Gemini: {\"prompt_tokens\": X, \"candidates_tokens\": Y, \"total_tokens\": Z}\n        - LiteLLM: {\"input_tokens\": X, \"output_tokens\": Y, \"total_tokens\": Z}\n        - Others: try to extract output/completion tokens\n\n        Args:\n        ----\n            token_usage: Token usage in various formats (dict, int, or None)\n\n        Returns:\n        -------\n            int | None: Output token count, or None if not available\n\n        \"\"\"\n        if token_usage is None:\n            return None\n\n        # Handle integer token counts (cannot distinguish input vs output)\n        if isinstance(token_usage, int):\n            return None\n\n        # Handle dictionary token counts\n        if isinstance(token_usage, dict):\n            # First try to get output_tokens (Anthropic, LiteLLM style)\n            if \"output_tokens\" in token_usage:\n                return token_usage[\"output_tokens\"]\n\n            # Try completion_tokens (OpenAI style)\n            if \"completion_tokens\" in token_usage:\n                return token_usage[\"completion_tokens\"]\n\n            # Try candidates_tokens (Gemini style)\n            if \"candidates_tokens\" in token_usage:\n                return token_usage[\"candidates_tokens\"]\n\n        # If we can't extract meaningful output token count, return None\n        return None\n\n    def compute_cumulative_token_usage(self) -&gt; dict:\n        \"\"\"Compute the token usage by looping over the messages.\n\n        Extracts token usage information from each message's usage_metadata and\n        computes running cumulative totals throughout the conversation.\n        Handles various token usage formats from different LLM providers.\n\n        Returns\n        -------\n            dict: Token usage information with lists of running totals:\n                - \"total_tokens\": list[int] - running total at each message\n                - \"input_tokens\": list[int] - running input total at each message\n                - \"output_tokens\": list[int] - running output total at each message\n\n        \"\"\"\n        # Initialize data structures\n        individual_usage = {\n            \"total_tokens\": [],\n            \"input_tokens\": [],\n            \"output_tokens\": [],\n        }\n\n        # Extract individual token counts for each AI message\n        for message in self.messages:\n            if isinstance(message, AIMessage):\n                usage_metadata = getattr(message, \"usage_metadata\", None)\n                individual_usage[\"total_tokens\"].append(self._extract_total_tokens(usage_metadata))\n                individual_usage[\"input_tokens\"].append(self._extract_input_tokens(usage_metadata))\n                individual_usage[\"output_tokens\"].append(self._extract_output_tokens(usage_metadata))\n\n        # Compute running cumulative totals for each message\n        per_message_cumulative = {\n            \"total_tokens\": [],\n            \"input_tokens\": [],\n            \"output_tokens\": [],\n        }\n\n        for token_type in [\"total_tokens\", \"input_tokens\", \"output_tokens\"]:\n            running_total = 0\n            for count in individual_usage[token_type]:\n                if count is not None:\n                    running_total += count\n                per_message_cumulative[token_type].append(running_total)\n\n        return per_message_cumulative\n\n    @abstractmethod\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; None:\n        \"\"\"Set the API key.\"\"\"\n\n    def get_prompts(self) -&gt; dict:\n        \"\"\"Get the prompts.\"\"\"\n        return self.prompts\n\n    def set_prompts(self, prompts: dict) -&gt; None:\n        \"\"\"Set the prompts.\"\"\"\n        self.prompts = prompts\n\n    def _tool_formatter(self, tools: list[Callable], mcp: bool = False) -&gt; str:\n        \"\"\"Format the tools. Only for model not supporting tool calling.\"\"\"\n        tools_description = \"\"\n\n        for idx, tool in enumerate(tools):\n            tools_description += f\"&lt;tool_{idx}&gt;\\n\"\n            tools_description += f\"Tool name: {tool.name}\\n\"\n            tools_description += f\"Tool description: {tool.description}\\n\"\n            if mcp:\n                tools_description += f\"Tool call schema:\\n {tool.tool_call_schema}\\n\"\n            else:\n                tools_description += f\"Tool call schema:\\n {tool.args}\\n\"\n            tools_description += f\"&lt;/tool_{idx}&gt;\\n\"\n        return tools_description\n\n    def _create_tool_prompt(\n        self, tools: list[Callable], additional_tools_instructions: str = None, mcp: bool = False\n    ) -&gt; str:\n        \"\"\"Create the tool prompt. Only for model not supporting tool calling.\"\"\"\n        prompt_template = ChatPromptTemplate.from_template(TOOL_USAGE_PROMPT)\n        tools_description = self._tool_formatter(tools, mcp=mcp)\n        new_message = prompt_template.invoke(\n            {\n                \"user_question\": self.messages[-1].content,\n                \"tools\": tools_description,\n                \"additional_tools_instructions\": additional_tools_instructions if additional_tools_instructions else \"\",\n            }\n        )\n        return new_message.messages[0]\n\n    def bind_tools(self, tools: list[Callable]) -&gt; None:\n        \"\"\"Bind tools to the chat.\"\"\"\n        # Check if the model supports tool calling\n        # (exploit the enum class in available_models.py)\n        if self.model_name in TOOL_CALLING_MODELS and self.ca_chat:\n            self.chat = self.chat.bind_tools(tools)\n            self.ca_chat = self.ca_chat.bind_tools(tools)\n\n        elif self.model_name in TOOL_CALLING_MODELS:\n            self.chat = self.chat.bind_tools(tools)\n\n        # elif self.model_name not in TOOL_CALLING_MODELS:\n        #    self.tools_prompt = self._create_tool_prompt(tools, additional_instructions)\n\n        # If not, fail gracefully\n        # raise ValueError(f\"Model {self.model_name} does not support tool calling.\")\n\n    def append_ai_message(self, message: str | AIMessage) -&gt; None:\n        \"\"\"Add a message from the AI to the conversation.\n\n        Args:\n        ----\n            message (str): The message from the AI.\n\n        \"\"\"\n        if isinstance(message, AIMessage):\n            self.messages.append(message)\n        elif isinstance(message, str):\n            self.messages.append(\n                AIMessage(\n                    content=message,\n                ),\n            )\n        else:\n            raise ValueError(f\"Invalid message type: {type(message)}\")\n\n    def append_system_message(self, message: str) -&gt; None:\n        \"\"\"Add a system message to the conversation.\n\n        Args:\n        ----\n            message (str): The system message.\n\n        \"\"\"\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n\n    def append_ca_message(self, message: str) -&gt; None:\n        \"\"\"Add a message to the correcting agent conversation.\n\n        Args:\n        ----\n            message (str): The message to the correcting agent.\n\n        \"\"\"\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n\n    def append_user_message(self, message: str) -&gt; None:\n        \"\"\"Add a message from the user to the conversation.\n\n        Args:\n        ----\n            message (str): The message from the user.\n\n        \"\"\"\n        self.messages.append(\n            HumanMessage(\n                content=message,\n            ),\n        )\n\n    def append_image_message(\n        self,\n        message: str,\n        image_url: str,\n        local: bool = False,\n    ) -&gt; None:\n        \"\"\"Add a user message with an image to the conversation.\n\n        Also checks, in addition to the `local` flag, if the image URL is a\n        local file path. If it is local, the image will be encoded as a base64\n        string to be passed to the LLM.\n\n        Args:\n        ----\n            message (str): The message from the user.\n            image_url (str): The URL of the image.\n            local (bool): Whether the image is local or not. If local, it will\n                be encoded as a base64 string to be passed to the LLM.\n\n        \"\"\"\n        parsed_url = urllib.parse.urlparse(image_url)\n        if local or not parsed_url.netloc:\n            image_url = f\"data:image/jpeg;base64,{encode_image(image_url)}\"\n        else:\n            image_url = f\"data:image/jpeg;base64,{encode_image_from_url(image_url)}\"\n\n        self.messages.append(\n            HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": message},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n                ],\n            ),\n        )\n\n    def setup(self, context: str) -&gt; None:\n        \"\"\"Set up the conversation with general prompts and a context.\"\"\"\n        for msg in self.prompts[\"primary_model_prompts\"]:\n            if msg:\n                self.append_system_message(msg)\n\n        for msg in self.prompts[\"correcting_agent_prompts\"]:\n            if msg:\n                self.append_ca_message(msg)\n\n        self.context = context\n        msg = f\"The topic of the research is {context}.\"\n        self.append_system_message(msg)\n\n    def setup_data_input_manual(self, data_input: str) -&gt; None:\n        \"\"\"Set up the data input manually.\"\"\"\n        self.data_input = data_input\n        msg = f\"The user has given information on the data input: {data_input}.\"\n        self.append_system_message(msg)\n\n    def setup_data_input_tool(self, df, input_file_name: str) -&gt; None:\n        \"\"\"Set up the data input tool.\"\"\"\n        self.data_input_tool = df\n\n        for tool_name in self.prompts[\"tool_prompts\"]:\n            if tool_name in input_file_name:\n                msg = self.prompts[\"tool_prompts\"][tool_name].format(df=df)\n                self.append_system_message(msg)\n\n    def query(\n        self,\n        text: str,\n        image_url: str | None = None,\n        structured_model: BaseModel | None = None,\n        wrap_structured_output: bool | None = None,\n        tools: list[Callable] | None = None,\n        explain_tool_result: bool | None = None,\n        additional_tools_instructions: str | None = None,\n        general_instructions_tool_interpretation: str | None = None,\n        additional_instructions_tool_interpretation: str | None = None,\n        mcp: bool | None = None,\n        return_tool_calls_as_ai_message: bool | None = None,\n        track_tool_calls: bool | None = None,\n        **kwargs,\n    ) -&gt; tuple[str, dict | None, str | None]:\n        \"\"\"Query the LLM API using the user's query.\n\n        Appends the most recent query to the conversation, optionally injects\n        context from the RAG agent, and runs the primary query method of the\n        child class.\n\n        Args:\n        ----\n            text (str): The user query.\n\n            image_url (str): The URL of an image to include in the conversation.\n                Optional and only supported for models with vision capabilities.\n\n            structured_model (BaseModel): The structured output model to use for the query.\n\n            wrap_structured_output (bool): Whether to wrap the structured output in JSON quotes.\n\n            tools (list[Callable]): The tools to use for the query.\n\n            explain_tool_result (bool): Whether to explain the tool result.\n\n            additional_tools_instructions (str): The additional instructions for the query.\n                Mainly used for tools that do not support tool calling.\n\n            general_instructions_tool_interpretation (str): The general\n                instructions for the tool interpretation.\n                Overrides the default prompt in `GENERAL_TOOL_RESULT_INTERPRETATION_PROMPT`.\n\n            additional_instructions_tool_interpretation (str): The additional\n                instructions for the tool interpretation.\n                Overrides the default prompt in `ADDITIONAL_TOOL_RESULT_INTERPRETATION_PROMPT`.\n\n            mcp (bool): If you want to use MCP mode, this should be set to True.\n\n            return_tool_calls_as_ai_message (bool): If you want to return the tool calls as an AI message, this should be set to True.\n\n            track_tool_calls (bool): If you want to track the tool calls, this should be set to True.\n\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n        -------\n            tuple: A tuple containing the response from the API, the token usage\n                information, and the correction if necessary/desired.\n\n        \"\"\"\n        if mcp:\n            self.mcp = True\n\n        # save the last human prompt that may be used for answer enhancement\n        self.last_human_prompt = text\n\n        # if additional_tools_instructions are provided, save them\n        if additional_tools_instructions:\n            self.additional_tools_instructions = additional_tools_instructions\n\n        # override the default prompts if other provided\n        self.general_instructions_tool_interpretation = (\n            general_instructions_tool_interpretation\n            if general_instructions_tool_interpretation\n            else GENERAL_TOOL_RESULT_INTERPRETATION_PROMPT\n        )\n        self.additional_instructions_tool_interpretation = (\n            additional_instructions_tool_interpretation\n            if additional_instructions_tool_interpretation\n            else ADDITIONAL_TOOL_RESULT_INTERPRETATION_PROMPT\n        )\n        if not image_url:\n            self.append_user_message(text)\n        else:\n            self.append_image_message(text, image_url)\n\n        self._inject_context(text)\n\n        # tools passed at this step are used only for this message\n        msg, token_usage = self._primary_query(\n            tools=tools,\n            explain_tool_result=explain_tool_result,\n            return_tool_calls_as_ai_message=return_tool_calls_as_ai_message,\n            structured_model=structured_model,\n            wrap_structured_output=wrap_structured_output,\n            track_tool_calls=track_tool_calls,\n        )\n\n        # case of structured output\n        if (token_usage == -1) and structured_model:\n            return (msg, 0, None)\n\n        if not token_usage:\n            # indicates error\n            return (msg, None, None)\n\n        if not self.correct:\n            return (msg, token_usage, None)\n\n        cor_msg = \"Correcting (using single sentences) ...\" if self.split_correction else \"Correcting ...\"\n\n        if st:\n            with st.spinner(cor_msg):\n                corrections = self._correct_query(text)\n        else:\n            corrections = self._correct_query(text)\n\n        if not corrections:\n            return (msg, token_usage, None)\n\n        correction = \"\\n\".join(corrections)\n        return (msg, token_usage, correction)\n\n    def _correct_query(self, msg: str) -&gt; list[str]:\n        corrections = []\n        if self.split_correction:\n            nltk.download(\"punkt\")\n            tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n            sentences = tokenizer.tokenize(msg)\n            for sentence in sentences:\n                correction = self._correct_response(sentence)\n\n                if str(correction).lower() not in [\"ok\", \"ok.\"]:\n                    corrections.append(correction)\n        else:\n            correction = self._correct_response(msg)\n\n            if str(correction).lower() not in [\"ok\", \"ok.\"]:\n                corrections.append(correction)\n\n        return corrections\n\n    @abstractmethod\n    def _primary_query(self, **kwargs) -&gt; tuple[str, dict | None]:\n        \"\"\"Run the primary query.\n\n        Args:\n        ----\n            **kwargs: Keyword arguments that may include:\n                - text: The user query.\n                - tools: List of tools for tool-calling models\n                - explain_tool_result: Whether to explain tool results\n                - return_tool_calls_as_ai_message: Whether to return tool calls as AI message\n                - structured_model: Structured output model\n                - wrap_structured_output: Whether to wrap structured output\n                - track_tool_calls: Whether to track tool calls\n                - Other model-specific parameters\n\n        Returns:\n        -------\n            tuple: A tuple containing the response message and token usage information.\n\n        \"\"\"\n\n    @abstractmethod\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response.\"\"\"\n\n    def _process_manual_tool_call(\n        self,\n        tool_call: list[dict],\n        available_tools: list[Callable],\n        explain_tool_result: bool = False,\n    ) -&gt; str:\n        \"\"\"Process manual tool calls from the model response.\n\n        This method handles the processing of tool calls for models that don't natively\n        support tool calling. It takes the parsed JSON response and executes the\n        appropriate tool.\n\n        Args:\n        ----\n            tool_call (list[dict]): The parsed tool call information from the model response.\n            available_tools (list[Callable]): The tools available for execution.\n            explain_tool_result (bool): Whether to explain the tool result.\n\n        Returns:\n        -------\n            str: The processed message containing the tool name, arguments, and result.\n\n        \"\"\"\n        tool_name = tool_call[\"tool_name\"]\n        tool_func = next((t for t in available_tools if t.name == tool_name), None)\n\n        # Remove the tool name from the tool call in order to invoke the tool\n        # This is beacause tool_name is not a valid argument for the tool\n        del tool_call[\"tool_name\"]\n\n        # Execute the tool based on whether we're in async context or not\n        if self.mcp:\n            loop = asyncio.get_running_loop()\n            tool_result = loop.run_until_complete(tool_func.ainvoke(tool_call))\n        else:\n            tool_result = tool_func.invoke(tool_call)\n\n        msg = f\"Tool: {tool_name}\\nArguments: {tool_call}\\nTool result: {tool_result}\"\n\n        if explain_tool_result:\n            tool_result_interpretation = self.chat.invoke(\n                TOOL_RESULT_INTERPRETATION_PROMPT.format(\n                    original_question=self.last_human_prompt,\n                    tool_result=tool_result,\n                    general_instructions=self.general_instructions_tool_interpretation,\n                    additional_instructions=self.additional_instructions_tool_interpretation,\n                )\n            )\n            msg += f\"\\nTool result interpretation: {tool_result_interpretation.content}\"\n\n        self.append_ai_message(msg)\n\n        return msg\n\n    def _process_tool_calls(\n        self,\n        tool_calls: list[dict],\n        available_tools: list[Callable],\n        response_content: str,\n        explain_tool_result: bool = False,\n        return_tool_calls_as_ai_message: bool = False,\n        track_tool_calls: bool = False,\n    ) -&gt; str:\n        \"\"\"Process tool calls from the model response.\n\n        This method handles the processing of tool calls returned by the model.\n        It can either automatically execute the tools and return their results,\n        or format the tool calls as text.\n\n        Args:\n        ----\n            tool_calls: The tool calls from the model response.\n            response_content: The text content of the response (used as fallback).\n            available_tools: The tools available in the chat.\n            explain_tool_result (bool): Whether to explain the tool result.\n            return_tool_calls_as_ai_message (bool): If you want to return the tool calls as an AI message, this should be set to True.\n            track_tool_calls (bool): If you want to track the tool calls, this should be set to True.\n\n        Returns:\n        -------\n            str: The processed message, either tool results or formatted tool calls.\n\n        \"\"\"\n        if not tool_calls:\n            return response_content\n\n        msg = \"\"\n\n        if self.tool_call_mode == \"auto\":\n            # Collect tool results for collective explanation when multiple tools are called\n            tool_results_for_explanation = []\n\n            for idx, tool_call in enumerate(tool_calls):\n                # Extract tool name and arguments\n                tool_name = tool_call[\"name\"]\n                tool_args = tool_call[\"args\"]\n                tool_call_id = tool_call[\"id\"]\n\n                # Find the matching tool function\n                tool_func = next((t for t in available_tools if t.name == tool_name), None)\n\n                if tool_func:\n                    # Execute the tool\n                    try:\n                        if self.mcp:\n                            loop = asyncio.get_running_loop()\n                            tool_result = loop.run_until_complete(tool_func.ainvoke(tool_args))\n                        else:\n                            tool_result = tool_func.invoke(tool_args)\n                        # Add the tool result to the conversation\n                        if return_tool_calls_as_ai_message:\n                            self.append_ai_message(f\"Tool call ({tool_name}) \\nResult: {tool_result!s}\")\n                        else:\n                            self.messages.append(\n                                ToolMessage(content=str(tool_result), name=tool_name, tool_call_id=tool_call_id)\n                            )\n\n                        if track_tool_calls:\n                            self.tool_calls.append(\n                                {\"name\": tool_name, \"args\": tool_args, \"id\": tool_call_id, \"result\": tool_result}\n                            )\n\n                        if idx &gt; 0:\n                            msg += \"\\n\"\n                        msg += f\"Tool call ({tool_name}) result: {tool_result!s}\"\n\n                        # Collect tool results for explanation if needed\n                        if explain_tool_result:\n                            tool_results_for_explanation.append(\n                                {\"name\": tool_name, \"args\": tool_args, \"result\": tool_result}\n                            )\n\n                    except Exception as e:\n                        # Handle tool execution errors\n                        error_message = f\"Error executing tool {tool_name}: {e!s}\"\n                        self.messages.append(\n                            ToolMessage(content=error_message, name=tool_name, tool_call_id=tool_call_id)\n                        )\n\n                        # Track failed tool calls\n                        if track_tool_calls:\n                            self.tool_calls.append(\n                                {\"name\": tool_name, \"args\": tool_args, \"id\": tool_call_id, \"error\": str(e)}\n                            )\n\n                        if idx &gt; 0:\n                            msg += \"\\n\"\n                        msg += error_message\n                # Handle missing/unknown tool\n                elif track_tool_calls:\n                    self.tool_calls.append(\n                        {\"name\": tool_name, \"args\": tool_args, \"id\": tool_call_id, \"error\": \"Tool not found\"}\n                    )\n\n            # Handle tool result explanation\n            if explain_tool_result and tool_results_for_explanation:\n                if len(tool_results_for_explanation) &gt; 1:\n                    # Multiple tools: explain all results together\n                    combined_tool_results = \"\\n\\n\".join(\n                        [\n                            f\"Tool: {tr['name']}\\nArguments: {tr['args']}\\nResult: {tr['result']}\"\n                            for tr in tool_results_for_explanation\n                        ]\n                    )\n\n                    tool_result_interpretation = self.chat.invoke(\n                        TOOL_RESULT_INTERPRETATION_PROMPT.format(\n                            original_question=self.last_human_prompt,\n                            tool_result=combined_tool_results,\n                            general_instructions=self.general_instructions_tool_interpretation,\n                            additional_instructions=self.additional_instructions_tool_interpretation,\n                        )\n                    )\n                    self.messages.append(tool_result_interpretation)\n                    msg += f\"\\nTool results interpretation: {tool_result_interpretation.content}\"\n                else:\n                    # Single tool: explain individual result (maintain current behavior)\n                    tool_result_data = tool_results_for_explanation[0]\n                    tool_result_interpretation = self.chat.invoke(\n                        TOOL_RESULT_INTERPRETATION_PROMPT.format(\n                            original_question=self.last_human_prompt,\n                            tool_result=tool_result_data[\"result\"],\n                            general_instructions=self.general_instructions_tool_interpretation,\n                            additional_instructions=self.additional_instructions_tool_interpretation,\n                        )\n                    )\n                    self.messages.append(tool_result_interpretation)\n                    msg += f\"\\nTool result interpretation: {tool_result_interpretation.content}\"\n\n            return msg\n\n        if self.tool_call_mode == \"text\":\n            # Join all tool calls in a text format\n            tool_calls_text = []\n            for tool_call in tool_calls:\n                tool_name = tool_call[\"name\"]\n                tool_args = tool_call[\"args\"]\n                tool_call_id = tool_call[\"id\"]\n                tool_calls_text.append(f\"Tool: {tool_name} - Arguments: {tool_args} - Tool call id: {tool_call_id}\")\n\n            # Join with line breaks and set as the message\n            msg = \"\\n\".join(tool_calls_text)\n\n            # Append the formatted tool calls as an AI message\n            self.append_ai_message(msg)\n            return msg\n\n        # Invalid tool call mode, log warning and return original content\n        logger.warning(f\"Invalid tool call mode: {self.tool_call_mode}. Using original response content.\")\n        return response_content\n\n    def _inject_context_by_ragagent_selector(self, text: str) -&gt; list[str]:\n        \"\"\"Inject the context generated by RagAgentSelector.\n\n        The RagAgentSelector will choose the appropriate rag agent to generate\n        context according to user's question.\n\n        Args:\n        ----\n            text (str): The user query to be used for choosing rag agent\n\n        \"\"\"\n        rag_agents: list[RagAgent] = [agent for agent in self.rag_agents if agent.use_prompt]\n        decider_agent = RagAgentSelector(\n            rag_agents=rag_agents,\n            conversation_factory=lambda: self,\n        )\n        result = decider_agent.execute(text)\n        if result.tool_result is not None and len(result.tool_result) &gt; 0:\n            return result.tool_result\n        # find rag agent selected\n        rag_agent = next(\n            [agent for agent in rag_agents if agent.mode == result.answer],\n            None,\n        )\n        if rag_agent is None:\n            return None\n        return rag_agent.generate_responses(text)\n\n    def _inject_context(self, text: str) -&gt; None:\n        \"\"\"Inject the context received from the RAG agent into the prompt.\n\n        The RAG agent will find the most similar n text fragments and add them\n        to the message history object for usage in the next prompt. Uses the\n        document summarisation prompt set to inject the context. The ultimate\n        prompt should include the placeholder for the statements, `{statements}`\n        (used for formatting the string).\n\n        Args:\n        ----\n            text (str): The user query to be used for similarity search.\n\n        \"\"\"\n        sim_msg = \"Performing similarity search to inject fragments ...\"\n\n        if st:\n            with st.spinner(sim_msg):\n                statements = []\n                if self.use_ragagent_selector:\n                    statements = self._inject_context_by_ragagent_selector(text)\n                else:\n                    for agent in self.rag_agents:\n                        try:\n                            docs = agent.generate_responses(text)\n                            statements = statements + [doc[0] for doc in docs]\n                        except ValueError as e:\n                            logger.warning(e)\n\n        else:\n            statements = []\n            if self.use_ragagent_selector:\n                statements = self._inject_context_by_ragagent_selector(text)\n            else:\n                for agent in self.rag_agents:\n                    try:\n                        docs = agent.generate_responses(text)\n                        statements = statements + [doc[0] for doc in docs]\n                    except ValueError as e:\n                        logger.warning(e)\n\n        if statements and len(statements) &gt; 0:\n            prompts = self.prompts[\"rag_agent_prompts\"]\n            self.current_statements = statements\n            for i, prompt in enumerate(prompts):\n                # if last prompt, format the statements into the prompt\n                if i == len(prompts) - 1:\n                    self.append_system_message(\n                        prompt.format(statements=statements),\n                    )\n                else:\n                    self.append_system_message(prompt)\n\n    def get_last_injected_context(self) -&gt; list[dict]:\n        \"\"\"Get a formatted list of the last context.\n\n        Get the last context injected into the conversation. Contains one\n        dictionary for each RAG mode.\n\n        Returns\n        -------\n            List[dict]: A list of dictionaries containing the mode and context\n            for each RAG agent.\n\n        \"\"\"\n        return [{\"mode\": agent.mode, \"context\": agent.last_response} for agent in self.rag_agents]\n\n    def get_msg_json(self) -&gt; str:\n        \"\"\"Return a JSON representation of the conversation.\n\n        Returns a list of dicts of the messages in the conversation in JSON\n        format. The keys of the dicts are the roles, the values are the\n        messages.\n\n        Returns\n        -------\n            str: A JSON representation of the messages in the conversation.\n\n        \"\"\"\n        d = []\n        for msg in self.messages:\n            if isinstance(msg, SystemMessage):\n                role = \"system\"\n            elif isinstance(msg, HumanMessage):\n                role = \"user\"\n            elif isinstance(msg, AIMessage):\n                role = \"ai\"\n            else:\n                error_msg = f\"Unknown message type: {type(msg)}\"\n                raise TypeError(error_msg)\n\n            d.append({role: msg.content})\n\n        return json.dumps(d)\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset the conversation to the initial state.\"\"\"\n        self.history = []\n        self.messages = []\n        self.ca_messages = []\n        self.current_statements = []\n        self.tool_calls.clear()\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.ca_chat","title":"<code>ca_chat</code>  <code>property</code> <code>writable</code>","text":"<p>Access the correcting agent chat attribute with error handling.</p>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.chat","title":"<code>chat</code>  <code>property</code> <code>writable</code>","text":"<p>Access the chat attribute with error handling.</p>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.use_ragagent_selector","title":"<code>use_ragagent_selector</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to use the ragagent selector.</p>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.append_ai_message","title":"<code>append_ai_message(message)</code>","text":"<p>Add a message from the AI to the conversation.</p> <pre><code>message (str): The message from the AI.\n</code></pre> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def append_ai_message(self, message: str | AIMessage) -&gt; None:\n    \"\"\"Add a message from the AI to the conversation.\n\n    Args:\n    ----\n        message (str): The message from the AI.\n\n    \"\"\"\n    if isinstance(message, AIMessage):\n        self.messages.append(message)\n    elif isinstance(message, str):\n        self.messages.append(\n            AIMessage(\n                content=message,\n            ),\n        )\n    else:\n        raise ValueError(f\"Invalid message type: {type(message)}\")\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.append_ca_message","title":"<code>append_ca_message(message)</code>","text":"<p>Add a message to the correcting agent conversation.</p> <pre><code>message (str): The message to the correcting agent.\n</code></pre> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def append_ca_message(self, message: str) -&gt; None:\n    \"\"\"Add a message to the correcting agent conversation.\n\n    Args:\n    ----\n        message (str): The message to the correcting agent.\n\n    \"\"\"\n    self.ca_messages.append(\n        SystemMessage(\n            content=message,\n        ),\n    )\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.append_image_message","title":"<code>append_image_message(message, image_url, local=False)</code>","text":"<p>Add a user message with an image to the conversation.</p> <p>Also checks, in addition to the <code>local</code> flag, if the image URL is a local file path. If it is local, the image will be encoded as a base64 string to be passed to the LLM.</p> <pre><code>message (str): The message from the user.\nimage_url (str): The URL of the image.\nlocal (bool): Whether the image is local or not. If local, it will\n    be encoded as a base64 string to be passed to the LLM.\n</code></pre> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def append_image_message(\n    self,\n    message: str,\n    image_url: str,\n    local: bool = False,\n) -&gt; None:\n    \"\"\"Add a user message with an image to the conversation.\n\n    Also checks, in addition to the `local` flag, if the image URL is a\n    local file path. If it is local, the image will be encoded as a base64\n    string to be passed to the LLM.\n\n    Args:\n    ----\n        message (str): The message from the user.\n        image_url (str): The URL of the image.\n        local (bool): Whether the image is local or not. If local, it will\n            be encoded as a base64 string to be passed to the LLM.\n\n    \"\"\"\n    parsed_url = urllib.parse.urlparse(image_url)\n    if local or not parsed_url.netloc:\n        image_url = f\"data:image/jpeg;base64,{encode_image(image_url)}\"\n    else:\n        image_url = f\"data:image/jpeg;base64,{encode_image_from_url(image_url)}\"\n\n    self.messages.append(\n        HumanMessage(\n            content=[\n                {\"type\": \"text\", \"text\": message},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n            ],\n        ),\n    )\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.append_system_message","title":"<code>append_system_message(message)</code>","text":"<p>Add a system message to the conversation.</p> <pre><code>message (str): The system message.\n</code></pre> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def append_system_message(self, message: str) -&gt; None:\n    \"\"\"Add a system message to the conversation.\n\n    Args:\n    ----\n        message (str): The system message.\n\n    \"\"\"\n    self.messages.append(\n        SystemMessage(\n            content=message,\n        ),\n    )\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.append_user_message","title":"<code>append_user_message(message)</code>","text":"<p>Add a message from the user to the conversation.</p> <pre><code>message (str): The message from the user.\n</code></pre> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def append_user_message(self, message: str) -&gt; None:\n    \"\"\"Add a message from the user to the conversation.\n\n    Args:\n    ----\n        message (str): The message from the user.\n\n    \"\"\"\n    self.messages.append(\n        HumanMessage(\n            content=message,\n        ),\n    )\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.bind_tools","title":"<code>bind_tools(tools)</code>","text":"<p>Bind tools to the chat.</p> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def bind_tools(self, tools: list[Callable]) -&gt; None:\n    \"\"\"Bind tools to the chat.\"\"\"\n    # Check if the model supports tool calling\n    # (exploit the enum class in available_models.py)\n    if self.model_name in TOOL_CALLING_MODELS and self.ca_chat:\n        self.chat = self.chat.bind_tools(tools)\n        self.ca_chat = self.ca_chat.bind_tools(tools)\n\n    elif self.model_name in TOOL_CALLING_MODELS:\n        self.chat = self.chat.bind_tools(tools)\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.compute_cumulative_token_usage","title":"<code>compute_cumulative_token_usage()</code>","text":"<p>Compute the token usage by looping over the messages.</p> <p>Extracts token usage information from each message's usage_metadata and computes running cumulative totals throughout the conversation. Handles various token usage formats from different LLM providers.</p>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.compute_cumulative_token_usage--returns","title":"Returns","text":"<pre><code>dict: Token usage information with lists of running totals:\n    - \"total_tokens\": list[int] - running total at each message\n    - \"input_tokens\": list[int] - running input total at each message\n    - \"output_tokens\": list[int] - running output total at each message\n</code></pre> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def compute_cumulative_token_usage(self) -&gt; dict:\n    \"\"\"Compute the token usage by looping over the messages.\n\n    Extracts token usage information from each message's usage_metadata and\n    computes running cumulative totals throughout the conversation.\n    Handles various token usage formats from different LLM providers.\n\n    Returns\n    -------\n        dict: Token usage information with lists of running totals:\n            - \"total_tokens\": list[int] - running total at each message\n            - \"input_tokens\": list[int] - running input total at each message\n            - \"output_tokens\": list[int] - running output total at each message\n\n    \"\"\"\n    # Initialize data structures\n    individual_usage = {\n        \"total_tokens\": [],\n        \"input_tokens\": [],\n        \"output_tokens\": [],\n    }\n\n    # Extract individual token counts for each AI message\n    for message in self.messages:\n        if isinstance(message, AIMessage):\n            usage_metadata = getattr(message, \"usage_metadata\", None)\n            individual_usage[\"total_tokens\"].append(self._extract_total_tokens(usage_metadata))\n            individual_usage[\"input_tokens\"].append(self._extract_input_tokens(usage_metadata))\n            individual_usage[\"output_tokens\"].append(self._extract_output_tokens(usage_metadata))\n\n    # Compute running cumulative totals for each message\n    per_message_cumulative = {\n        \"total_tokens\": [],\n        \"input_tokens\": [],\n        \"output_tokens\": [],\n    }\n\n    for token_type in [\"total_tokens\", \"input_tokens\", \"output_tokens\"]:\n        running_total = 0\n        for count in individual_usage[token_type]:\n            if count is not None:\n                running_total += count\n            per_message_cumulative[token_type].append(running_total)\n\n    return per_message_cumulative\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.find_rag_agent","title":"<code>find_rag_agent(mode)</code>","text":"<p>Find the rag_agent with the given mode.</p> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def find_rag_agent(self, mode: str) -&gt; tuple[int, RagAgent]:\n    \"\"\"Find the rag_agent with the given mode.\"\"\"\n    for i, val in enumerate(self.rag_agents):\n        if val.mode == mode:\n            return i, val\n    return -1, None\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.get_last_injected_context","title":"<code>get_last_injected_context()</code>","text":"<p>Get a formatted list of the last context.</p> <p>Get the last context injected into the conversation. Contains one dictionary for each RAG mode.</p>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.get_last_injected_context--returns","title":"Returns","text":"<pre><code>List[dict]: A list of dictionaries containing the mode and context\nfor each RAG agent.\n</code></pre> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def get_last_injected_context(self) -&gt; list[dict]:\n    \"\"\"Get a formatted list of the last context.\n\n    Get the last context injected into the conversation. Contains one\n    dictionary for each RAG mode.\n\n    Returns\n    -------\n        List[dict]: A list of dictionaries containing the mode and context\n        for each RAG agent.\n\n    \"\"\"\n    return [{\"mode\": agent.mode, \"context\": agent.last_response} for agent in self.rag_agents]\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.get_msg_json","title":"<code>get_msg_json()</code>","text":"<p>Return a JSON representation of the conversation.</p> <p>Returns a list of dicts of the messages in the conversation in JSON format. The keys of the dicts are the roles, the values are the messages.</p>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.get_msg_json--returns","title":"Returns","text":"<pre><code>str: A JSON representation of the messages in the conversation.\n</code></pre> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def get_msg_json(self) -&gt; str:\n    \"\"\"Return a JSON representation of the conversation.\n\n    Returns a list of dicts of the messages in the conversation in JSON\n    format. The keys of the dicts are the roles, the values are the\n    messages.\n\n    Returns\n    -------\n        str: A JSON representation of the messages in the conversation.\n\n    \"\"\"\n    d = []\n    for msg in self.messages:\n        if isinstance(msg, SystemMessage):\n            role = \"system\"\n        elif isinstance(msg, HumanMessage):\n            role = \"user\"\n        elif isinstance(msg, AIMessage):\n            role = \"ai\"\n        else:\n            error_msg = f\"Unknown message type: {type(msg)}\"\n            raise TypeError(error_msg)\n\n        d.append({role: msg.content})\n\n    return json.dumps(d)\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.get_prompts","title":"<code>get_prompts()</code>","text":"<p>Get the prompts.</p> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def get_prompts(self) -&gt; dict:\n    \"\"\"Get the prompts.\"\"\"\n    return self.prompts\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.query","title":"<code>query(text, image_url=None, structured_model=None, wrap_structured_output=None, tools=None, explain_tool_result=None, additional_tools_instructions=None, general_instructions_tool_interpretation=None, additional_instructions_tool_interpretation=None, mcp=None, return_tool_calls_as_ai_message=None, track_tool_calls=None, **kwargs)</code>","text":"<p>Query the LLM API using the user's query.</p> <p>Appends the most recent query to the conversation, optionally injects context from the RAG agent, and runs the primary query method of the child class.</p> <pre><code>text (str): The user query.\n\nimage_url (str): The URL of an image to include in the conversation.\n    Optional and only supported for models with vision capabilities.\n\nstructured_model (BaseModel): The structured output model to use for the query.\n\nwrap_structured_output (bool): Whether to wrap the structured output in JSON quotes.\n\ntools (list[Callable]): The tools to use for the query.\n\nexplain_tool_result (bool): Whether to explain the tool result.\n\nadditional_tools_instructions (str): The additional instructions for the query.\n    Mainly used for tools that do not support tool calling.\n\ngeneral_instructions_tool_interpretation (str): The general\n    instructions for the tool interpretation.\n    Overrides the default prompt in `GENERAL_TOOL_RESULT_INTERPRETATION_PROMPT`.\n\nadditional_instructions_tool_interpretation (str): The additional\n    instructions for the tool interpretation.\n    Overrides the default prompt in `ADDITIONAL_TOOL_RESULT_INTERPRETATION_PROMPT`.\n\nmcp (bool): If you want to use MCP mode, this should be set to True.\n\nreturn_tool_calls_as_ai_message (bool): If you want to return the tool calls as an AI message, this should be set to True.\n\ntrack_tool_calls (bool): If you want to track the tool calls, this should be set to True.\n\n**kwargs: Additional keyword arguments.\n</code></pre> <pre><code>tuple: A tuple containing the response from the API, the token usage\n    information, and the correction if necessary/desired.\n</code></pre> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def query(\n    self,\n    text: str,\n    image_url: str | None = None,\n    structured_model: BaseModel | None = None,\n    wrap_structured_output: bool | None = None,\n    tools: list[Callable] | None = None,\n    explain_tool_result: bool | None = None,\n    additional_tools_instructions: str | None = None,\n    general_instructions_tool_interpretation: str | None = None,\n    additional_instructions_tool_interpretation: str | None = None,\n    mcp: bool | None = None,\n    return_tool_calls_as_ai_message: bool | None = None,\n    track_tool_calls: bool | None = None,\n    **kwargs,\n) -&gt; tuple[str, dict | None, str | None]:\n    \"\"\"Query the LLM API using the user's query.\n\n    Appends the most recent query to the conversation, optionally injects\n    context from the RAG agent, and runs the primary query method of the\n    child class.\n\n    Args:\n    ----\n        text (str): The user query.\n\n        image_url (str): The URL of an image to include in the conversation.\n            Optional and only supported for models with vision capabilities.\n\n        structured_model (BaseModel): The structured output model to use for the query.\n\n        wrap_structured_output (bool): Whether to wrap the structured output in JSON quotes.\n\n        tools (list[Callable]): The tools to use for the query.\n\n        explain_tool_result (bool): Whether to explain the tool result.\n\n        additional_tools_instructions (str): The additional instructions for the query.\n            Mainly used for tools that do not support tool calling.\n\n        general_instructions_tool_interpretation (str): The general\n            instructions for the tool interpretation.\n            Overrides the default prompt in `GENERAL_TOOL_RESULT_INTERPRETATION_PROMPT`.\n\n        additional_instructions_tool_interpretation (str): The additional\n            instructions for the tool interpretation.\n            Overrides the default prompt in `ADDITIONAL_TOOL_RESULT_INTERPRETATION_PROMPT`.\n\n        mcp (bool): If you want to use MCP mode, this should be set to True.\n\n        return_tool_calls_as_ai_message (bool): If you want to return the tool calls as an AI message, this should be set to True.\n\n        track_tool_calls (bool): If you want to track the tool calls, this should be set to True.\n\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n    -------\n        tuple: A tuple containing the response from the API, the token usage\n            information, and the correction if necessary/desired.\n\n    \"\"\"\n    if mcp:\n        self.mcp = True\n\n    # save the last human prompt that may be used for answer enhancement\n    self.last_human_prompt = text\n\n    # if additional_tools_instructions are provided, save them\n    if additional_tools_instructions:\n        self.additional_tools_instructions = additional_tools_instructions\n\n    # override the default prompts if other provided\n    self.general_instructions_tool_interpretation = (\n        general_instructions_tool_interpretation\n        if general_instructions_tool_interpretation\n        else GENERAL_TOOL_RESULT_INTERPRETATION_PROMPT\n    )\n    self.additional_instructions_tool_interpretation = (\n        additional_instructions_tool_interpretation\n        if additional_instructions_tool_interpretation\n        else ADDITIONAL_TOOL_RESULT_INTERPRETATION_PROMPT\n    )\n    if not image_url:\n        self.append_user_message(text)\n    else:\n        self.append_image_message(text, image_url)\n\n    self._inject_context(text)\n\n    # tools passed at this step are used only for this message\n    msg, token_usage = self._primary_query(\n        tools=tools,\n        explain_tool_result=explain_tool_result,\n        return_tool_calls_as_ai_message=return_tool_calls_as_ai_message,\n        structured_model=structured_model,\n        wrap_structured_output=wrap_structured_output,\n        track_tool_calls=track_tool_calls,\n    )\n\n    # case of structured output\n    if (token_usage == -1) and structured_model:\n        return (msg, 0, None)\n\n    if not token_usage:\n        # indicates error\n        return (msg, None, None)\n\n    if not self.correct:\n        return (msg, token_usage, None)\n\n    cor_msg = \"Correcting (using single sentences) ...\" if self.split_correction else \"Correcting ...\"\n\n    if st:\n        with st.spinner(cor_msg):\n            corrections = self._correct_query(text)\n    else:\n        corrections = self._correct_query(text)\n\n    if not corrections:\n        return (msg, token_usage, None)\n\n    correction = \"\\n\".join(corrections)\n    return (msg, token_usage, correction)\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.reset","title":"<code>reset()</code>","text":"<p>Reset the conversation to the initial state.</p> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the conversation to the initial state.\"\"\"\n    self.history = []\n    self.messages = []\n    self.ca_messages = []\n    self.current_statements = []\n    self.tool_calls.clear()\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>  <code>abstractmethod</code>","text":"<p>Set the API key.</p> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>@abstractmethod\ndef set_api_key(self, api_key: str, user: str | None = None) -&gt; None:\n    \"\"\"Set the API key.\"\"\"\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.set_prompts","title":"<code>set_prompts(prompts)</code>","text":"<p>Set the prompts.</p> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def set_prompts(self, prompts: dict) -&gt; None:\n    \"\"\"Set the prompts.\"\"\"\n    self.prompts = prompts\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.set_rag_agent","title":"<code>set_rag_agent(agent)</code>","text":"<p>Update or insert rag_agent.</p> <p>If the rag_agent with the same mode already exists, it will be updated. Otherwise, the new rag_agent will be inserted.</p> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def set_rag_agent(self, agent: RagAgent) -&gt; None:\n    \"\"\"Update or insert rag_agent.\n\n    If the rag_agent with the same mode already exists, it will be updated.\n    Otherwise, the new rag_agent will be inserted.\n    \"\"\"\n    i, _ = self.find_rag_agent(agent.mode)\n    if i &lt; 0:\n        # insert\n        self.rag_agents.append(agent)\n    else:\n        # update\n        self.rag_agents[i] = agent\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.set_user_name","title":"<code>set_user_name(user_name)</code>","text":"<p>Set the user name.</p> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def set_user_name(self, user_name: str) -&gt; None:\n    \"\"\"Set the user name.\"\"\"\n    self.user_name = user_name\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.setup","title":"<code>setup(context)</code>","text":"<p>Set up the conversation with general prompts and a context.</p> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def setup(self, context: str) -&gt; None:\n    \"\"\"Set up the conversation with general prompts and a context.\"\"\"\n    for msg in self.prompts[\"primary_model_prompts\"]:\n        if msg:\n            self.append_system_message(msg)\n\n    for msg in self.prompts[\"correcting_agent_prompts\"]:\n        if msg:\n            self.append_ca_message(msg)\n\n    self.context = context\n    msg = f\"The topic of the research is {context}.\"\n    self.append_system_message(msg)\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.setup_data_input_manual","title":"<code>setup_data_input_manual(data_input)</code>","text":"<p>Set up the data input manually.</p> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def setup_data_input_manual(self, data_input: str) -&gt; None:\n    \"\"\"Set up the data input manually.\"\"\"\n    self.data_input = data_input\n    msg = f\"The user has given information on the data input: {data_input}.\"\n    self.append_system_message(msg)\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.Conversation.setup_data_input_tool","title":"<code>setup_data_input_tool(df, input_file_name)</code>","text":"<p>Set up the data input tool.</p> Source code in <code>biochatter/llm_connect/conversation.py</code> <pre><code>def setup_data_input_tool(self, df, input_file_name: str) -&gt; None:\n    \"\"\"Set up the data input tool.\"\"\"\n    self.data_input_tool = df\n\n    for tool_name in self.prompts[\"tool_prompts\"]:\n        if tool_name in input_file_name:\n            msg = self.prompts[\"tool_prompts\"][tool_name].format(df=df)\n            self.append_system_message(msg)\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.GeminiConversation","title":"<code>GeminiConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the Google Gemini model.</p> Source code in <code>biochatter/llm_connect/gemini.py</code> <pre><code>class GeminiConversation(Conversation):\n    \"\"\"Conversation class for the Google Gemini model.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n        tools: list[Callable] = None,\n        tool_call_mode: Literal[\"auto\", \"text\"] = \"auto\",\n    ) -&gt; None:\n        \"\"\"Initialise the GeminiConversation class.\n\n        Connect to Google's Gemini API and set up a conversation with the user.\n        Also initialise a second conversational agent to provide corrections to\n        the model output, if necessary.\n\n        Args:\n        ----\n            model_name (str): The name of the model to use.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            correct (bool): Whether to correct the model output.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n\n            tools (list[Callable]): List of tool functions to use with the model.\n\n            tool_call_mode (str): The mode to use for tool calls.\n                \"auto\": Automatically call tools.\n                \"text\": Only return text output of the tool call.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n            tools=tools,\n            tool_call_mode=tool_call_mode,\n        )\n\n        self.ca_model_name = \"gemini-2.0-flash\"\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the Google Gemini API.\n\n        If the key is valid, initialise the conversational agent. Optionally set\n        the user for usage statistics.\n\n        Args:\n        ----\n            api_key (str): The API key for the Google Gemini API.\n\n            user (str, optional): The user for usage statistics. If provided and\n                equals \"community\", will track usage stats.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        self.user = user\n\n        try:\n            self.chat = ChatGoogleGenerativeAI(\n                model=self.model_name,\n                temperature=0,\n                google_api_key=api_key,\n            )\n            self.ca_chat = ChatGoogleGenerativeAI(\n                model=self.ca_model_name,\n                temperature=0,\n                google_api_key=api_key,\n            )\n\n            # if binding happens here, tools will be available for all messages\n            if self.tools:\n                self.bind_tools(self.tools)\n\n            return True\n\n        except Exception:  # Google Genai doesn't expose specific exception types\n            self._chat = None\n            self._ca_chat = None\n            return False\n\n    def _primary_query(self, tools: list[Callable] | None = None, **kwargs) -&gt; tuple:\n        \"\"\"Query the Google Gemini API with the user's message.\n\n        Return the response using the message history (flattery system messages,\n        prior conversation) as context. Correct the response if necessary.\n\n        Args:\n        ----\n            tools (list[Callable]): The tools to use for the query. Tools\n            passed at this step are used only for this message and not stored\n            as part of the conversation object.\n\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n        -------\n            tuple: A tuple containing the response from the Gemini API and\n                the token usage.\n\n        \"\"\"\n        if kwargs:\n            kwargs.pop(\"tools\", None)\n            warnings.warn(f\"Warning: {kwargs} are not used by this class\", UserWarning)\n\n        # bind tools to the chat if provided in the query\n        chat = self.chat.bind_tools(tools) if (tools and self.model_name in TOOL_CALLING_MODELS) else self.chat\n\n        try:\n            response = chat.invoke(self.messages)\n        except Exception as e:\n            return str(e), None\n\n        # Process tool calls if present\n        if response.tool_calls:\n            msg = self._process_tool_calls(response.tool_calls, tools, response.content)\n        else:\n            msg = response.content\n            self.append_ai_message(msg)\n\n        token_usage_raw = response.usage_metadata\n        token_usage = self._extract_total_tokens(token_usage_raw)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response from the Gemini API.\n\n        Send the response to a secondary language model. Optionally split the\n        response into single sentences and correct each sentence individually.\n        Update usage stats.\n\n        Args:\n        ----\n            msg (str): The response from the Gemini API.\n\n        Returns:\n        -------\n            str: The corrected response (or OK if no correction necessary).\n\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n            ),\n        )\n\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n\n        response = self.ca_chat.invoke(ca_messages)\n\n        correction = response.content\n        token_usage_raw = response.usage_metadata\n        token_usage = self._extract_total_tokens(token_usage_raw)\n\n        return correction\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.GeminiConversation.__init__","title":"<code>__init__(model_name, prompts, correct=False, split_correction=False, tools=None, tool_call_mode='auto')</code>","text":"<p>Initialise the GeminiConversation class.</p> <p>Connect to Google's Gemini API and set up a conversation with the user. Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <pre><code>model_name (str): The name of the model to use.\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\ncorrect (bool): Whether to correct the model output.\n\nsplit_correction (bool): Whether to correct the model output by\n    splitting the output into sentences and correcting each\n    sentence individually.\n\ntools (list[Callable]): List of tool functions to use with the model.\n\ntool_call_mode (str): The mode to use for tool calls.\n    \"auto\": Automatically call tools.\n    \"text\": Only return text output of the tool call.\n</code></pre> Source code in <code>biochatter/llm_connect/gemini.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = False,\n    split_correction: bool = False,\n    tools: list[Callable] = None,\n    tool_call_mode: Literal[\"auto\", \"text\"] = \"auto\",\n) -&gt; None:\n    \"\"\"Initialise the GeminiConversation class.\n\n    Connect to Google's Gemini API and set up a conversation with the user.\n    Also initialise a second conversational agent to provide corrections to\n    the model output, if necessary.\n\n    Args:\n    ----\n        model_name (str): The name of the model to use.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        correct (bool): Whether to correct the model output.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n\n        tools (list[Callable]): List of tool functions to use with the model.\n\n        tool_call_mode (str): The mode to use for tool calls.\n            \"auto\": Automatically call tools.\n            \"text\": Only return text output of the tool call.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n        tools=tools,\n        tool_call_mode=tool_call_mode,\n    )\n\n    self.ca_model_name = \"gemini-2.0-flash\"\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.GeminiConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the Google Gemini API.</p> <p>If the key is valid, initialise the conversational agent. Optionally set the user for usage statistics.</p> <pre><code>api_key (str): The API key for the Google Gemini API.\n\nuser (str, optional): The user for usage statistics. If provided and\n    equals \"community\", will track usage stats.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/llm_connect/gemini.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the Google Gemini API.\n\n    If the key is valid, initialise the conversational agent. Optionally set\n    the user for usage statistics.\n\n    Args:\n    ----\n        api_key (str): The API key for the Google Gemini API.\n\n        user (str, optional): The user for usage statistics. If provided and\n            equals \"community\", will track usage stats.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    self.user = user\n\n    try:\n        self.chat = ChatGoogleGenerativeAI(\n            model=self.model_name,\n            temperature=0,\n            google_api_key=api_key,\n        )\n        self.ca_chat = ChatGoogleGenerativeAI(\n            model=self.ca_model_name,\n            temperature=0,\n            google_api_key=api_key,\n        )\n\n        # if binding happens here, tools will be available for all messages\n        if self.tools:\n            self.bind_tools(self.tools)\n\n        return True\n\n    except Exception:  # Google Genai doesn't expose specific exception types\n        self._chat = None\n        self._ca_chat = None\n        return False\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.GptConversation","title":"<code>GptConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the OpenAI GPT model.</p> Source code in <code>biochatter/llm_connect/openai.py</code> <pre><code>class GptConversation(Conversation):\n    \"\"\"Conversation class for the OpenAI GPT model.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n        base_url: str = None,\n        update_token_usage: Callable | None = None,\n    ) -&gt; None:\n        \"\"\"Connect to OpenAI's GPT API and set up a conversation with the user.\n\n        Also initialise a second conversational agent to provide corrections to\n        the model output, if necessary.\n\n        Args:\n        ----\n            model_name (str): The name of the model to use.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n\n            base_url (str): Optional OpenAI base_url value to use custom\n                endpoint URL instead of default\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n        self.base_url = base_url\n        self.ca_model_name = \"gpt-3.5-turbo\"\n        # TODO make accessible by drop-down\n\n        self._update_token_usage = update_token_usage\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the OpenAI API.\n\n        If the key is valid, initialise the conversational agent. Optionally set\n        the user for usage statistics.\n\n        Args:\n        ----\n            api_key (str): The API key for the OpenAI API.\n\n            user (str, optional): The user for usage statistics. If provided and\n                equals \"community\", will track usage stats.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        client = openai.OpenAI(\n            api_key=api_key,\n            base_url=self.base_url,\n        )\n        self.user = user\n\n        try:\n            client.models.list()\n            self.chat = ChatOpenAI(\n                model_name=self.model_name,\n                temperature=0,\n                openai_api_key=api_key,\n                base_url=self.base_url,\n            )\n            self.ca_chat = ChatOpenAI(\n                model_name=self.ca_model_name,\n                temperature=0,\n                openai_api_key=api_key,\n                base_url=self.base_url,\n            )\n            if user == \"community\":\n                self.usage_stats = get_stats(user=user)\n\n            return True\n\n        except openai._exceptions.AuthenticationError:\n            self._chat = None\n            self._ca_chat = None\n            return False\n\n    def _primary_query(self, **kwargs) -&gt; tuple:\n        \"\"\"Query the OpenAI API with the user's message.\n\n        Return the response using the message history (flattery system messages,\n        prior conversation) as context. Correct the response if necessary.\n\n        Args:\n        ----\n            **kwargs: Keyword arguments (not used by this basic GPT implementation,\n                     but accepted for compatibility with the base Conversation interface)\n\n        Returns:\n        -------\n            tuple: A tuple containing the response from the OpenAI API and the\n                token usage.\n\n        \"\"\"\n        if kwargs:\n            warnings.warn(f\"Warning: {kwargs} are not used by this class\", UserWarning)\n\n        try:\n            response = self.chat.generate([self.messages])\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n\n        msg = response.generations[0][0].text\n        token_usage_raw = response.llm_output.get(\"token_usage\")\n        token_usage = self._extract_total_tokens(token_usage_raw)\n\n        self._update_usage_stats(self.model_name, token_usage_raw)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response from the OpenAI API.\n\n        Send the response to a secondary language model. Optionally split the\n        response into single sentences and correct each sentence individually.\n        Update usage stats.\n\n        Args:\n        ----\n            msg (str): The response from the OpenAI API.\n\n        Returns:\n        -------\n            str: The corrected response (or OK if no correction necessary).\n\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n            ),\n        )\n\n        response = self.ca_chat.generate([ca_messages])\n\n        correction = response.generations[0][0].text\n        token_usage = response.llm_output.get(\"token_usage\")\n\n        self._update_usage_stats(self.ca_model_name, token_usage)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict) -&gt; None:\n        \"\"\"Update redis database with token usage statistics.\n\n        Use the usage_stats object with the increment method.\n\n        Args:\n        ----\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n\n        \"\"\"\n        if self.user == \"community\":\n            # Only process integer values\n            stats_dict = {f\"{k}:{model}\": v for k, v in token_usage.items() if isinstance(v, int | float)}\n            self.usage_stats.increment(\n                \"usage:[date]:[user]\",\n                stats_dict,\n            )\n\n        if self._update_token_usage is not None:\n            self._update_token_usage(self.user, model, token_usage)\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.GptConversation.__init__","title":"<code>__init__(model_name, prompts, correct=False, split_correction=False, base_url=None, update_token_usage=None)</code>","text":"<p>Connect to OpenAI's GPT API and set up a conversation with the user.</p> <p>Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <pre><code>model_name (str): The name of the model to use.\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\nsplit_correction (bool): Whether to correct the model output by\n    splitting the output into sentences and correcting each\n    sentence individually.\n\nbase_url (str): Optional OpenAI base_url value to use custom\n    endpoint URL instead of default\n</code></pre> Source code in <code>biochatter/llm_connect/openai.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = False,\n    split_correction: bool = False,\n    base_url: str = None,\n    update_token_usage: Callable | None = None,\n) -&gt; None:\n    \"\"\"Connect to OpenAI's GPT API and set up a conversation with the user.\n\n    Also initialise a second conversational agent to provide corrections to\n    the model output, if necessary.\n\n    Args:\n    ----\n        model_name (str): The name of the model to use.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n\n        base_url (str): Optional OpenAI base_url value to use custom\n            endpoint URL instead of default\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n    self.base_url = base_url\n    self.ca_model_name = \"gpt-3.5-turbo\"\n    # TODO make accessible by drop-down\n\n    self._update_token_usage = update_token_usage\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.GptConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the OpenAI API.</p> <p>If the key is valid, initialise the conversational agent. Optionally set the user for usage statistics.</p> <pre><code>api_key (str): The API key for the OpenAI API.\n\nuser (str, optional): The user for usage statistics. If provided and\n    equals \"community\", will track usage stats.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/llm_connect/openai.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the OpenAI API.\n\n    If the key is valid, initialise the conversational agent. Optionally set\n    the user for usage statistics.\n\n    Args:\n    ----\n        api_key (str): The API key for the OpenAI API.\n\n        user (str, optional): The user for usage statistics. If provided and\n            equals \"community\", will track usage stats.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    client = openai.OpenAI(\n        api_key=api_key,\n        base_url=self.base_url,\n    )\n    self.user = user\n\n    try:\n        client.models.list()\n        self.chat = ChatOpenAI(\n            model_name=self.model_name,\n            temperature=0,\n            openai_api_key=api_key,\n            base_url=self.base_url,\n        )\n        self.ca_chat = ChatOpenAI(\n            model_name=self.ca_model_name,\n            temperature=0,\n            openai_api_key=api_key,\n            base_url=self.base_url,\n        )\n        if user == \"community\":\n            self.usage_stats = get_stats(user=user)\n\n        return True\n\n    except openai._exceptions.AuthenticationError:\n        self._chat = None\n        self._ca_chat = None\n        return False\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LangChainConversation","title":"<code>LangChainConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for a generic LangChain model.</p> Source code in <code>biochatter/llm_connect/langchain.py</code> <pre><code>class LangChainConversation(Conversation):\n    \"\"\"Conversation class for a generic LangChain model.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        model_provider: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n        tools: list[Callable] = None,\n        tool_call_mode: Literal[\"auto\", \"text\"] = \"auto\",\n        async_mode: bool = False,\n        mcp: bool = False,\n        force_tool: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialise the LangChainConversation class.\n\n        Connect to a generic LangChain model and set up a conversation with the\n        user. Also initialise a second conversational agent to provide\n        corrections to the model output, if necessary.\n\n        Args:\n        ----\n            model_name (str): The name of the model to use.\n            model_provider (str): The provider of the model to use.\n            prompts (dict): A dictionary of prompts to use for the conversation.\n            correct (bool): Whether to correct the model output.\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n            tools (list[Callable]): List of tool functions to use with the\n                model.\n            tool_call_mode (str): The mode to use for tool calls.\n                \"auto\": Automatically call tools.\n                \"text\": Only return text output of the tool call.\n            async_mode (bool): Whether to run in async mode. Defaults to False.\n            mcp (bool): If you want to use MCP mode, this should be set to True.\n            force_tool (bool): If you want to force the model to use tools, this should be set to True.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n            tools=tools,\n            tool_call_mode=tool_call_mode,\n            mcp=mcp,\n            force_tool=force_tool,\n        )\n\n        self.model_name = model_name\n        self.model_provider = model_provider\n        self.async_mode = async_mode\n\n    # TODO: the name of this method is overloaded, since the api key is loaded\n    # from the environment variables and not as an argument\n    def set_api_key(self, api_key: str | None = None, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the model provider.\n\n        If the key is valid, initialise the conversational agent. Optionally set\n        the user for usage statistics.\n\n        Args:\n        ----\n            api_key (str): The API key for the model provider.\n\n            user (str, optional): The user for usage statistics. If provided and\n                equals \"community\", will track usage stats.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        self.user = user\n\n        try:\n            self.chat = init_chat_model(\n                model=self.model_name,\n                model_provider=self.model_provider,\n                temperature=0,\n            )\n            self.ca_chat = init_chat_model(\n                model=self.model_name,\n                model_provider=self.model_provider,\n                temperature=0,\n            )\n\n            # if binding happens here, tools will be available for all messages\n            if self.tools:\n                self.bind_tools(self.tools)\n\n            return True\n\n        except Exception:  # Google Genai doesn't expose specific exception types\n            self._chat = None\n            self._ca_chat = None\n            return False\n\n    def _primary_query(\n        self,\n        tools: list[Callable] | None = None,\n        explain_tool_result: bool = False,\n        return_tool_calls_as_ai_message: bool = False,\n        structured_model: BaseModel | None = None,\n        wrap_structured_output: bool = False,\n        track_tool_calls: bool = False,\n    ) -&gt; tuple:\n        \"\"\"Run the primary query.\n\n        Args:\n        ----\n            tools (list[Callable], optional): Additional tools to use for this specific query.\n            explain_tool_result (bool, optional): Whether to explain the tool result.\n            return_tool_calls_as_ai_message (bool, optional): Whether to return tool calls as an AI message.\n            structured_model (BaseModel, optional): The structured output model to use.\n            wrap_structured_output (bool, optional): Whether to wrap the structured output in JSON quotes.\n            track_tool_calls (bool, optional): Whether to track the tool calls.\n\n        Returns:\n        -------\n            tuple: A tuple containing the response message and token usage information.\n\n        \"\"\"\n        token_usage = None  # Initialize token_usage\n        msg = None  # Initialize msg\n\n        starting_tools = self.tools if self.tools else []\n        in_chat_tools = tools if tools else []\n        available_tools = starting_tools + in_chat_tools\n\n        if structured_model and len(available_tools) &gt; 0:\n            raise ValueError(\"Structured output and tools cannot be used together at the moment.\")\n\n        if self.model_name in STRUCTURED_OUTPUT_MODELS and structured_model:\n            chat = self.chat.with_structured_output(structured_model)\n        elif structured_model and self.model_name not in STRUCTURED_OUTPUT_MODELS:\n            # add to the end of the prompt an instruction to return a structured output\n            chat = self.chat\n            self.messages[-1].content = (\n                self.messages[-1].content\n                + \"\\n\\nPlease return a structured output following this schema: \"\n                + str(structured_model.model_json_schema())\n                + (\n                    \" Just return the JSON object wrapped in ```json tags and nothing else.\"\n                    if wrap_structured_output\n                    else \" Just return the JSON object and nothing else.\"\n                )\n            )\n\n        if (self.model_name in TOOL_CALLING_MODELS and not structured_model) or self.force_tool:\n            chat = self.chat.bind_tools(available_tools)\n        elif self.model_name not in TOOL_CALLING_MODELS and len(available_tools) &gt; 0:\n            self.tools_prompt = self._create_tool_prompt(\n                tools=available_tools,\n                additional_instructions=self.additional_tools_instructions,\n            )\n            if not self.messages:\n                msg = \"No messages available in the conversation\"\n                raise ValueError(msg)\n            self.messages[-1] = self.tools_prompt\n            chat = self.chat\n        elif len(available_tools) == 0 and not structured_model:\n            chat = self.chat\n\n        try:\n            response = chat.invoke(self.messages)\n        except Exception as e:\n            return str(e), None\n\n        # Structured output don't have tool calls attribute\n        if hasattr(response, \"tool_calls\"):\n            token_usage_raw = response.usage_metadata if response.usage_metadata else None\n            token_usage = self._extract_total_tokens(token_usage_raw)\n            # case in which the model called tools\n            if len(response.tool_calls) &gt; 0:\n                self.append_ai_message(response)\n                msg = self._process_tool_calls(\n                    tool_calls=response.tool_calls,\n                    available_tools=available_tools,\n                    response_content=response.content,\n                    explain_tool_result=explain_tool_result,\n                    return_tool_calls_as_ai_message=return_tool_calls_as_ai_message,\n                    track_tool_calls=track_tool_calls,\n                )\n            # case where the model does not support tool calling natively, called a tool and we need manual processing\n            elif self.model_name not in TOOL_CALLING_MODELS and self.tools_prompt:\n                cleaned_content = (\n                    response.content.replace('\"\"\"', \"\").replace(\"json\", \"\").replace(\"`\", \"\").replace(\"\\n\", \"\").strip()\n                )\n                try:\n                    tool_call_data = json.loads(cleaned_content)\n                    msg = self._process_manual_tool_call(\n                        tool_call=tool_call_data,\n                        available_tools=available_tools,\n                        explain_tool_result=explain_tool_result,\n                    )\n                    # token_usage remains None (from line 176) as per successful tool call path logic\n                except json.JSONDecodeError:\n                    # If JSON parsing fails, the model didn't return a valid tool call.\n                    # Treat as a regular message from the LLM.\n                    msg = response.content  # Use original content\n                    # Update token_usage, similar to 'no tool calls' or 'manual structured output' paths\n            # case where the model does not support structured output but the user has provided a structured model\n            elif self.model_name not in STRUCTURED_OUTPUT_MODELS and structured_model:\n                # check that the output conforms to the structured model\n                pydantic_manual_validator(response.content, structured_model)\n                msg = response.content\n\n            # no tool calls\n            else:\n                msg = response.content\n                self.append_ai_message(response)\n\n        # even if there are no tool calls, the standard langchain output has a tool_calls attribute\n        # therefore, this case only happens when the returned ouput from the invoke is a structured output\n        else:\n            msg = response.model_dump_json()\n            if wrap_structured_output:\n                msg = \"```json\\n\" + msg + \"\\n```\"\n            # we don't return None because is used to signal an error, instead here we just can't count the tokens\n            token_usage = -1\n            self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response from the Gemini API.\n\n        Send the response to a secondary language model. Optionally split the\n        response into single sentences and correct each sentence individually.\n        Update usage stats.\n\n        Args:\n        ----\n            msg (str): The response from the Gemini API.\n\n        Returns:\n        -------\n            str: The corrected response (or OK if no correction necessary).\n\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n            ),\n        )\n\n        response = self.ca_chat.invoke(ca_messages)\n\n        correction = response.content\n        token_usage_raw = response.usage_metadata\n        token_usage = self._extract_total_tokens(token_usage_raw)\n\n        return correction, token_usage\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LangChainConversation.__init__","title":"<code>__init__(model_name, model_provider, prompts, correct=False, split_correction=False, tools=None, tool_call_mode='auto', async_mode=False, mcp=False, force_tool=False)</code>","text":"<p>Initialise the LangChainConversation class.</p> <p>Connect to a generic LangChain model and set up a conversation with the user. Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <pre><code>model_name (str): The name of the model to use.\nmodel_provider (str): The provider of the model to use.\nprompts (dict): A dictionary of prompts to use for the conversation.\ncorrect (bool): Whether to correct the model output.\nsplit_correction (bool): Whether to correct the model output by\n    splitting the output into sentences and correcting each\n    sentence individually.\ntools (list[Callable]): List of tool functions to use with the\n    model.\ntool_call_mode (str): The mode to use for tool calls.\n    \"auto\": Automatically call tools.\n    \"text\": Only return text output of the tool call.\nasync_mode (bool): Whether to run in async mode. Defaults to False.\nmcp (bool): If you want to use MCP mode, this should be set to True.\nforce_tool (bool): If you want to force the model to use tools, this should be set to True.\n</code></pre> Source code in <code>biochatter/llm_connect/langchain.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    model_provider: str,\n    prompts: dict,\n    correct: bool = False,\n    split_correction: bool = False,\n    tools: list[Callable] = None,\n    tool_call_mode: Literal[\"auto\", \"text\"] = \"auto\",\n    async_mode: bool = False,\n    mcp: bool = False,\n    force_tool: bool = False,\n) -&gt; None:\n    \"\"\"Initialise the LangChainConversation class.\n\n    Connect to a generic LangChain model and set up a conversation with the\n    user. Also initialise a second conversational agent to provide\n    corrections to the model output, if necessary.\n\n    Args:\n    ----\n        model_name (str): The name of the model to use.\n        model_provider (str): The provider of the model to use.\n        prompts (dict): A dictionary of prompts to use for the conversation.\n        correct (bool): Whether to correct the model output.\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n        tools (list[Callable]): List of tool functions to use with the\n            model.\n        tool_call_mode (str): The mode to use for tool calls.\n            \"auto\": Automatically call tools.\n            \"text\": Only return text output of the tool call.\n        async_mode (bool): Whether to run in async mode. Defaults to False.\n        mcp (bool): If you want to use MCP mode, this should be set to True.\n        force_tool (bool): If you want to force the model to use tools, this should be set to True.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n        tools=tools,\n        tool_call_mode=tool_call_mode,\n        mcp=mcp,\n        force_tool=force_tool,\n    )\n\n    self.model_name = model_name\n    self.model_provider = model_provider\n    self.async_mode = async_mode\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LangChainConversation.set_api_key","title":"<code>set_api_key(api_key=None, user=None)</code>","text":"<p>Set the API key for the model provider.</p> <p>If the key is valid, initialise the conversational agent. Optionally set the user for usage statistics.</p> <pre><code>api_key (str): The API key for the model provider.\n\nuser (str, optional): The user for usage statistics. If provided and\n    equals \"community\", will track usage stats.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/llm_connect/langchain.py</code> <pre><code>def set_api_key(self, api_key: str | None = None, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the model provider.\n\n    If the key is valid, initialise the conversational agent. Optionally set\n    the user for usage statistics.\n\n    Args:\n    ----\n        api_key (str): The API key for the model provider.\n\n        user (str, optional): The user for usage statistics. If provided and\n            equals \"community\", will track usage stats.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    self.user = user\n\n    try:\n        self.chat = init_chat_model(\n            model=self.model_name,\n            model_provider=self.model_provider,\n            temperature=0,\n        )\n        self.ca_chat = init_chat_model(\n            model=self.model_name,\n            model_provider=self.model_provider,\n            temperature=0,\n        )\n\n        # if binding happens here, tools will be available for all messages\n        if self.tools:\n            self.bind_tools(self.tools)\n\n        return True\n\n    except Exception:  # Google Genai doesn't expose specific exception types\n        self._chat = None\n        self._ca_chat = None\n        return False\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LiteLLMConversation","title":"<code>LiteLLMConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>A unified interface for multiple LLM models using LiteLLM.</p> <p>This class implements the abstract methods from the Conversation parent class and provides a unified way to interact with different LLM providers through LiteLLM, which supports models from OpenAI, Anthropic, HuggingFace, and more.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> <code>prompts</code> <code>dict</code> <p>Dictionary containing various prompts used in the conversation.</p> <code>correct</code> <code>bool</code> <p>Whether to use a correcting agent.</p> <code>split_correction</code> <code>bool</code> <p>Whether to split corrections by sentence.</p> <code>rag_agents</code> <code>list</code> <p>List of RAG agents available for context enhancement.</p> <code>history</code> <code>list</code> <p>Conversation history for logging/printing.</p> <code>messages</code> <code>list</code> <p>Messages in the conversation.</p> <code>ca_messages</code> <code>list</code> <p>Messages for the correcting agent.</p> <code>api_key</code> <code>str</code> <p>API key for the LLM provider.</p> <code>user</code> <code>str</code> <p>Username for the API, if required.</p> Source code in <code>biochatter/llm_connect/llmlite.py</code> <pre><code>class LiteLLMConversation(Conversation):\n    \"\"\"A unified interface for multiple LLM models using LiteLLM.\n\n    This class implements the abstract methods from the Conversation parent class\n    and provides a unified way to interact with different LLM providers through\n    LiteLLM, which supports models from OpenAI, Anthropic, HuggingFace, and more.\n\n    Attributes:\n        model_name (str): The name of the model to use.\n        prompts (dict): Dictionary containing various prompts used in the conversation.\n        correct (bool): Whether to use a correcting agent.\n        split_correction (bool): Whether to split corrections by sentence.\n        rag_agents (list): List of RAG agents available for context enhancement.\n        history (list): Conversation history for logging/printing.\n        messages (list): Messages in the conversation.\n        ca_messages (list): Messages for the correcting agent.\n        api_key (str): API key for the LLM provider.\n        user (str): Username for the API, if required.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n        use_ragagent_selector: bool = False,\n        update_token_usage: Callable | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize a UnifiedConversation instance.\n\n        Args:\n            model_name (str): The name of the model to use.\n            prompts (dict): Dictionary containing various prompts used in the conversation.\n            correct (bool): Whether to use a correcting agent. Defaults to False.\n            split_correction (bool): Whether to split corrections by sentence. Defaults to False.\n            use_ragagent_selector (bool): Whether to use RagAgentSelector. Defaults to False.\n            update_token_usage (Callable): A function to update the token usage statistics.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n            use_ragagent_selector=use_ragagent_selector,\n        )\n        self.api_key = None\n        self.user = None\n        self.ca_model_name = model_name\n        self._update_token_usage = update_token_usage\n\n    def get_litellm_object(self, api_key: str, model: str) -&gt; ChatLiteLLM:\n        \"\"\"Get a LiteLLM object for the specified model and API key.\n\n        Args:\n            api_key (str): The API key for the LLM provider.\n            model (str): The name of the model to use.\n\n        Returns:\n            ChatLiteLLM: An instance of ChatLiteLLM configured with the specified model, temperature, max tokens and API key.\n\n        Raises:\n            ValueError: If the API key is None.\n            litellm.exceptions.AuthenticationError: If there is an authentication error.\n            litellm.exceptions.InvalidRequestError: If the request is invalid.\n            litellm.exceptions.RateLimitError: If the rate limit is exceeded.\n            litellm.exceptions.ServiceUnavailableError: If the service is unavailable.\n            litellm.exceptions.APIError: If there is a general API error.\n            litellm.exceptions.Timeout: If the request times out.\n            litellm.exceptions.APIConnectionError: If there is a connection error.\n            litellm.exceptions.InternalServerError: If there is an internal server error.\n            Exception: If there is an unexpected error.\n\n        \"\"\"\n        if api_key is None:\n            raise ValueError(\"API key must not be None\")\n\n        try:\n            max_tokens = self.get_model_max_tokens(model)\n        except:\n            max_tokens = None\n\n        kwargs = {\"temperature\": 0, \"max_token\": max_tokens, \"model_name\": model}\n\n        if self.model_name.startswith(\"gpt-\"):\n            api_key_kwarg = \"openai_api_key\"\n        elif self.model_name.startswith(\"claude-\"):\n            api_key_kwarg = \"anthropic_api_key\"\n        elif self.model_name.startswith(\"azure/\"):\n            api_key_kwarg = \"azure_api_key\"\n        elif self.model_name.startswith(\"mistral/\") or self.model_name in [\n            \"mistral-tiny\",\n            \"mistral-small\",\n            \"mistral-medium\",\n            \"mistral-large-latest\",\n        ]:\n            api_key_kwarg = \"api_key\"\n        else:\n            api_key_kwarg = \"api_key\"\n\n        kwargs[api_key_kwarg] = api_key\n        try:\n            return ChatLiteLLM(**kwargs)\n\n        except (\n            litellm.exceptions.AuthenticationError,\n            litellm.exceptions.InvalidRequestError,\n            litellm.exceptions.RateLimitError,\n            litellm.exceptions.ServiceUnavailableError,\n            litellm.exceptions.APIError,\n            litellm.exceptions.Timeout,\n            litellm.exceptions.APIConnectionError,\n            litellm.exceptions.InternalServerError,\n        ) as api_setup_error:\n            raise api_setup_error\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the LLM provider.\n\n        Args:\n            api_key (str): The API key for the LLM provider.\n            user (Union[str, None]): The username\n\n        Returns:\n            bool: True if the API key is successfully set, False otherwise.\n\n        Raises:\n            ValueError: If the model name or correction model name is not set.\n            TypeError: If the LiteLLM object initialization fails.\n            Exception: If there is an unexpected error.\n\n        \"\"\"\n        try:\n            if self.model_name is None:\n                raise ValueError(\"Primary Model name is not set.\")\n\n            if self.ca_model_name is None:\n                raise ValueError(\"Correction Model name is not set.\")\n\n            self.chat = self.get_litellm_object(api_key, self.model_name)\n            if self.chat is None:\n                raise TypeError(\"Failed to intialize primary agent chat object.\")\n\n            self.ca_chat = self.get_litellm_object(api_key, self.ca_model_name)\n            if self.ca_chat is None:\n                raise TypeError(\"Failed to intialize correcting agent chat object.\")\n\n            self.user = user\n            if user == \"community\":\n                self.usage_stats = get_stats(user=user)\n            return True\n\n        except (ValueError, TypeError):\n            self.chat = None\n            self.ca_chat = None\n            return False\n        except Exception:\n            self.chat = None\n            self.ca_chat = None\n            return False\n\n    def json_serializable(self, obj):\n        \"\"\"Convert non-serializable objects to serializable format.\"\"\"\n        if obj is None:\n            raise ValueError(\"Object is None\")\n        if hasattr(obj, \"__dict__\"):\n            return obj.__dict__\n        if hasattr(obj, \"dict\") and callable(obj.dict):\n            return obj.dict()\n        try:\n            return str(obj)\n        except:\n            return repr(obj)\n\n    def parse_llm_response(self, response) -&gt; dict | None:\n        \"\"\"Parse the response from the LLM.\"\"\"\n        try:\n            full_json = json.loads(json.dumps(response, default=self.json_serializable))\n\n            if not full_json.get(\"generations\"):\n                return None\n\n            generations = full_json[\"generations\"]\n            if not generations or not generations[0]:\n                return None\n\n            first_generation = generations[0][0]\n            if not first_generation or not first_generation.get(\"message\"):\n                return None\n\n            message = first_generation[\"message\"]\n            if not message.get(\"response_metadata\"):\n                return None\n\n            response_metadata = message[\"response_metadata\"]\n            if not response_metadata.get(\"token_usage\"):\n                return None\n\n            return response_metadata[\"token_usage\"]\n\n        except (KeyError, IndexError, TypeError, json.JSONDecodeError) as e:\n            print(f\"Error parsing LLM response: {e}\")\n            return None\n\n        except Exception as e:\n            print(f\"Unexpected error while parsing LLM response: {e}\")\n            return None\n\n    def _primary_query(self, **kwargs) -&gt; tuple:\n        \"\"\"Query the LLM API with the user's message.\n\n        Return the response using the message history (flattery system messages,\n        prior conversation) as context. Correct the response if necessary.\n\n        Args:\n        ----\n            **kwargs: Keyword arguments (not used by this basic LiteLLM implementation,\n                     but accepted for compatibility with the base Conversation interface)\n\n        Returns:\n            tuple: A tuple containing the response from the LLM API and the token usage.\n\n        \"\"\"\n        if kwargs:\n            warnings.warn(f\"Warning: {kwargs} are not used by this class\", UserWarning)\n\n        try:\n            response = self.chat.generate([self.messages])\n        except (\n            AttributeError,\n            litellm.exceptions.APIError,\n            litellm.exceptions.OpenAIError,\n            litellm.exceptions.RateLimitError,\n            litellm.exceptions.APIConnectionError,\n            litellm.exceptions.BadRequestError,\n            litellm.exceptions.AuthenticationError,\n            litellm.exceptions.InternalServerError,\n            litellm.exceptions.PermissionDeniedError,\n            litellm.exceptions.UnprocessableEntityError,\n            litellm.exceptions.APIResponseValidationError,\n            litellm.exceptions.BudgetExceededError,\n            litellm.exceptions.RejectedRequestError,\n            litellm.exceptions.ServiceUnavailableError,\n            litellm.exceptions.Timeout,\n        ) as e:\n            return e, None\n        except Exception as e:\n            return e, None\n\n        msg = response.generations[0][0].text\n        token_usage_raw = self.parse_llm_response(response)\n        token_usage = self._extract_total_tokens(token_usage_raw)\n\n        self.append_ai_message(msg)\n\n        self._update_usage_stats(self.model_name, token_usage)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response from the LLM.\n\n        Args:\n            msg (str): The response message to correct.\n\n        Returns:\n            str: The corrected response message.\n\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n            ),\n        )\n\n        response = self.ca_chat.generate([ca_messages])\n\n        correction = response.generations[0][0].text\n        token_usage_raw = self.parse_llm_response(response)\n        token_usage = self._extract_total_tokens(token_usage_raw)\n\n        self._update_usage_stats(self.ca_model_name, token_usage_raw)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict) -&gt; None:\n        \"\"\"Update the usage statistics.\n\n        Args:\n            model (str): The model name.\n            token_usage (dict): The token usage information.\n\n        \"\"\"\n        if self.user == \"community\" and model:\n            stats_dict = {f\"{k}:{model}\": v for k, v in token_usage.items() if isinstance(v, int | float)}\n            self.usage_stats.increment(\n                \"usage:[date]:[user]\",\n                stats_dict,\n            )\n\n        if self._update_token_usage is not None:\n            self._update_token_usage(self.user, model, token_usage)\n\n    def get_all_model_list(self) -&gt; list:\n        \"\"\"Get a list of all available models.\"\"\"\n        return litellm.model_list\n\n    def get_models_by_provider(self):\n        \"\"\"Get a dictionary of models grouped by their provider.\"\"\"\n        return litellm.models_by_provider\n\n    def get_all_model_info(self) -&gt; dict:\n        \"\"\"Get information about all available models.\"\"\"\n        return litellm.model_cost\n\n    def get_model_info(self, model: str) -&gt; dict:\n        \"\"\"Get information about a specific model.\n\n        Args:\n            model (str): The name of the model.\n\n        Returns:\n            dict: A dictionary containing information about the specified model.\n\n        \"\"\"\n        models_info: dict = self.get_all_model_info()\n        if model not in models_info:\n            raise litellm.exceptions.NotFoundError(\n                f\"{model} model's information is not available.\",\n                model=model,\n                llm_provider=\"Unknown\",\n            )\n        return models_info[model]\n\n    def get_model_max_tokens(self, model: str) -&gt; int:\n        \"\"\"Get the maximum number of tokens for a specific model.\n\n        Args:\n            model (str): The name of the model.\n\n        Returns:\n            int: The maximum number of tokens for the specified model.\n\n        \"\"\"\n        try:\n            model_info = self.get_model_info(model)\n            if \"max_tokens\" not in model_info:\n                raise litellm.exceptions.NotFoundError(\n                    f\"Max token information for {model} is not available.\",\n                    model=model,\n                    llm_provider=\"Unknown\",\n                )\n            return model_info[\"max_tokens\"]\n        except litellm.exceptions.NotFoundError as e:\n            raise e\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LiteLLMConversation.__init__","title":"<code>__init__(model_name, prompts, correct=False, split_correction=False, use_ragagent_selector=False, update_token_usage=None)</code>","text":"<p>Initialize a UnifiedConversation instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>prompts</code> <code>dict</code> <p>Dictionary containing various prompts used in the conversation.</p> required <code>correct</code> <code>bool</code> <p>Whether to use a correcting agent. Defaults to False.</p> <code>False</code> <code>split_correction</code> <code>bool</code> <p>Whether to split corrections by sentence. Defaults to False.</p> <code>False</code> <code>use_ragagent_selector</code> <code>bool</code> <p>Whether to use RagAgentSelector. Defaults to False.</p> <code>False</code> <code>update_token_usage</code> <code>Callable</code> <p>A function to update the token usage statistics.</p> <code>None</code> Source code in <code>biochatter/llm_connect/llmlite.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = False,\n    split_correction: bool = False,\n    use_ragagent_selector: bool = False,\n    update_token_usage: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initialize a UnifiedConversation instance.\n\n    Args:\n        model_name (str): The name of the model to use.\n        prompts (dict): Dictionary containing various prompts used in the conversation.\n        correct (bool): Whether to use a correcting agent. Defaults to False.\n        split_correction (bool): Whether to split corrections by sentence. Defaults to False.\n        use_ragagent_selector (bool): Whether to use RagAgentSelector. Defaults to False.\n        update_token_usage (Callable): A function to update the token usage statistics.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n        use_ragagent_selector=use_ragagent_selector,\n    )\n    self.api_key = None\n    self.user = None\n    self.ca_model_name = model_name\n    self._update_token_usage = update_token_usage\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LiteLLMConversation.get_all_model_info","title":"<code>get_all_model_info()</code>","text":"<p>Get information about all available models.</p> Source code in <code>biochatter/llm_connect/llmlite.py</code> <pre><code>def get_all_model_info(self) -&gt; dict:\n    \"\"\"Get information about all available models.\"\"\"\n    return litellm.model_cost\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LiteLLMConversation.get_all_model_list","title":"<code>get_all_model_list()</code>","text":"<p>Get a list of all available models.</p> Source code in <code>biochatter/llm_connect/llmlite.py</code> <pre><code>def get_all_model_list(self) -&gt; list:\n    \"\"\"Get a list of all available models.\"\"\"\n    return litellm.model_list\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LiteLLMConversation.get_litellm_object","title":"<code>get_litellm_object(api_key, model)</code>","text":"<p>Get a LiteLLM object for the specified model and API key.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for the LLM provider.</p> required <code>model</code> <code>str</code> <p>The name of the model to use.</p> required <p>Returns:</p> Name Type Description <code>ChatLiteLLM</code> <code>ChatLiteLLM</code> <p>An instance of ChatLiteLLM configured with the specified model, temperature, max tokens and API key.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API key is None.</p> <code>AuthenticationError</code> <p>If there is an authentication error.</p> <code>InvalidRequestError</code> <p>If the request is invalid.</p> <code>RateLimitError</code> <p>If the rate limit is exceeded.</p> <code>ServiceUnavailableError</code> <p>If the service is unavailable.</p> <code>APIError</code> <p>If there is a general API error.</p> <code>Timeout</code> <p>If the request times out.</p> <code>APIConnectionError</code> <p>If there is a connection error.</p> <code>InternalServerError</code> <p>If there is an internal server error.</p> <code>Exception</code> <p>If there is an unexpected error.</p> Source code in <code>biochatter/llm_connect/llmlite.py</code> <pre><code>def get_litellm_object(self, api_key: str, model: str) -&gt; ChatLiteLLM:\n    \"\"\"Get a LiteLLM object for the specified model and API key.\n\n    Args:\n        api_key (str): The API key for the LLM provider.\n        model (str): The name of the model to use.\n\n    Returns:\n        ChatLiteLLM: An instance of ChatLiteLLM configured with the specified model, temperature, max tokens and API key.\n\n    Raises:\n        ValueError: If the API key is None.\n        litellm.exceptions.AuthenticationError: If there is an authentication error.\n        litellm.exceptions.InvalidRequestError: If the request is invalid.\n        litellm.exceptions.RateLimitError: If the rate limit is exceeded.\n        litellm.exceptions.ServiceUnavailableError: If the service is unavailable.\n        litellm.exceptions.APIError: If there is a general API error.\n        litellm.exceptions.Timeout: If the request times out.\n        litellm.exceptions.APIConnectionError: If there is a connection error.\n        litellm.exceptions.InternalServerError: If there is an internal server error.\n        Exception: If there is an unexpected error.\n\n    \"\"\"\n    if api_key is None:\n        raise ValueError(\"API key must not be None\")\n\n    try:\n        max_tokens = self.get_model_max_tokens(model)\n    except:\n        max_tokens = None\n\n    kwargs = {\"temperature\": 0, \"max_token\": max_tokens, \"model_name\": model}\n\n    if self.model_name.startswith(\"gpt-\"):\n        api_key_kwarg = \"openai_api_key\"\n    elif self.model_name.startswith(\"claude-\"):\n        api_key_kwarg = \"anthropic_api_key\"\n    elif self.model_name.startswith(\"azure/\"):\n        api_key_kwarg = \"azure_api_key\"\n    elif self.model_name.startswith(\"mistral/\") or self.model_name in [\n        \"mistral-tiny\",\n        \"mistral-small\",\n        \"mistral-medium\",\n        \"mistral-large-latest\",\n    ]:\n        api_key_kwarg = \"api_key\"\n    else:\n        api_key_kwarg = \"api_key\"\n\n    kwargs[api_key_kwarg] = api_key\n    try:\n        return ChatLiteLLM(**kwargs)\n\n    except (\n        litellm.exceptions.AuthenticationError,\n        litellm.exceptions.InvalidRequestError,\n        litellm.exceptions.RateLimitError,\n        litellm.exceptions.ServiceUnavailableError,\n        litellm.exceptions.APIError,\n        litellm.exceptions.Timeout,\n        litellm.exceptions.APIConnectionError,\n        litellm.exceptions.InternalServerError,\n    ) as api_setup_error:\n        raise api_setup_error\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LiteLLMConversation.get_model_info","title":"<code>get_model_info(model)</code>","text":"<p>Get information about a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the model.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing information about the specified model.</p> Source code in <code>biochatter/llm_connect/llmlite.py</code> <pre><code>def get_model_info(self, model: str) -&gt; dict:\n    \"\"\"Get information about a specific model.\n\n    Args:\n        model (str): The name of the model.\n\n    Returns:\n        dict: A dictionary containing information about the specified model.\n\n    \"\"\"\n    models_info: dict = self.get_all_model_info()\n    if model not in models_info:\n        raise litellm.exceptions.NotFoundError(\n            f\"{model} model's information is not available.\",\n            model=model,\n            llm_provider=\"Unknown\",\n        )\n    return models_info[model]\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LiteLLMConversation.get_model_max_tokens","title":"<code>get_model_max_tokens(model)</code>","text":"<p>Get the maximum number of tokens for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the model.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The maximum number of tokens for the specified model.</p> Source code in <code>biochatter/llm_connect/llmlite.py</code> <pre><code>def get_model_max_tokens(self, model: str) -&gt; int:\n    \"\"\"Get the maximum number of tokens for a specific model.\n\n    Args:\n        model (str): The name of the model.\n\n    Returns:\n        int: The maximum number of tokens for the specified model.\n\n    \"\"\"\n    try:\n        model_info = self.get_model_info(model)\n        if \"max_tokens\" not in model_info:\n            raise litellm.exceptions.NotFoundError(\n                f\"Max token information for {model} is not available.\",\n                model=model,\n                llm_provider=\"Unknown\",\n            )\n        return model_info[\"max_tokens\"]\n    except litellm.exceptions.NotFoundError as e:\n        raise e\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LiteLLMConversation.get_models_by_provider","title":"<code>get_models_by_provider()</code>","text":"<p>Get a dictionary of models grouped by their provider.</p> Source code in <code>biochatter/llm_connect/llmlite.py</code> <pre><code>def get_models_by_provider(self):\n    \"\"\"Get a dictionary of models grouped by their provider.\"\"\"\n    return litellm.models_by_provider\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LiteLLMConversation.json_serializable","title":"<code>json_serializable(obj)</code>","text":"<p>Convert non-serializable objects to serializable format.</p> Source code in <code>biochatter/llm_connect/llmlite.py</code> <pre><code>def json_serializable(self, obj):\n    \"\"\"Convert non-serializable objects to serializable format.\"\"\"\n    if obj is None:\n        raise ValueError(\"Object is None\")\n    if hasattr(obj, \"__dict__\"):\n        return obj.__dict__\n    if hasattr(obj, \"dict\") and callable(obj.dict):\n        return obj.dict()\n    try:\n        return str(obj)\n    except:\n        return repr(obj)\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LiteLLMConversation.parse_llm_response","title":"<code>parse_llm_response(response)</code>","text":"<p>Parse the response from the LLM.</p> Source code in <code>biochatter/llm_connect/llmlite.py</code> <pre><code>def parse_llm_response(self, response) -&gt; dict | None:\n    \"\"\"Parse the response from the LLM.\"\"\"\n    try:\n        full_json = json.loads(json.dumps(response, default=self.json_serializable))\n\n        if not full_json.get(\"generations\"):\n            return None\n\n        generations = full_json[\"generations\"]\n        if not generations or not generations[0]:\n            return None\n\n        first_generation = generations[0][0]\n        if not first_generation or not first_generation.get(\"message\"):\n            return None\n\n        message = first_generation[\"message\"]\n        if not message.get(\"response_metadata\"):\n            return None\n\n        response_metadata = message[\"response_metadata\"]\n        if not response_metadata.get(\"token_usage\"):\n            return None\n\n        return response_metadata[\"token_usage\"]\n\n    except (KeyError, IndexError, TypeError, json.JSONDecodeError) as e:\n        print(f\"Error parsing LLM response: {e}\")\n        return None\n\n    except Exception as e:\n        print(f\"Unexpected error while parsing LLM response: {e}\")\n        return None\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.LiteLLMConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for the LLM provider.</p> required <code>user</code> <code>Union[str, None]</code> <p>The username</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the API key is successfully set, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model name or correction model name is not set.</p> <code>TypeError</code> <p>If the LiteLLM object initialization fails.</p> <code>Exception</code> <p>If there is an unexpected error.</p> Source code in <code>biochatter/llm_connect/llmlite.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the LLM provider.\n\n    Args:\n        api_key (str): The API key for the LLM provider.\n        user (Union[str, None]): The username\n\n    Returns:\n        bool: True if the API key is successfully set, False otherwise.\n\n    Raises:\n        ValueError: If the model name or correction model name is not set.\n        TypeError: If the LiteLLM object initialization fails.\n        Exception: If there is an unexpected error.\n\n    \"\"\"\n    try:\n        if self.model_name is None:\n            raise ValueError(\"Primary Model name is not set.\")\n\n        if self.ca_model_name is None:\n            raise ValueError(\"Correction Model name is not set.\")\n\n        self.chat = self.get_litellm_object(api_key, self.model_name)\n        if self.chat is None:\n            raise TypeError(\"Failed to intialize primary agent chat object.\")\n\n        self.ca_chat = self.get_litellm_object(api_key, self.ca_model_name)\n        if self.ca_chat is None:\n            raise TypeError(\"Failed to intialize correcting agent chat object.\")\n\n        self.user = user\n        if user == \"community\":\n            self.usage_stats = get_stats(user=user)\n        return True\n\n    except (ValueError, TypeError):\n        self.chat = None\n        self.ca_chat = None\n        return False\n    except Exception:\n        self.chat = None\n        self.ca_chat = None\n        return False\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation","title":"<code>OllamaConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the Ollama model.</p> Source code in <code>biochatter/llm_connect/ollama.py</code> <pre><code>class OllamaConversation(Conversation):\n    \"\"\"Conversation class for the Ollama model.\"\"\"\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the Ollama API. Not implemented.\n\n        Args:\n        ----\n            api_key (str): The API key for the Ollama API.\n\n            user (str): The user for usage statistics.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        err = \"Ollama does not require an API key.\"\n        raise NotImplementedError(err)\n\n    def __init__(\n        self,\n        base_url: str,\n        prompts: dict,\n        model_name: str = \"llama3\",\n        correct: bool = False,\n        split_correction: bool = False,\n    ) -&gt; None:\n        \"\"\"Connect to an Ollama LLM via the Ollama/Langchain library.\n\n        Set up a conversation with the user. Also initialise a second\n        conversational agent to provide corrections to the model output, if\n        necessary.\n\n        Args:\n        ----\n            base_url (str): The base URL of the Ollama instance.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            model_name (str): The name of the model to use. Can be any model\n                name available in your Ollama instance.\n\n            correct (bool): Whether to correct the model output.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each sentence\n                individually.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n        self.model_name = model_name\n        self.model = ChatOllama(\n            base_url=base_url,\n            model=self.model_name,\n            temperature=0.0,\n        )\n\n        self.ca_model_name = \"mixtral:latest\"\n\n        self.ca_model = ChatOllama(\n            base_url=base_url,\n            model_name=self.ca_model_name,\n            temperature=0.0,\n        )\n\n    def append_system_message(self, message: str) -&gt; None:\n        \"\"\"Override the system message addition.\n\n        Ollama does not accept multiple system messages. Concatenate them if\n        there are multiple.\n\n        Args:\n        ----\n            message (str): The message to append.\n\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.messages):\n            self.messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.messages):\n                if isinstance(msg, SystemMessage):\n                    self.messages[i].content += f\"\\n{message}\"\n                    break\n\n    def append_ca_message(self, message: str) -&gt; None:\n        \"\"\"Override the system message addition for the correcting agent.\n\n        Ollama does not accept multiple system messages. Concatenate them if\n        there are multiple.\n\n        TODO this currently assumes that the correcting agent is the same model\n        as the primary one.\n\n        Args:\n        ----\n            message (str): The message to append.\n\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n            self.ca_messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.ca_messages):\n                if isinstance(msg, SystemMessage):\n                    self.ca_messages[i].content += f\"\\n{message}\"\n                    break\n\n    def _primary_query(self, **kwargs) -&gt; tuple:\n        \"\"\"Query the Ollama client API with the user's message.\n\n        Return the response using the message history (flattery system messages,\n        prior conversation) as context. Correct the response if necessary.\n\n        Returns\n        -------\n            tuple: A tuple containing the response from the Ollama API\n            (formatted similarly to responses from the OpenAI API) and the token\n            usage.\n\n        \"\"\"\n        if kwargs:\n            warnings.warn(f\"Warning: {kwargs} are not used by this class\", UserWarning)\n\n        try:\n            messages = self._create_history(self.messages)\n            response = self.model.invoke(\n                messages,\n                # ,generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n            )\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n        response_dict = response.dict()\n        msg = response_dict[\"content\"]\n        token_usage_raw = response_dict[\"response_metadata\"][\"eval_count\"]\n        token_usage = self._extract_total_tokens(token_usage_raw)\n\n        self._update_usage_stats(self.model_name, token_usage_raw)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _create_history(self, messages: list) -&gt; list:\n        history = []\n        for _, m in enumerate(messages):\n            if isinstance(m, AIMessage):\n                history.append(AIMessage(content=m.content))\n            elif isinstance(m, HumanMessage):\n                history.append(HumanMessage(content=m.content))\n            elif isinstance(m, SystemMessage):\n                history.append(SystemMessage(content=m.content))\n\n        return history\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response from the Ollama API.\n\n        Send the response to a secondary language model. Optionally split the\n        response into single sentences and correct each sentence individually.\n        Update usage stats.\n\n        Args:\n        ----\n            msg (str): The response from the model.\n\n        Returns:\n        -------\n            str: The corrected response (or OK if no correction necessary).\n\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n            ),\n        )\n        response = self.ca_model.invoke(\n            chat_history=self._create_history(self.messages),\n        ).dict()\n        correction = response[\"content\"]\n        token_usage_raw = response[\"eval_count\"]\n        token_usage = self._extract_total_tokens(token_usage_raw)\n\n        self._update_usage_stats(self.ca_model_name, token_usage_raw)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict) -&gt; None:\n        \"\"\"Update redis database with token usage statistics.\n\n        Use the usage_stats object with the increment method.\n\n        Args:\n        ----\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n\n        \"\"\"\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation.__init__","title":"<code>__init__(base_url, prompts, model_name='llama3', correct=False, split_correction=False)</code>","text":"<p>Connect to an Ollama LLM via the Ollama/Langchain library.</p> <p>Set up a conversation with the user. Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <pre><code>base_url (str): The base URL of the Ollama instance.\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\nmodel_name (str): The name of the model to use. Can be any model\n    name available in your Ollama instance.\n\ncorrect (bool): Whether to correct the model output.\n\nsplit_correction (bool): Whether to correct the model output by\n    splitting the output into sentences and correcting each sentence\n    individually.\n</code></pre> Source code in <code>biochatter/llm_connect/ollama.py</code> <pre><code>def __init__(\n    self,\n    base_url: str,\n    prompts: dict,\n    model_name: str = \"llama3\",\n    correct: bool = False,\n    split_correction: bool = False,\n) -&gt; None:\n    \"\"\"Connect to an Ollama LLM via the Ollama/Langchain library.\n\n    Set up a conversation with the user. Also initialise a second\n    conversational agent to provide corrections to the model output, if\n    necessary.\n\n    Args:\n    ----\n        base_url (str): The base URL of the Ollama instance.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        model_name (str): The name of the model to use. Can be any model\n            name available in your Ollama instance.\n\n        correct (bool): Whether to correct the model output.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each sentence\n            individually.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n    self.model_name = model_name\n    self.model = ChatOllama(\n        base_url=base_url,\n        model=self.model_name,\n        temperature=0.0,\n    )\n\n    self.ca_model_name = \"mixtral:latest\"\n\n    self.ca_model = ChatOllama(\n        base_url=base_url,\n        model_name=self.ca_model_name,\n        temperature=0.0,\n    )\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation.append_ca_message","title":"<code>append_ca_message(message)</code>","text":"<p>Override the system message addition for the correcting agent.</p> <p>Ollama does not accept multiple system messages. Concatenate them if there are multiple.</p> <p>TODO this currently assumes that the correcting agent is the same model as the primary one.</p> <pre><code>message (str): The message to append.\n</code></pre> Source code in <code>biochatter/llm_connect/ollama.py</code> <pre><code>def append_ca_message(self, message: str) -&gt; None:\n    \"\"\"Override the system message addition for the correcting agent.\n\n    Ollama does not accept multiple system messages. Concatenate them if\n    there are multiple.\n\n    TODO this currently assumes that the correcting agent is the same model\n    as the primary one.\n\n    Args:\n    ----\n        message (str): The message to append.\n\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.ca_messages):\n            if isinstance(msg, SystemMessage):\n                self.ca_messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation.append_system_message","title":"<code>append_system_message(message)</code>","text":"<p>Override the system message addition.</p> <p>Ollama does not accept multiple system messages. Concatenate them if there are multiple.</p> <pre><code>message (str): The message to append.\n</code></pre> Source code in <code>biochatter/llm_connect/ollama.py</code> <pre><code>def append_system_message(self, message: str) -&gt; None:\n    \"\"\"Override the system message addition.\n\n    Ollama does not accept multiple system messages. Concatenate them if\n    there are multiple.\n\n    Args:\n    ----\n        message (str): The message to append.\n\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.messages):\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.messages):\n            if isinstance(msg, SystemMessage):\n                self.messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.OllamaConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the Ollama API. Not implemented.</p> <pre><code>api_key (str): The API key for the Ollama API.\n\nuser (str): The user for usage statistics.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/llm_connect/ollama.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the Ollama API. Not implemented.\n\n    Args:\n    ----\n        api_key (str): The API key for the Ollama API.\n\n        user (str): The user for usage statistics.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    err = \"Ollama does not require an API key.\"\n    raise NotImplementedError(err)\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.OpenRouterConversation","title":"<code>OpenRouterConversation</code>","text":"<p>               Bases: <code>LangChainConversation</code></p> <p>Conversation class for the OpenRouter API.</p> Source code in <code>biochatter/llm_connect/openrouter.py</code> <pre><code>class OpenRouterConversation(LangChainConversation):\n    \"\"\"Conversation class for the OpenRouter API.\"\"\"\n\n    def __init__(self, model_name: str, prompts: dict, **kwargs):\n        super().__init__(model_name, \"\", prompts, **kwargs)\n\n    def set_api_key(self, api_key: str | None = None, user: str | None = None) -&gt; bool:\n        \"\"\"Set the API key for the model provider.\n\n        If the key is valid, initialise the conversational agent. Optionally set\n        the user for usage statistics.\n\n        Args:\n        ----\n            api_key (str): The API key for the model provider.\n\n            user (str, optional): The user for usage statistics. If provided and\n                equals \"community\", will track usage stats.\n\n        Returns:\n        -------\n            bool: True if the API key is valid, False otherwise.\n\n        \"\"\"\n        self.user = user\n\n        try:\n            self.chat = ChatOpenRouter(\n                model_name=self.model_name,\n                temperature=0,\n            )\n            self.ca_chat = ChatOpenRouter(\n                model_name=self.model_name,\n                temperature=0,\n            )\n\n            # if binding happens here, tools will be available for all messages\n            if self.tools:\n                self.bind_tools(self.tools)\n\n            return True\n\n        except Exception:\n            self._chat = None\n            self._ca_chat = None\n            return False\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.OpenRouterConversation.set_api_key","title":"<code>set_api_key(api_key=None, user=None)</code>","text":"<p>Set the API key for the model provider.</p> <p>If the key is valid, initialise the conversational agent. Optionally set the user for usage statistics.</p> <pre><code>api_key (str): The API key for the model provider.\n\nuser (str, optional): The user for usage statistics. If provided and\n    equals \"community\", will track usage stats.\n</code></pre> <pre><code>bool: True if the API key is valid, False otherwise.\n</code></pre> Source code in <code>biochatter/llm_connect/openrouter.py</code> <pre><code>def set_api_key(self, api_key: str | None = None, user: str | None = None) -&gt; bool:\n    \"\"\"Set the API key for the model provider.\n\n    If the key is valid, initialise the conversational agent. Optionally set\n    the user for usage statistics.\n\n    Args:\n    ----\n        api_key (str): The API key for the model provider.\n\n        user (str, optional): The user for usage statistics. If provided and\n            equals \"community\", will track usage stats.\n\n    Returns:\n    -------\n        bool: True if the API key is valid, False otherwise.\n\n    \"\"\"\n    self.user = user\n\n    try:\n        self.chat = ChatOpenRouter(\n            model_name=self.model_name,\n            temperature=0,\n        )\n        self.ca_chat = ChatOpenRouter(\n            model_name=self.model_name,\n            temperature=0,\n        )\n\n        # if binding happens here, tools will be available for all messages\n        if self.tools:\n            self.bind_tools(self.tools)\n\n        return True\n\n    except Exception:\n        self._chat = None\n        self._ca_chat = None\n        return False\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.WasmConversation","title":"<code>WasmConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the wasm model.</p> Source code in <code>biochatter/llm_connect/misc.py</code> <pre><code>class WasmConversation(Conversation):\n    \"\"\"Conversation class for the wasm model.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = False,\n        split_correction: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the WasmConversation class.\n\n        This class is used to return the complete query as a string to be used\n        in the frontend running the wasm model. It does not call the API itself,\n        but updates the message history similarly to the other conversation\n        classes. It overrides the `query` method from the `Conversation` class\n        to return a plain string that contains the entire message for the model\n        as the first element of the tuple. The second and third elements are\n        `None` as there is no token usage or correction for the wasm model.\n\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n\n    def query(self, text: str) -&gt; tuple:\n        \"\"\"Return the entire message history as a single string.\n\n        This is the message that is sent to the wasm model.\n\n        Args:\n        ----\n            text (str): The user query.\n\n        Returns:\n        -------\n            tuple: A tuple containing the message history as a single string,\n                and `None` for the second and third elements of the tuple.\n\n        \"\"\"\n        self.append_user_message(text)\n\n        self._inject_context(text)\n\n        return (self._primary_query(), None, None)\n\n    def _primary_query(self):\n        \"\"\"Concatenate all messages in the conversation.\n\n        Build a single string from all messages in the conversation.\n        Currently discards information about roles (system, user).\n\n        Returns\n        -------\n            str: A single string from all messages in the conversation.\n\n        \"\"\"\n        return \"\\n\".join([m.content for m in self.messages])\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Do not use for the wasm model.\"\"\"\n        return \"ok\"\n\n    def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n        \"\"\"Do not use for the wasm model.\"\"\"\n        return True\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.WasmConversation.__init__","title":"<code>__init__(model_name, prompts, correct=False, split_correction=False)</code>","text":"<p>Initialize the WasmConversation class.</p> <p>This class is used to return the complete query as a string to be used in the frontend running the wasm model. It does not call the API itself, but updates the message history similarly to the other conversation classes. It overrides the <code>query</code> method from the <code>Conversation</code> class to return a plain string that contains the entire message for the model as the first element of the tuple. The second and third elements are <code>None</code> as there is no token usage or correction for the wasm model.</p> Source code in <code>biochatter/llm_connect/misc.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = False,\n    split_correction: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the WasmConversation class.\n\n    This class is used to return the complete query as a string to be used\n    in the frontend running the wasm model. It does not call the API itself,\n    but updates the message history similarly to the other conversation\n    classes. It overrides the `query` method from the `Conversation` class\n    to return a plain string that contains the entire message for the model\n    as the first element of the tuple. The second and third elements are\n    `None` as there is no token usage or correction for the wasm model.\n\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.WasmConversation.query","title":"<code>query(text)</code>","text":"<p>Return the entire message history as a single string.</p> <p>This is the message that is sent to the wasm model.</p> <pre><code>text (str): The user query.\n</code></pre> <pre><code>tuple: A tuple containing the message history as a single string,\n    and `None` for the second and third elements of the tuple.\n</code></pre> Source code in <code>biochatter/llm_connect/misc.py</code> <pre><code>def query(self, text: str) -&gt; tuple:\n    \"\"\"Return the entire message history as a single string.\n\n    This is the message that is sent to the wasm model.\n\n    Args:\n    ----\n        text (str): The user query.\n\n    Returns:\n    -------\n        tuple: A tuple containing the message history as a single string,\n            and `None` for the second and third elements of the tuple.\n\n    \"\"\"\n    self.append_user_message(text)\n\n    self._inject_context(text)\n\n    return (self._primary_query(), None, None)\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.WasmConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Do not use for the wasm model.</p> Source code in <code>biochatter/llm_connect/misc.py</code> <pre><code>def set_api_key(self, api_key: str, user: str | None = None) -&gt; bool:\n    \"\"\"Do not use for the wasm model.\"\"\"\n    return True\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation","title":"<code>XinferenceConversation</code>","text":"<p>               Bases: <code>Conversation</code></p> <p>Conversation class for the Xinference deployment.</p> Source code in <code>biochatter/llm_connect/xinference.py</code> <pre><code>class XinferenceConversation(Conversation):\n    \"\"\"Conversation class for the Xinference deployment.\"\"\"\n\n    def __init__(\n        self,\n        base_url: str,\n        prompts: dict,\n        model_name: str = \"auto\",\n        correct: bool = False,\n        split_correction: bool = False,\n    ) -&gt; None:\n        \"\"\"Connect to an open-source LLM via the Xinference client.\n\n        Connect to a running Xinference deployment and set up a conversation\n        with the user. Also initialise a second conversational agent to\n        provide corrections to the model output, if necessary.\n\n        Args:\n        ----\n            base_url (str): The base URL of the Xinference instance (should not\n            include the /v1 part).\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            model_name (str): The name of the model to use. Will be mapped to\n            the according uid from the list of available models. Can be set to\n            \"auto\" to use the first available model.\n\n            correct (bool): Whether to correct the model output.\n\n            split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each sentence\n            individually.\n\n        \"\"\"\n        # Shaohong: Please keep this xinference importing code here, so that,\n        # we don't need to depend on xinference if we dont need it (xinference\n        # is expensive to install)\n        from xinference.client import Client\n\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n        )\n        self.client = Client(base_url=base_url)\n\n        self.models = {}\n        self.load_models()\n\n        self.ca_model_name = model_name\n\n        self.set_api_key()\n\n        # TODO make accessible by drop-down\n\n    def load_models(self) -&gt; None:\n        \"\"\"Load the models from the Xinference client.\"\"\"\n        for id, model in self.client.list_models().items():\n            model[\"id\"] = id\n            self.models[model[\"model_name\"]] = model\n\n    def append_system_message(self, message: str) -&gt; None:\n        \"\"\"Override the system message addition.\n\n        Xinference does not accept multiple system messages. We concatenate them\n        if there are multiple.\n\n        Args:\n        ----\n            message (str): The message to append.\n\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.messages):\n            self.messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.messages):\n                if isinstance(msg, SystemMessage):\n                    self.messages[i].content += f\"\\n{message}\"\n                    break\n\n    def append_ca_message(self, message: str) -&gt; None:\n        \"\"\"Override the system message addition for the correcting agent.\n\n        Xinference does not accept multiple system messages. We concatenate them\n        if there are multiple.\n\n        TODO this currently assumes that the correcting agent is the same model\n        as the primary one.\n\n        Args:\n        ----\n            message (str): The message to append.\n\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n            self.ca_messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.ca_messages):\n                if isinstance(msg, SystemMessage):\n                    self.ca_messages[i].content += f\"\\n{message}\"\n                    break\n\n    def _primary_query(self, **kwargs) -&gt; tuple:\n        \"\"\"Query the Xinference client API.\n\n        Use the user's message and return the response using the message history\n        (flattery system messages, prior conversation) as context. Correct the\n        response if necessary.\n\n        LLaMA2 architecture does not accept separate system messages, so we\n        concatenate the system message with the user message to form the prompt.\n        'LLaMA enforces a strict rule that chats should alternate\n        user/assistant/user/assistant, and the system message, if present,\n        should be embedded into the first user message.' (from\n        https://discuss.huggingface.co/t/issue-with-llama-2-chat-template-and-out-of-date-documentation/61645/3)\n\n        Returns\n        -------\n            tuple: A tuple containing the response from the Xinference API\n            (formatted similarly to responses from the OpenAI API) and the token\n            usage.\n\n        \"\"\"\n        if kwargs:\n            warnings.warn(f\"Warning: {kwargs} are not used by this class\", UserWarning)\n\n        try:\n            history = self._create_history()\n            # TODO this is for LLaMA2 arch, may be different for newer models\n            prompt = history.pop()\n            response = self.model.chat(\n                prompt=prompt[\"content\"],\n                chat_history=history,\n                generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n            )\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n\n        msg = response[\"choices\"][0][\"message\"][\"content\"]\n        token_usage_raw = response[\"usage\"]\n        token_usage = self._extract_total_tokens(token_usage_raw)\n\n        self._update_usage_stats(self.model_name, token_usage_raw)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _create_history(self) -&gt; list:\n        \"\"\"Create a history of messages from the conversation.\n\n        Returns\n        -------\n            list: A list of messages from the conversation.\n\n        \"\"\"\n        history = []\n        # extract text components from message contents\n        msg_texts = [m.content[0][\"text\"] if isinstance(m.content, list) else m.content for m in self.messages]\n\n        # check if last message is an image message\n        is_image_message = False\n        if isinstance(self.messages[-1].content, list):\n            is_image_message = self.messages[-1].content[1][\"type\"] == \"image_url\"\n\n        # find location of last AI message (if any)\n        last_ai_message = None\n        for i, m in enumerate(self.messages):\n            if isinstance(m, AIMessage):\n                last_ai_message = i\n\n        # concatenate all messages before the last AI message into one message\n        if last_ai_message:\n            history.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\\n\".join(\n                        [m for m in msg_texts[:last_ai_message]],\n                    ),\n                },\n            )\n            # then append the last AI message\n            history.append(\n                {\n                    \"role\": \"assistant\",\n                    \"content\": msg_texts[last_ai_message],\n                },\n            )\n\n            # then concatenate all messages after that\n            # into one HumanMessage\n            history.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\\n\".join(\n                        [m for m in msg_texts[last_ai_message + 1 :]],\n                    ),\n                },\n            )\n\n        # if there is no AI message, concatenate all messages into one user\n        # message\n        else:\n            history.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\\n\".join([m for m in msg_texts[:]]),\n                },\n            )\n\n        # if the last message is an image message, add the image to the history\n        if is_image_message:\n            history[-1].content = [\n                {\"type\": \"text\", \"text\": history[-1].content},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": self.messages[-1].content[1][\"image_url\"][\"url\"],\n                    },\n                },\n            ]\n        return history\n\n    def _correct_response(self, msg: str) -&gt; str:\n        \"\"\"Correct the response from the Xinference API.\n\n        Send the response to a secondary language model. Optionally split the\n        response into single sentences and correct each sentence individually.\n        Update usage stats.\n\n        Args:\n        ----\n            msg (str): The response from the model.\n\n        Returns:\n        -------\n            str: The corrected response (or OK if no correction necessary).\n\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond with just 'OK', and nothing else!\",\n            ),\n        )\n        history = []\n        for m in self.messages:\n            if isinstance(m, SystemMessage):\n                history.append({\"role\": \"system\", \"content\": m.content})\n            elif isinstance(m, HumanMessage):\n                history.append({\"role\": \"user\", \"content\": m.content})\n            elif isinstance(m, AIMessage):\n                history.append({\"role\": \"assistant\", \"content\": m.content})\n        prompt = history.pop()\n        response = self.ca_model.chat(\n            prompt=prompt[\"content\"],\n            chat_history=history,\n            generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n        )\n\n        correction = response[\"choices\"][0][\"message\"][\"content\"]\n        token_usage_raw = response[\"usage\"]\n        token_usage = self._extract_total_tokens(token_usage_raw)\n\n        self._update_usage_stats(self.ca_model_name, token_usage_raw)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict) -&gt; None:\n        \"\"\"Update redis database with token usage statistics.\n\n        Use the usage_stats object with the increment method.\n\n        Args:\n        ----\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n\n        \"\"\"\n\n    def set_api_key(self) -&gt; bool:\n        \"\"\"Try to get the Xinference model from the client API.\n\n        If the model is found, initialise the conversational agent. If the model\n        is not found, `get_model` will raise a RuntimeError.\n\n        Returns\n        -------\n            bool: True if the model is found, False otherwise.\n\n        \"\"\"\n        try:\n            if self.model_name is None or self.model_name == \"auto\":\n                self.model_name = self.list_models_by_type(\"chat\")[0]\n            self.model = self.client.get_model(\n                self.models[self.model_name][\"id\"],\n            )\n\n            if self.ca_model_name is None or self.ca_model_name == \"auto\":\n                self.ca_model_name = self.list_models_by_type(\"chat\")[0]\n            self.ca_model = self.client.get_model(\n                self.models[self.ca_model_name][\"id\"],\n            )\n            return True\n\n        except RuntimeError:\n            self._chat = None\n            self._ca_chat = None\n            return False\n\n    def list_models_by_type(self, model_type: str) -&gt; list[str]:\n        \"\"\"List the models by type.\n\n        Args:\n        ----\n            model_type (str): The type of model to list.\n\n        Returns:\n        -------\n            list[str]: A list of model names.\n\n        \"\"\"\n        names = []\n        if model_type in [\"embed\", \"embedding\"]:\n            for name, model in self.models.items():\n                if \"model_ability\" in model:\n                    if \"embed\" in model[\"model_ability\"]:\n                        names.append(name)\n                elif model[\"model_type\"] == \"embedding\":\n                    names.append(name)\n            return names\n        for name, model in self.models.items():\n            if \"model_ability\" in model:\n                if model_type in model[\"model_ability\"]:\n                    names.append(name)\n            elif model[\"model_type\"] == model_type:\n                names.append(name)\n        return names\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.__init__","title":"<code>__init__(base_url, prompts, model_name='auto', correct=False, split_correction=False)</code>","text":"<p>Connect to an open-source LLM via the Xinference client.</p> <p>Connect to a running Xinference deployment and set up a conversation with the user. Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <pre><code>base_url (str): The base URL of the Xinference instance (should not\ninclude the /v1 part).\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\nmodel_name (str): The name of the model to use. Will be mapped to\nthe according uid from the list of available models. Can be set to\n\"auto\" to use the first available model.\n\ncorrect (bool): Whether to correct the model output.\n\nsplit_correction (bool): Whether to correct the model output by\nsplitting the output into sentences and correcting each sentence\nindividually.\n</code></pre> Source code in <code>biochatter/llm_connect/xinference.py</code> <pre><code>def __init__(\n    self,\n    base_url: str,\n    prompts: dict,\n    model_name: str = \"auto\",\n    correct: bool = False,\n    split_correction: bool = False,\n) -&gt; None:\n    \"\"\"Connect to an open-source LLM via the Xinference client.\n\n    Connect to a running Xinference deployment and set up a conversation\n    with the user. Also initialise a second conversational agent to\n    provide corrections to the model output, if necessary.\n\n    Args:\n    ----\n        base_url (str): The base URL of the Xinference instance (should not\n        include the /v1 part).\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        model_name (str): The name of the model to use. Will be mapped to\n        the according uid from the list of available models. Can be set to\n        \"auto\" to use the first available model.\n\n        correct (bool): Whether to correct the model output.\n\n        split_correction (bool): Whether to correct the model output by\n        splitting the output into sentences and correcting each sentence\n        individually.\n\n    \"\"\"\n    # Shaohong: Please keep this xinference importing code here, so that,\n    # we don't need to depend on xinference if we dont need it (xinference\n    # is expensive to install)\n    from xinference.client import Client\n\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n    )\n    self.client = Client(base_url=base_url)\n\n    self.models = {}\n    self.load_models()\n\n    self.ca_model_name = model_name\n\n    self.set_api_key()\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.append_ca_message","title":"<code>append_ca_message(message)</code>","text":"<p>Override the system message addition for the correcting agent.</p> <p>Xinference does not accept multiple system messages. We concatenate them if there are multiple.</p> <p>TODO this currently assumes that the correcting agent is the same model as the primary one.</p> <pre><code>message (str): The message to append.\n</code></pre> Source code in <code>biochatter/llm_connect/xinference.py</code> <pre><code>def append_ca_message(self, message: str) -&gt; None:\n    \"\"\"Override the system message addition for the correcting agent.\n\n    Xinference does not accept multiple system messages. We concatenate them\n    if there are multiple.\n\n    TODO this currently assumes that the correcting agent is the same model\n    as the primary one.\n\n    Args:\n    ----\n        message (str): The message to append.\n\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.ca_messages):\n            if isinstance(msg, SystemMessage):\n                self.ca_messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.append_system_message","title":"<code>append_system_message(message)</code>","text":"<p>Override the system message addition.</p> <p>Xinference does not accept multiple system messages. We concatenate them if there are multiple.</p> <pre><code>message (str): The message to append.\n</code></pre> Source code in <code>biochatter/llm_connect/xinference.py</code> <pre><code>def append_system_message(self, message: str) -&gt; None:\n    \"\"\"Override the system message addition.\n\n    Xinference does not accept multiple system messages. We concatenate them\n    if there are multiple.\n\n    Args:\n    ----\n        message (str): The message to append.\n\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.messages):\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.messages):\n            if isinstance(msg, SystemMessage):\n                self.messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.list_models_by_type","title":"<code>list_models_by_type(model_type)</code>","text":"<p>List the models by type.</p> <pre><code>model_type (str): The type of model to list.\n</code></pre> <pre><code>list[str]: A list of model names.\n</code></pre> Source code in <code>biochatter/llm_connect/xinference.py</code> <pre><code>def list_models_by_type(self, model_type: str) -&gt; list[str]:\n    \"\"\"List the models by type.\n\n    Args:\n    ----\n        model_type (str): The type of model to list.\n\n    Returns:\n    -------\n        list[str]: A list of model names.\n\n    \"\"\"\n    names = []\n    if model_type in [\"embed\", \"embedding\"]:\n        for name, model in self.models.items():\n            if \"model_ability\" in model:\n                if \"embed\" in model[\"model_ability\"]:\n                    names.append(name)\n            elif model[\"model_type\"] == \"embedding\":\n                names.append(name)\n        return names\n    for name, model in self.models.items():\n        if \"model_ability\" in model:\n            if model_type in model[\"model_ability\"]:\n                names.append(name)\n        elif model[\"model_type\"] == model_type:\n            names.append(name)\n    return names\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.load_models","title":"<code>load_models()</code>","text":"<p>Load the models from the Xinference client.</p> Source code in <code>biochatter/llm_connect/xinference.py</code> <pre><code>def load_models(self) -&gt; None:\n    \"\"\"Load the models from the Xinference client.\"\"\"\n    for id, model in self.client.list_models().items():\n        model[\"id\"] = id\n        self.models[model[\"model_name\"]] = model\n</code></pre>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.set_api_key","title":"<code>set_api_key()</code>","text":"<p>Try to get the Xinference model from the client API.</p> <p>If the model is found, initialise the conversational agent. If the model is not found, <code>get_model</code> will raise a RuntimeError.</p>"},{"location":"api-docs/llm_connect/#biochatter.llm_connect.XinferenceConversation.set_api_key--returns","title":"Returns","text":"<pre><code>bool: True if the model is found, False otherwise.\n</code></pre> Source code in <code>biochatter/llm_connect/xinference.py</code> <pre><code>def set_api_key(self) -&gt; bool:\n    \"\"\"Try to get the Xinference model from the client API.\n\n    If the model is found, initialise the conversational agent. If the model\n    is not found, `get_model` will raise a RuntimeError.\n\n    Returns\n    -------\n        bool: True if the model is found, False otherwise.\n\n    \"\"\"\n    try:\n        if self.model_name is None or self.model_name == \"auto\":\n            self.model_name = self.list_models_by_type(\"chat\")[0]\n        self.model = self.client.get_model(\n            self.models[self.model_name][\"id\"],\n        )\n\n        if self.ca_model_name is None or self.ca_model_name == \"auto\":\n            self.ca_model_name = self.list_models_by_type(\"chat\")[0]\n        self.ca_model = self.client.get_model(\n            self.models[self.ca_model_name][\"id\"],\n        )\n        return True\n\n    except RuntimeError:\n        self._chat = None\n        self._ca_chat = None\n        return False\n</code></pre>"},{"location":"api-docs/podcast/","title":"Podcast Reference","text":"<p>Here we handle generation of podcasts from texts.</p>"},{"location":"api-docs/podcast/#biochatter.podcast.Podcaster","title":"<code>Podcaster</code>","text":"Source code in <code>biochatter/podcast.py</code> <pre><code>class Podcaster:\n    def __init__(\n        self,\n        document: Document,\n        model_name: str = \"gpt-3.5-turbo\",\n    ) -&gt; None:\n        \"\"\"Orchestrates the podcasting of a document.\"\"\"\n        self.document = document\n        self.model_name = model_name\n\n    def generate_podcast(self, characters_per_paragraph: int) -&gt; None:\n        \"\"\"Podcasts the document.\n\n        Todo:\n        ----\n        - chain of density prompting for variable summary length\n\n        \"\"\"\n        full_text = self.document[0].page_content\n\n        # split text by sentence\n        sentences = self._split_text(full_text)\n\n        # could embed sentences and cluster on cosine similarity to identify\n        # paragraphs here\n\n        # preprocess text\n        for i, sentence in enumerate(sentences):\n            # special cases i.e. and e.g. - if sentence ends with one of these,\n            # append next sentence\n            special_cases = [\"i.e.\", \"e.g.\"]\n            if sentence.endswith(tuple(special_cases)):\n                sentences[i] = sentence + \" \" + sentences[i + 1]\n                del sentences[i + 1]\n\n        # concatenate first 5 sentences for title and author extraction\n        first_5 = \"\\n\".join(sentences[:5])\n        self.podcast_info = self._title_and_authors(first_5)\n\n        # LLM to determine section breaks?\n\n        # go through sections and summarise each\n        self.processed_sections = self._process_sections(\n            sentences,\n            characters_per_paragraph,\n        )\n\n        # summarise the summaries\n\n    def _split_text(self, text: str) -&gt; list[str]:\n        \"\"\"Splits consecutive text into sentences.\"\"\"\n        nltk.download(\"punkt\")\n        tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n        return tokenizer.tokenize(text)\n\n    def _title_and_authors(self, text: str) -&gt; str:\n        \"\"\"Extracts title and authors from document.\n\n        Args:\n        ----\n            text (str): text to extract title and authors from\n\n        Returns:\n        -------\n            str: title and authors\n\n        \"\"\"\n        # first sentence - extract title, authors\n        c_first = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n        c_first.set_api_key(api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"podcast\")\n        c_first.append_system_message(FIRST_PROMPT)\n        msg, token_usage, correction = c_first.query(text)\n        # split at authors ('Authors:' or '\\nAuthors:')\n        if \"Authors:\" in msg:\n            title = msg.split(\"Title:\")[1].split(\"Authors:\")[0].strip()\n            authors = msg.split(\"Authors:\")[1].strip()\n            return f\"{title}, by {authors}, podcasted by biochatter.\"\n        else:\n            return \"A podcast by biochatter.\"\n\n    def _process_section(self, text: str, summarise: bool = False) -&gt; str:\n        \"\"\"Processes a section of the document. Summarises if summarise is True,\n        otherwise just makes the text more listenable.\n\n        Args:\n        ----\n            text (str): text to summarise\n\n            summarise (bool): whether to summarise the text\n\n        Returns:\n        -------\n            str: summarised text\n\n        \"\"\"\n        # summarise section\n        c = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n        c.set_api_key(api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"podcast\")\n        if summarise:\n            c.append_system_message(SUMMARISE_PROMPT)\n        else:\n            c.append_system_message(PROCESS_PROMPT)\n        msg, token_usage, correction = c.query(text)\n        return msg\n\n    def _process_sections(\n        self,\n        sentences: list,\n        characters_per_paragraph: int,\n    ) -&gt; list:\n        \"\"\"Processes sections of the document. Concatenates sentences until\n        characters_per_paragraph is reached, removing each sentence from the\n        list as it is added to the section to be processed.\n\n        Args:\n        ----\n            sentences (list): list of sentences to summarise\n\n            characters_per_paragraph (int): number of characters per paragraph\n\n        Returns:\n        -------\n            list: list of processed sections\n\n        \"\"\"\n        summarised_sections = []\n        section = \"\"\n        while sentences:\n            sentence = sentences.pop(0)\n            tmp = section + sentence\n            if len(tmp) &lt; characters_per_paragraph and sentences:\n                section += sentence\n            else:\n                if sentences:\n                    sentences.insert(0, sentence)\n                summarised_section = self._process_section(section)\n                # filter \"no content\" sections\n                if not (\"no content\" in summarised_section.lower() and len(summarised_section) &lt; 30):\n                    summarised_sections.append(summarised_section)\n                section = \"\"\n\n        return summarised_sections\n\n    def podcast_to_file(\n        self,\n        path: str,\n        model: str = \"gtts\",\n        voice: str = \"alloy\",\n    ) -&gt; None:\n        \"\"\"Uses text-to-speech to generate audio for the summarised paper podcast.\n\n        Args:\n        ----\n            path (str): path to save audio file to\n\n            model (str): model to use for text-to-speech. Currently supported:\n                'gtts' (Google Text-to-Speech, free),\n                'tts-1' (OpenAI API, paid, prioritises speed),\n                'tts-1-hd' (OpenAI API, paid, prioritises quality)\n\n            voice (str): voice to use for text-to-speech. See OpenAI API\n                documentation for available voices.\n\n        \"\"\"\n        full_text = self.podcast_to_text()\n\n        if model == \"gtts\":\n            audio = gTTS(text=full_text)\n            audio.save(path)\n        else:\n            client = OpenAI()\n\n            # Save the intro to the original file\n            response = client.audio.speech.create(\n                model=model,\n                voice=voice,\n                input=(\n                    \"You are listening to: \\n\\n\" + self.podcast_info + \"\\n\\n\" + \" Text-to-speech generated by OpenAI.\"\n                ),\n            )\n            first_path = path.rsplit(\".\", 1)[0] + \"_0.mp3\"\n            response.stream_to_file(first_path)\n\n            # Concatenate the sections\n            full_text = \"\"\n            for i, section in enumerate(self.processed_sections):\n                full_text += section + \"\\n\\n\"\n\n            # Make sections of 4000 characters max (at sentence boundaries)\n            nltk.download(\"punkt\")\n            tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n            sentences = deque(\n                tokenizer.tokenize(full_text),\n            )  # Use a deque instead of a list\n\n            # Split the text into sections by filling one section until it\n            # exceeds 4000 characters, then starting a new section (not adding\n            # the sentence that would exceed the limit)\n            sections = []\n            section = \"\"\n            while sentences:\n                sentence = sentences[0]\n                tmp = section + sentence\n                if len(tmp) &lt; 4000:\n                    section += sentences.popleft()\n                else:\n                    sections.append(section)\n                    section = \"\"\n\n            sections.append(section)  # Add the penultimate section\n\n            # Last section: conclude the podcast\n            sections.append(\n                f\"This was {self.podcast_info}. Thank you for listening.\",\n            )\n\n            # Save each section to a separate file with an integer suffix\n            for i, section in enumerate(sections):\n                response = client.audio.speech.create(\n                    model=model,\n                    voice=voice,\n                    input=section,\n                )\n                # Insert the integer suffix just before the .mp3 extension\n                section_path = path.rsplit(\".\", 1)[0] + f\"_{i+1}.mp3\"\n                response.stream_to_file(section_path)\n\n    def podcast_to_text(self):\n        \"\"\"Returns the summarised paper podcast as text.\"\"\"\n        full_text = \"You are listening to: \" + self.podcast_info + \"\\n\\n\"\n        for section in self.processed_sections:\n            full_text += section + \"\\n\\n\"\n        return full_text\n</code></pre>"},{"location":"api-docs/podcast/#biochatter.podcast.Podcaster.__init__","title":"<code>__init__(document, model_name='gpt-3.5-turbo')</code>","text":"<p>Orchestrates the podcasting of a document.</p> Source code in <code>biochatter/podcast.py</code> <pre><code>def __init__(\n    self,\n    document: Document,\n    model_name: str = \"gpt-3.5-turbo\",\n) -&gt; None:\n    \"\"\"Orchestrates the podcasting of a document.\"\"\"\n    self.document = document\n    self.model_name = model_name\n</code></pre>"},{"location":"api-docs/podcast/#biochatter.podcast.Podcaster.generate_podcast","title":"<code>generate_podcast(characters_per_paragraph)</code>","text":"<p>Podcasts the document.</p>"},{"location":"api-docs/podcast/#biochatter.podcast.Podcaster.generate_podcast--todo","title":"Todo:","text":"<ul> <li>chain of density prompting for variable summary length</li> </ul> Source code in <code>biochatter/podcast.py</code> <pre><code>def generate_podcast(self, characters_per_paragraph: int) -&gt; None:\n    \"\"\"Podcasts the document.\n\n    Todo:\n    ----\n    - chain of density prompting for variable summary length\n\n    \"\"\"\n    full_text = self.document[0].page_content\n\n    # split text by sentence\n    sentences = self._split_text(full_text)\n\n    # could embed sentences and cluster on cosine similarity to identify\n    # paragraphs here\n\n    # preprocess text\n    for i, sentence in enumerate(sentences):\n        # special cases i.e. and e.g. - if sentence ends with one of these,\n        # append next sentence\n        special_cases = [\"i.e.\", \"e.g.\"]\n        if sentence.endswith(tuple(special_cases)):\n            sentences[i] = sentence + \" \" + sentences[i + 1]\n            del sentences[i + 1]\n\n    # concatenate first 5 sentences for title and author extraction\n    first_5 = \"\\n\".join(sentences[:5])\n    self.podcast_info = self._title_and_authors(first_5)\n\n    # LLM to determine section breaks?\n\n    # go through sections and summarise each\n    self.processed_sections = self._process_sections(\n        sentences,\n        characters_per_paragraph,\n    )\n</code></pre>"},{"location":"api-docs/podcast/#biochatter.podcast.Podcaster.podcast_to_file","title":"<code>podcast_to_file(path, model='gtts', voice='alloy')</code>","text":"<p>Uses text-to-speech to generate audio for the summarised paper podcast.</p> <pre><code>path (str): path to save audio file to\n\nmodel (str): model to use for text-to-speech. Currently supported:\n    'gtts' (Google Text-to-Speech, free),\n    'tts-1' (OpenAI API, paid, prioritises speed),\n    'tts-1-hd' (OpenAI API, paid, prioritises quality)\n\nvoice (str): voice to use for text-to-speech. See OpenAI API\n    documentation for available voices.\n</code></pre> Source code in <code>biochatter/podcast.py</code> <pre><code>def podcast_to_file(\n    self,\n    path: str,\n    model: str = \"gtts\",\n    voice: str = \"alloy\",\n) -&gt; None:\n    \"\"\"Uses text-to-speech to generate audio for the summarised paper podcast.\n\n    Args:\n    ----\n        path (str): path to save audio file to\n\n        model (str): model to use for text-to-speech. Currently supported:\n            'gtts' (Google Text-to-Speech, free),\n            'tts-1' (OpenAI API, paid, prioritises speed),\n            'tts-1-hd' (OpenAI API, paid, prioritises quality)\n\n        voice (str): voice to use for text-to-speech. See OpenAI API\n            documentation for available voices.\n\n    \"\"\"\n    full_text = self.podcast_to_text()\n\n    if model == \"gtts\":\n        audio = gTTS(text=full_text)\n        audio.save(path)\n    else:\n        client = OpenAI()\n\n        # Save the intro to the original file\n        response = client.audio.speech.create(\n            model=model,\n            voice=voice,\n            input=(\n                \"You are listening to: \\n\\n\" + self.podcast_info + \"\\n\\n\" + \" Text-to-speech generated by OpenAI.\"\n            ),\n        )\n        first_path = path.rsplit(\".\", 1)[0] + \"_0.mp3\"\n        response.stream_to_file(first_path)\n\n        # Concatenate the sections\n        full_text = \"\"\n        for i, section in enumerate(self.processed_sections):\n            full_text += section + \"\\n\\n\"\n\n        # Make sections of 4000 characters max (at sentence boundaries)\n        nltk.download(\"punkt\")\n        tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n        sentences = deque(\n            tokenizer.tokenize(full_text),\n        )  # Use a deque instead of a list\n\n        # Split the text into sections by filling one section until it\n        # exceeds 4000 characters, then starting a new section (not adding\n        # the sentence that would exceed the limit)\n        sections = []\n        section = \"\"\n        while sentences:\n            sentence = sentences[0]\n            tmp = section + sentence\n            if len(tmp) &lt; 4000:\n                section += sentences.popleft()\n            else:\n                sections.append(section)\n                section = \"\"\n\n        sections.append(section)  # Add the penultimate section\n\n        # Last section: conclude the podcast\n        sections.append(\n            f\"This was {self.podcast_info}. Thank you for listening.\",\n        )\n\n        # Save each section to a separate file with an integer suffix\n        for i, section in enumerate(sections):\n            response = client.audio.speech.create(\n                model=model,\n                voice=voice,\n                input=section,\n            )\n            # Insert the integer suffix just before the .mp3 extension\n            section_path = path.rsplit(\".\", 1)[0] + f\"_{i+1}.mp3\"\n            response.stream_to_file(section_path)\n</code></pre>"},{"location":"api-docs/podcast/#biochatter.podcast.Podcaster.podcast_to_text","title":"<code>podcast_to_text()</code>","text":"<p>Returns the summarised paper podcast as text.</p> Source code in <code>biochatter/podcast.py</code> <pre><code>def podcast_to_text(self):\n    \"\"\"Returns the summarised paper podcast as text.\"\"\"\n    full_text = \"You are listening to: \" + self.podcast_info + \"\\n\\n\"\n    for section in self.processed_sections:\n        full_text += section + \"\\n\\n\"\n    return full_text\n</code></pre>"},{"location":"api-docs/reflexion/","title":"Reflexion Agent Reference","text":"<p>Here we handle the implementation of a LangGraph-based multi-agent system for reflexion on a user-defined task.</p>"},{"location":"api-docs/reflexion/#the-base-module","title":"The base module","text":""},{"location":"api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent","title":"<code>ReflexionAgent</code>","text":"<p>               Bases: <code>ABC</code></p> <p>LLM agent reflexion framework:</p> <p>start -&gt; draft -&gt; execute tool -&gt; revise -&gt; evaluation -&gt; end                     /|\\                        |                      ---------------------------</p> Source code in <code>biochatter/langgraph_agent_base.py</code> <pre><code>class ReflexionAgent(ABC):\n    r\"\"\"LLM agent reflexion framework:\n\n    start -&gt; draft -&gt; execute tool -&gt; revise -&gt; evaluation -&gt; end\n                        /|\\                        |\n                         ---------------------------\n    \"\"\"\n\n    RECURSION_LIMIT = 30\n\n    def __init__(\n        self,\n        conversation_factory: Callable,\n        max_steps: int | None = 20,\n        agent_logger: ReflexionAgentLogger | None = ReflexionAgentLogger(),\n    ):\n        \"\"\"Args:\n        ----\n          conversation_factory Callable: the callback to create Conversation\n          max_steps int: max steps for reflextion loop\n\n        \"\"\"\n        if max_steps &lt;= 0:\n            max_steps = ReflexionAgent.RECURSION_LIMIT\n        recursion_limit = ReflexionAgent.RECURSION_LIMIT\n        if recursion_limit &lt; max_steps:\n            recursion_limit = max_steps\n        self.initial_responder = None\n        self.revise_responder = None\n        self.max_steps = max_steps\n        self.recursion_limit = recursion_limit\n        self.conversation = conversation_factory()\n        self.agent_logger = agent_logger\n\n    def _should_continue(self, state: list[BaseMessage]):\n        \"\"\"Determine if we need to continue reflexion\n        Args:\n          state List[BaseMessage]: message history\n        \"\"\"\n        num_steps = ReflexionAgent._get_num_iterations(state)\n        if num_steps &gt; self.max_steps:\n            return END\n        return EXECUTE_TOOL_NODE\n\n    @abstractmethod\n    def _tool_function(self, state: list[BaseMessage]) -&gt; ToolMessage:\n        \"\"\"Tool function, execute tool based on initial draft or revised answer\n        Args:\n          state List[BaseMessage]: message history\n        Returns:\n          ToolMessage\n        \"\"\"\n\n    @abstractmethod\n    def _create_initial_responder(\n        self,\n        prompt: str | None = None,\n    ) -&gt; ResponderWithRetries:\n        \"\"\"Draft responder, draft initial answer\n        Args:\n          prompt str: prompt for LLM to draft initial answer\n        \"\"\"\n\n    @abstractmethod\n    def _create_revise_responder(\n        self,\n        prompt: str | None = None,\n    ) -&gt; ResponderWithRetries:\n        \"\"\"Revise responder, revise answer according to tool function result\n        Args:\n          prompt str: prompt for LLM to draft initial answer\n        \"\"\"\n\n    @abstractmethod\n    def _parse_final_result(\n        self,\n        messages: list[BaseMessage],\n    ) -&gt; ReflexionAgentResult:\n        \"\"\"Parse the result of the last step\n        Args:\n          output BaseMessage: last step message\n        Returns:\n          ReflexionAgentResult: the parsed reuslt of the last step\n        \"\"\"\n\n    def get_logs(self):\n        return self.agent_logger.logs\n\n    @staticmethod\n    def _get_num_iterations(state: list[BaseMessage]):\n        \"\"\"Calculate iteration number\n        Args:\n          state List[BaseMessage]: message history\n\n        Returns\n        -------\n          int: the iterations number\n\n        \"\"\"\n        i = 0\n        for m in state[::-1]:\n            if not isinstance(m, (ToolMessage, AIMessage)):\n                break\n            i += 1\n        return i\n\n    @staticmethod\n    def _get_user_question(state: list[BaseMessage]):\n        \"\"\"Get user's question from messages array\"\"\"\n        for m in state:\n            if not isinstance(m, HumanMessage):\n                continue\n            return m.content\n        return None\n\n    @staticmethod\n    def _get_last_tool_result(messages: list[BaseMessage]):\n        \"\"\"Get result of the last tool node\"\"\"\n        for m in messages[::-1]:\n            if not isinstance(m, ToolMessage):\n                continue\n            content = json.loads(m.content)\n            return content[\"result\"]\n        return None\n\n    def _build_graph(self, prompt: str | None = None):\n        \"\"\"Build Langgraph graph for execution of chained LLM processes.\n\n        Args:\n        ----\n          prompt str: prompt for LLM\n\n        Returns:\n        -------\n          CompiledGraph | None: a Langgraph graph or None in case of errors\n\n        \"\"\"\n        try:\n            self.initial_responder = self._create_initial_responder(prompt)\n            self.revise_responder = self._create_revise_responder(prompt)\n            builder = MessageGraph()\n            builder.add_node(DRAFT_NODE, self.initial_responder.respond)\n            builder.add_node(EXECUTE_TOOL_NODE, self._tool_function)\n            builder.add_node(REVISE_NODE, self.revise_responder.respond)\n            builder.add_edge(DRAFT_NODE, EXECUTE_TOOL_NODE)\n            builder.add_edge(EXECUTE_TOOL_NODE, REVISE_NODE)\n\n            builder.add_conditional_edges(REVISE_NODE, self._should_continue)\n            builder.set_entry_point(DRAFT_NODE)\n            graph = builder.compile()\n            return graph\n        except Exception as e:\n            logger.error(e)\n            return None\n\n    def _execute_graph(\n        self,\n        graph: CompiledGraph | None = None,\n        question: str | None = \"\",\n    ) -&gt; ReflexionAgentResult:\n        \"\"\"Execute Langgraph graph\n        Args:\n          graph CompiledGraph: Langgraph graph\n          question str: user question\n\n        Returns\n        -------\n          answer str | None: string answer parsed from Langgraph graph execution\n\n        \"\"\"\n        if graph is None:\n            return None\n        if len(question) == 0:\n            return None\n\n        events = graph.stream(\n            [HumanMessage(content=question)],\n            {\n                \"recursion_limit\": self.recursion_limit,\n            },\n        )\n        messages = [HumanMessage(content=question)]\n        for i, step in enumerate(events):\n            if isinstance(step, list):\n                node, output = (f\"{i}\", step[i])\n            else:\n                node, output = next(iter(step.items()))\n            self.agent_logger.log_step_message(i + 1, node, output)\n            messages.append(output)\n\n        final_result = self._parse_final_result(messages)\n        self.agent_logger.log_final_result(final_result)\n        return final_result\n\n    def execute(\n        self,\n        question: str,\n        prompt: str | None = None,\n    ) -&gt; ReflexionAgentResult:\n        \"\"\"Execute ReflexionAgent. Wrapper for building a graph and executing it,\n        returning the final answer.\n\n        Args:\n        ----\n          question str: user question\n          prompt str: user prompt\n\n        Returns:\n        -------\n          answer str | None: If it executes successfully, an answer to the\n            question will be returned, otherwise, it returns None\n\n        \"\"\"\n        if len(question) == 0:\n            return None\n        graph = self._build_graph(prompt)\n        return self._execute_graph(graph, question)\n</code></pre>"},{"location":"api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent.__init__","title":"<code>__init__(conversation_factory, max_steps=20, agent_logger=ReflexionAgentLogger())</code>","text":"<p>conversation_factory Callable: the callback to create Conversation   max_steps int: max steps for reflextion loop</p> Source code in <code>biochatter/langgraph_agent_base.py</code> <pre><code>def __init__(\n    self,\n    conversation_factory: Callable,\n    max_steps: int | None = 20,\n    agent_logger: ReflexionAgentLogger | None = ReflexionAgentLogger(),\n):\n    \"\"\"Args:\n    ----\n      conversation_factory Callable: the callback to create Conversation\n      max_steps int: max steps for reflextion loop\n\n    \"\"\"\n    if max_steps &lt;= 0:\n        max_steps = ReflexionAgent.RECURSION_LIMIT\n    recursion_limit = ReflexionAgent.RECURSION_LIMIT\n    if recursion_limit &lt; max_steps:\n        recursion_limit = max_steps\n    self.initial_responder = None\n    self.revise_responder = None\n    self.max_steps = max_steps\n    self.recursion_limit = recursion_limit\n    self.conversation = conversation_factory()\n    self.agent_logger = agent_logger\n</code></pre>"},{"location":"api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgent.execute","title":"<code>execute(question, prompt=None)</code>","text":"<p>Execute ReflexionAgent. Wrapper for building a graph and executing it, returning the final answer.</p> <p>question str: user question   prompt str: user prompt</p> <p>answer str | None: If it executes successfully, an answer to the     question will be returned, otherwise, it returns None</p> Source code in <code>biochatter/langgraph_agent_base.py</code> <pre><code>def execute(\n    self,\n    question: str,\n    prompt: str | None = None,\n) -&gt; ReflexionAgentResult:\n    \"\"\"Execute ReflexionAgent. Wrapper for building a graph and executing it,\n    returning the final answer.\n\n    Args:\n    ----\n      question str: user question\n      prompt str: user prompt\n\n    Returns:\n    -------\n      answer str | None: If it executes successfully, an answer to the\n        question will be returned, otherwise, it returns None\n\n    \"\"\"\n    if len(question) == 0:\n        return None\n    graph = self._build_graph(prompt)\n    return self._execute_graph(graph, question)\n</code></pre>"},{"location":"api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgentLogger","title":"<code>ReflexionAgentLogger</code>","text":"Source code in <code>biochatter/langgraph_agent_base.py</code> <pre><code>class ReflexionAgentLogger:\n    def __init__(self) -&gt; None:\n        self._logs: str = \"\"\n\n    def log_step_message(\n        self,\n        step: int,\n        node_name: str,\n        output: BaseMessage,\n    ):\n        \"\"\"Log step message\n        Args:\n          step int: step index\n          output BaseMessage: step message\n        \"\"\"\n\n    def log_final_result(self, final_result: dict[str, Any]) -&gt; None:\n        \"\"\"Log final result\n        Args:\n          output BaseMessage: last step message\n        \"\"\"\n\n    def _log_message(\n        self,\n        msg: str = \"\",\n        level: Literal[\"info\", \"error\", \"warn\"] | None = \"info\",\n    ):\n        \"\"\"Save log message\n\n        Args:\n        ----\n            msg: the message to be logged\n\n            level: the log level to write\n\n        \"\"\"\n        logger_func = logger.info if level == \"info\" else (logger.error if level == \"error\" else logger.warning)\n        logger_func(msg)\n        self._logs = self._logs + f\"[{level}]\" + f\"{datetime.now().isoformat()} - {msg}\\n\"\n\n    @property\n    def logs(self):\n        return self._logs\n</code></pre>"},{"location":"api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgentLogger.log_final_result","title":"<code>log_final_result(final_result)</code>","text":"<p>Log final result Args:   output BaseMessage: last step message</p> Source code in <code>biochatter/langgraph_agent_base.py</code> <pre><code>def log_final_result(self, final_result: dict[str, Any]) -&gt; None:\n    \"\"\"Log final result\n    Args:\n      output BaseMessage: last step message\n    \"\"\"\n</code></pre>"},{"location":"api-docs/reflexion/#biochatter.langgraph_agent_base.ReflexionAgentLogger.log_step_message","title":"<code>log_step_message(step, node_name, output)</code>","text":"<p>Log step message Args:   step int: step index   output BaseMessage: step message</p> Source code in <code>biochatter/langgraph_agent_base.py</code> <pre><code>def log_step_message(\n    self,\n    step: int,\n    node_name: str,\n    output: BaseMessage,\n):\n    \"\"\"Log step message\n    Args:\n      step int: step index\n      output BaseMessage: step message\n    \"\"\"\n</code></pre>"},{"location":"api-docs/reflexion/#biochatter.langgraph_agent_base.ResponderWithRetries","title":"<code>ResponderWithRetries</code>","text":"<p>Raise request to LLM with 3 retries</p> Source code in <code>biochatter/langgraph_agent_base.py</code> <pre><code>class ResponderWithRetries:\n    \"\"\"Raise request to LLM with 3 retries\"\"\"\n\n    def __init__(self, runnable, validator):\n        \"\"\"Args:\n        ----\n        runnable: LLM agent\n        validator: used to validate response\n\n        \"\"\"\n        self.runnable = runnable\n        self.validator = validator\n\n    @traceable\n    def respond(self, state: list[BaseMessage]):\n        \"\"\"Invoke LLM agent, this function will be called by LangGraph\n        Args:\n        state List[BaseMessage]: message history\n        \"\"\"\n        response = []\n        for attempt in range(3):\n            try:\n                response = self.runnable.invoke({\"messages\": state})\n                self.validator.invoke(response)\n                return response\n            except ValidationError as e:\n                state = state + [HumanMessage(content=repr(e))]\n        return response\n</code></pre>"},{"location":"api-docs/reflexion/#biochatter.langgraph_agent_base.ResponderWithRetries.__init__","title":"<code>__init__(runnable, validator)</code>","text":""},{"location":"api-docs/reflexion/#biochatter.langgraph_agent_base.ResponderWithRetries.__init__--args","title":"Args:","text":"<p>runnable: LLM agent validator: used to validate response</p> Source code in <code>biochatter/langgraph_agent_base.py</code> <pre><code>def __init__(self, runnable, validator):\n    \"\"\"Args:\n    ----\n    runnable: LLM agent\n    validator: used to validate response\n\n    \"\"\"\n    self.runnable = runnable\n    self.validator = validator\n</code></pre>"},{"location":"api-docs/reflexion/#biochatter.langgraph_agent_base.ResponderWithRetries.respond","title":"<code>respond(state)</code>","text":"<p>Invoke LLM agent, this function will be called by LangGraph Args: state List[BaseMessage]: message history</p> Source code in <code>biochatter/langgraph_agent_base.py</code> <pre><code>@traceable\ndef respond(self, state: list[BaseMessage]):\n    \"\"\"Invoke LLM agent, this function will be called by LangGraph\n    Args:\n    state List[BaseMessage]: message history\n    \"\"\"\n    response = []\n    for attempt in range(3):\n        try:\n            response = self.runnable.invoke({\"messages\": state})\n            self.validator.invoke(response)\n            return response\n        except ValidationError as e:\n            state = state + [HumanMessage(content=repr(e))]\n    return response\n</code></pre>"},{"location":"api-docs/reflexion/#the-kg-based-reflexion-agent","title":"The KG-based reflexion agent","text":""},{"location":"api-docs/reflexion/#biochatter.kg_langgraph_agent.GenerateQuery","title":"<code>GenerateQuery</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Generate the query.</p> Source code in <code>biochatter/kg_langgraph_agent.py</code> <pre><code>class GenerateQuery(BaseModel):\n    \"\"\"Generate the query.\"\"\"\n\n    answer: str = Field(\n        description=\"Cypher query for graph database according to user's question.\",\n    )\n    reflection: str = Field(\n        description=\"Your reflection on the initial answer, critique of what to improve\",\n    )\n    search_queries: list[str] = Field(description=SEARCH_QUERIES_DESCRIPTION)\n</code></pre>"},{"location":"api-docs/reflexion/#biochatter.kg_langgraph_agent.KGQueryReflexionAgent","title":"<code>KGQueryReflexionAgent</code>","text":"<p>               Bases: <code>ReflexionAgent</code></p> Source code in <code>biochatter/kg_langgraph_agent.py</code> <pre><code>class KGQueryReflexionAgent(ReflexionAgent):\n    def __init__(\n        self,\n        conversation_factory: Callable,\n        connection_args: dict[str, str],\n        query_lang: str | None = \"Cypher\",\n        max_steps: int | None = 20,\n    ):\n        r\"\"\"LLM agent reflexion framework:\n\n        start -&gt; draft -&gt; execute tool -&gt; revise -&gt; evaluation -&gt; end\n                            /|\\                        |\n                             ---------------------------\n\n        Adapts base class to build and refine a knowledge graph query, default\n        language Cypher. Currently hardcoded to connect to Neo4j for the KG\n        query implementation.\n\n        Args:\n        ----\n            conversation_factory: function to return the Conversation to use for\n                the LLM connection\n\n            connection_args: connection arguments for connecting to the database\n\n            query_lang: graph query language to use\n\n            max_steps: the maximum number of steps to execute in the graph\n\n        \"\"\"\n        super().__init__(\n            conversation_factory,\n            max_steps,\n            agent_logger=KGQueryReflexionAgentLogger(),\n        )\n        self.actor_prompt_template = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    (\n                        \"As a senior biomedical researcher and graph database expert, \"\n                        f\"your task is to generate '{query_lang}' queries to extract data from our graph database based on the user's question. \"\n                        \"\"\"Current time {time}. {instruction}\"\"\"\n                    ),\n                ),\n                MessagesPlaceholder(variable_name=\"messages\"),\n                (\n                    \"human\",\n                    (\n                        \"Note: 1. Only generate query according to the user's question above.\\n\"\n                        \"2. Please limit the results to a maximum of 30 items\"\n                    ),\n                ),\n            ],\n        ).partial(time=lambda: datetime.now().isoformat())\n        self.parser = JsonOutputToolsParser(return_id=True)\n        self.connection_args = connection_args\n        self.neodriver = None\n\n    def _connect_db(self):\n        if self.neodriver is not None:\n            return\n        try:\n            db_uri = \"bolt://\" + self.connection_args.get(\"host\") + \":\" + self.connection_args.get(\"port\")\n            self.neodriver = nu.Driver(\n                db_name=self.connection_args.get(\"db_name\") or \"neo4j\",\n                db_uri=db_uri,\n            )\n        except Exception as e:\n            logger.error(e)\n\n    def _query_graph_database(self, query: str):\n        \"\"\"Try to execute the query in Neo4j and return the result.\n\n        Args:\n        ----\n            query: the query string\n\n        \"\"\"\n        self._connect_db()\n        try:\n            return self.neodriver.query(query)\n        except Exception as e:\n            logger.error(str(e))\n            return []  # empty result\n\n    def _create_initial_responder(\n        self,\n        prompt: str | None = None,\n    ) -&gt; ResponderWithRetries:\n        llm: ChatOpenAI = self.conversation.chat\n        initial_chain = self.actor_prompt_template.partial(\n            instruction=prompt if prompt is not None else \"\",\n        ) | llm.bind_tools(\n            tools=[GenerateQuery],\n            tool_choice=\"GenerateQuery\",\n        )\n        validator = PydanticToolsParser(tools=[GenerateQuery])\n        return ResponderWithRetries(runnable=initial_chain, validator=validator)\n\n    def _create_revise_responder(\n        self,\n        prompt: str | None = None,\n    ) -&gt; ResponderWithRetries:\n        revision_instruction = \"\"\"\n        Revise your previous query using the query result and follow the guidelines:\n        1. If you consistently obtain empty results, please consider removing constraints such as relationship constraints to try to obtain a result.\n        2. You should use previous critique to improve your query.\n        3. Only generate a query without returning any other text.\n        \"\"\"\n        llm: ChatOpenAI = self.conversation.chat\n        revision_chain = self.actor_prompt_template.partial(\n            instruction=revision_instruction,\n        ) | llm.bind_tools(\n            tools=[ReviseQuery],\n            tool_choice=\"ReviseQuery\",\n        )\n        validator = PydanticToolsParser(tools=[ReviseQuery])\n        return ResponderWithRetries(\n            runnable=revision_chain,\n            validator=validator,\n        )\n\n    def _tool_function(self, state: list[BaseMessage]):\n        tool_message: AIMessage = state[-1]\n        parsed_tool_messages = self.parser.invoke(tool_message)\n        results = []\n        for parsed_message in parsed_tool_messages:\n            try:\n                parsed_args = parsed_message[\"args\"]\n                query = (\n                    parsed_args[REVISED_QUERY]\n                    if REVISED_QUERY in parsed_args\n                    else (parsed_args[REVISED_QUERY_DESCRIPTION] if REVISED_QUERY_DESCRIPTION in parsed_args else None)\n                )\n                if query is not None:\n                    result = self._query_graph_database(query)\n                    results.append({\"query\": query, \"result\": result[0]})\n                    continue\n                queries = (\n                    parsed_args[SEARCH_QUERIES]\n                    if SEARCH_QUERIES in parsed_args\n                    else parsed_args[SEARCH_QUERIES_DESCRIPTION]\n                )\n                queries = queries if len(queries) &gt; 0 else [parsed_args[ANSWER]]\n                for query in queries:\n                    result = self._query_graph_database(query)\n                    results.append(\n                        {\n                            \"query\": query,\n                            \"result\": result[0] if len(result) &gt; 0 else [],\n                        },\n                    )\n            except Exception as e:\n                logger.error(f\"Error occurred: {e!s}\")\n\n        content = None\n        if len(results) &gt; 1:\n            # If there are multiple results, we only return\n            # the first non-empty result\n            for res in results:\n                if res[\"result\"] and len(res[\"result\"]) &gt; 0:\n                    content = json.dumps(res)\n        if content is None:\n            content = json.dumps(results[0]) if len(results) &gt; 0 else \"\"\n        return ToolMessage(\n            content=content,\n            tool_call_id=parsed_message[\"id\"],\n        )\n\n    @staticmethod\n    def _get_last_tool_results_num(state: list[BaseMessage]):\n        i = 0\n        for m in state[::-1]:\n            if not isinstance(m, ToolMessage):\n                continue\n            message: ToolMessage = m\n            logger.info(f\"query result: {message.content}\")\n            results = (\n                json.loads(message.content)\n                if message.content is not None and len(message.content) &gt; 0\n                else {\"result\": []}\n            )\n            empty = True\n            if len(results[\"result\"]) &gt; 0:\n                # check if it is really not empty, remove the case: {\"result\": [{\"c.name\": None}]}\n                for res in results[\"result\"]:\n                    for k in res.keys():\n                        if res[k] is None:\n                            continue\n                        if isinstance(res[k], str) and (res[k] == \"None\" or res[k] == \"null\"):\n                            continue\n                        empty = False\n                        break\n                    if not empty:\n                        break\n            return len(results[\"result\"]) if not empty else 0\n\n        return 0\n\n    def _get_last_score(self, state: list[BaseMessage]) -&gt; int | None:\n        for m in state[::-1]:\n            if not isinstance(m, AIMessage):\n                continue\n            message: AIMessage = m\n            parsed_msg = self.parser.invoke(message)\n            try:\n                score = parsed_msg[0][\"args\"][\"score\"]\n                return int(score)\n            except Exception:\n                return None\n        return None\n\n    def _should_continue(self, state: list[BaseMessage]):\n        res = super()._should_continue(state)\n        if res == END:\n            return res\n        score = self._get_last_score(state)\n        if score is not None and score &gt;= 7:\n            return END\n        query_results_num = KGQueryReflexionAgent._get_last_tool_results_num(\n            state,\n        )\n        return END if query_results_num &gt; 0 else EXECUTE_TOOL_NODE\n\n    def _parse_final_result(\n        self,\n        messages: list[BaseMessage],\n    ) -&gt; ReflexionAgentResult:\n        output = messages[-1]\n        result = self.parser.invoke(output)[0][\"args\"]\n        tool_result = ReflexionAgent._get_last_tool_result(messages)\n        return ReflexionAgentResult(\n            answer=result[\"answer\"] if \"answer\" in result else None,\n            tool_result=tool_result,\n        )\n</code></pre>"},{"location":"api-docs/reflexion/#biochatter.kg_langgraph_agent.KGQueryReflexionAgent.__init__","title":"<code>__init__(conversation_factory, connection_args, query_lang='Cypher', max_steps=20)</code>","text":"<p>LLM agent reflexion framework:</p> <p>start -&gt; draft -&gt; execute tool -&gt; revise -&gt; evaluation -&gt; end                     /|\\                        |                      ---------------------------</p> <p>Adapts base class to build and refine a knowledge graph query, default language Cypher. Currently hardcoded to connect to Neo4j for the KG query implementation.</p> <pre><code>conversation_factory: function to return the Conversation to use for\n    the LLM connection\n\nconnection_args: connection arguments for connecting to the database\n\nquery_lang: graph query language to use\n\nmax_steps: the maximum number of steps to execute in the graph\n</code></pre> Source code in <code>biochatter/kg_langgraph_agent.py</code> <pre><code>def __init__(\n    self,\n    conversation_factory: Callable,\n    connection_args: dict[str, str],\n    query_lang: str | None = \"Cypher\",\n    max_steps: int | None = 20,\n):\n    r\"\"\"LLM agent reflexion framework:\n\n    start -&gt; draft -&gt; execute tool -&gt; revise -&gt; evaluation -&gt; end\n                        /|\\                        |\n                         ---------------------------\n\n    Adapts base class to build and refine a knowledge graph query, default\n    language Cypher. Currently hardcoded to connect to Neo4j for the KG\n    query implementation.\n\n    Args:\n    ----\n        conversation_factory: function to return the Conversation to use for\n            the LLM connection\n\n        connection_args: connection arguments for connecting to the database\n\n        query_lang: graph query language to use\n\n        max_steps: the maximum number of steps to execute in the graph\n\n    \"\"\"\n    super().__init__(\n        conversation_factory,\n        max_steps,\n        agent_logger=KGQueryReflexionAgentLogger(),\n    )\n    self.actor_prompt_template = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                (\n                    \"As a senior biomedical researcher and graph database expert, \"\n                    f\"your task is to generate '{query_lang}' queries to extract data from our graph database based on the user's question. \"\n                    \"\"\"Current time {time}. {instruction}\"\"\"\n                ),\n            ),\n            MessagesPlaceholder(variable_name=\"messages\"),\n            (\n                \"human\",\n                (\n                    \"Note: 1. Only generate query according to the user's question above.\\n\"\n                    \"2. Please limit the results to a maximum of 30 items\"\n                ),\n            ),\n        ],\n    ).partial(time=lambda: datetime.now().isoformat())\n    self.parser = JsonOutputToolsParser(return_id=True)\n    self.connection_args = connection_args\n    self.neodriver = None\n</code></pre>"},{"location":"api-docs/reflexion/#biochatter.kg_langgraph_agent.ReviseQuery","title":"<code>ReviseQuery</code>","text":"<p>               Bases: <code>GenerateQuery</code></p> <p>Revise your previous query according to your question.</p> Source code in <code>biochatter/kg_langgraph_agent.py</code> <pre><code>class ReviseQuery(GenerateQuery):\n    \"\"\"Revise your previous query according to your question.\"\"\"\n\n    revised_query: str = Field(description=REVISED_QUERY_DESCRIPTION)\n    score: str = Field(description=SCORE_DESCRIPTION)\n</code></pre>"},{"location":"api-docs/vectorstore/","title":"Vectorstore Agent Reference","text":"<p>Here we handle the application of vectorstore services to retrieval-augmented generation tasks by embedding documents and connections/management of vectorstore services and semantic search.</p>"},{"location":"api-docs/vectorstore/#vectorstore-implementation","title":"Vectorstore Implementation","text":"<p>Module for handling document embedding, storage and retrieval.</p> <p>This module provides classes for splitting documents into chunks, embedding them using various LLM providers (OpenAI, Xinference, Ollama), storing them in vector databases, and retrieving relevant passages through similarity search.</p>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder","title":"<code>DocumentEmbedder</code>","text":"<p>Handle retrieval-augmented generation (RAG) functionality of BioChatter.</p> <p>This class is responsible for: - Splitting text documents into manageable chunks - Embedding these chunks using various LLM providers - Storing embeddings in vector databases - Performing similarity searches for retrieval</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>class DocumentEmbedder:\n    \"\"\"Handle retrieval-augmented generation (RAG) functionality of BioChatter.\n\n    This class is responsible for:\n    - Splitting text documents into manageable chunks\n    - Embedding these chunks using various LLM providers\n    - Storing embeddings in vector databases\n    - Performing similarity searches for retrieval\n    \"\"\"\n\n    def __init__(\n        self,\n        used: bool = False,\n        online: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: list | None = None,\n        n_results: int = 3,\n        model: str | None = \"text-embedding-ada-002\",\n        vector_db_vendor: str | None = None,\n        connection_args: dict | None = None,\n        embedding_collection_name: str | None = None,\n        metadata_collection_name: str | None = None,\n        base_url: str | None = None,\n        embeddings: OpenAIEmbeddings | XinferenceEmbeddings | OllamaEmbeddings | AzureOpenAIEmbeddings | None = None,\n        documentids_workspace: list[str] | None = None,\n    ) -&gt; None:\n        r\"\"\"Initialize the DocumentEmbedder with the specified configuration.\n\n        Args:\n        ----\n            used (bool, optional): whether RAG has been used (frontend setting).\n                Defaults to False.\n\n            online (bool, optional): whether we are running the frontend online.\n                Defaults to False.\n\n            chunk_size (int, optional): size of chunks to split text into.\n                Defaults to 1000.\n\n            chunk_overlap (int, optional): overlap between chunks.\n                Defaults to 0.\n\n            split_by_characters (bool, optional): whether to split by characters\n                or tokens. Defaults to True.\n\n            separators (Optional[list], optional): list of separators to use\n                when splitting by characters. Defaults to [\" \", \",\", \"\\n\"].\n\n            n_results (int, optional): number of results to return from\n                similarity search. Defaults to 3.\n\n            model (Optional[str], optional): name of model to use for\n                embeddings. Defaults to 'text-embedding-ada-002'.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n                to use. Defaults to Milvus.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n                vector database connection. Defaults to None.\n\n            base_url (Optional[str], optional): base url of OpenAI API.\n\n            embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n                optional): Embeddings object to use. Defaults to OpenAI.\n\n            documentids_workspace (Optional[List[str]], optional): a list of\n                document IDs that defines the scope within which RAG operations\n                (remove, similarity search, and get all) occur. Defaults to\n                None, which means the operations will be performed across all\n                documents in the database.\n\n        \"\"\"\n        self.used = used\n        self.online = online\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.separators = separators or [\" \", \",\", \"\\n\"]\n        self.n_results = n_results\n        self.split_by_characters = split_by_characters\n        self.model_name = model\n\n        # TODO API Key handling to central config?\n        if base_url:\n            openai.api_base = base_url\n\n        self.embeddings = embeddings\n\n        # connection arguments\n        self.connection_args = connection_args or {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"19530\",\n        }\n        self.embedding_collection_name = embedding_collection_name\n        self.metadata_collection_name = metadata_collection_name\n        self.documentids_workspace = documentids_workspace\n\n        # TODO: vector db selection\n        self.vector_db_vendor = vector_db_vendor or \"milvus\"\n        # instantiate VectorDatabaseHost\n        self.database_host = None\n        self._init_database_host()\n\n    def _set_embeddings(\n        self,\n        embeddings: (OpenAIEmbeddings | XinferenceEmbeddings | OllamaEmbeddings | AzureOpenAIEmbeddings),\n    ) -&gt; None:\n        print(\"setting embedder\")\n        self.embeddings = embeddings\n\n    def _init_database_host(self) -&gt; None:\n        if self.vector_db_vendor == \"milvus\":\n            self.database_host = VectorDatabaseAgentMilvus(\n                embedding_func=self.embeddings,\n                connection_args=self.connection_args,\n                embedding_collection_name=self.embedding_collection_name,\n                metadata_collection_name=self.metadata_collection_name,\n            )\n        else:\n            raise NotImplementedError(self.vector_db_vendor)\n\n    def set_chunk_size(self, chunk_size: int) -&gt; None:\n        \"\"\"Set the chunk size for the text splitter.\"\"\"\n        self.chunk_size = chunk_size\n\n    def set_chunk_overlap(self, chunk_overlap: int) -&gt; None:\n        \"\"\"Set the chunk overlap for the text splitter.\"\"\"\n        self.chunk_overlap = chunk_overlap\n\n    def set_separators(self, separators: list) -&gt; None:\n        \"\"\"Set the separators for the text splitter.\"\"\"\n        self.separators = separators\n\n    def _characters_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        return RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=self.separators,\n        )\n\n    def _tokens_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        DEFAULT_OPENAI_MODEL = \"gpt-3.5-turbo\"\n        HUGGINGFACE_MODELS = [\"bigscience/bloom\"]\n        if self.model_name and self.model_name in HUGGINGFACE_MODELS:\n            tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n            return RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n                tokenizer,\n                chunk_size=self.chunk_size,\n                chunk_overlap=self.chunk_overlap,\n                separators=self.separators,\n            )\n\n        return RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            encoding_name=\"\",\n            model_name=(DEFAULT_OPENAI_MODEL if not self.model_name else self.model_name),\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=self.separators,\n        )\n\n    def _text_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        return self._characters_splitter() if self.split_by_characters else self._tokens_splitter()\n\n    def save_document(self, doc: list[Document]) -&gt; str:\n        \"\"\"Save a list of documents to the vector database.\n\n        Args:\n        ----\n            doc (List[Document]): document content, read with `DocumentReader`\n                functions `load_document()`, `document_from_pdf()`, or\n                `document_from_txt()`\n\n        Returns:\n        -------\n            str: document id, which can be used to remove an uploaded document\n                with `remove_document()`\n\n        \"\"\"\n        splitted = self._split_document(doc)\n        return self._store_embeddings(splitted)\n\n    def _split_document(self, document: list[Document]) -&gt; list[Document]:\n        \"\"\"Split a document into chunks.\"\"\"\n        text_splitter = self._text_splitter()\n        return text_splitter.split_documents(document)\n\n    def _store_embeddings(self, doc: list[Document]) -&gt; str:\n        \"\"\"Store embeddings for a list of documents.\"\"\"\n        return self.database_host.store_embeddings(documents=doc)\n\n    def connect(self) -&gt; None:\n        \"\"\"Connect to the vector database.\"\"\"\n        self.database_host.connect()\n\n    def get_all_documents(self) -&gt; list[dict]:\n        \"\"\"Get all documents from the vector database.\"\"\"\n        return self.database_host.get_all_documents(\n            doc_ids=self.documentids_workspace,\n        )\n\n    def remove_document(self, doc_id: str) -&gt; None:\n        \"\"\"Remove a document from the vector database.\"\"\"\n        return self.database_host.remove_document(\n            doc_id,\n            self.documentids_workspace,\n        )\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.__init__","title":"<code>__init__(used=False, online=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='text-embedding-ada-002', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, base_url=None, embeddings=None, documentids_workspace=None)</code>","text":"<p>Initialize the DocumentEmbedder with the specified configuration.</p> <pre><code>used (bool, optional): whether RAG has been used (frontend setting).\n    Defaults to False.\n\nonline (bool, optional): whether we are running the frontend online.\n    Defaults to False.\n\nchunk_size (int, optional): size of chunks to split text into.\n    Defaults to 1000.\n\nchunk_overlap (int, optional): overlap between chunks.\n    Defaults to 0.\n\nsplit_by_characters (bool, optional): whether to split by characters\n    or tokens. Defaults to True.\n\nseparators (Optional[list], optional): list of separators to use\n    when splitting by characters. Defaults to [\" \", \",\", \"\\n\"].\n\nn_results (int, optional): number of results to return from\n    similarity search. Defaults to 3.\n\nmodel (Optional[str], optional): name of model to use for\n    embeddings. Defaults to 'text-embedding-ada-002'.\n\nvector_db_vendor (Optional[str], optional): name of vector database\n    to use. Defaults to Milvus.\n\nconnection_args (Optional[dict], optional): arguments to pass to\n    vector database connection. Defaults to None.\n\nbase_url (Optional[str], optional): base url of OpenAI API.\n\nembeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n    optional): Embeddings object to use. Defaults to OpenAI.\n\ndocumentids_workspace (Optional[List[str]], optional): a list of\n    document IDs that defines the scope within which RAG operations\n    (remove, similarity search, and get all) occur. Defaults to\n    None, which means the operations will be performed across all\n    documents in the database.\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    used: bool = False,\n    online: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: list | None = None,\n    n_results: int = 3,\n    model: str | None = \"text-embedding-ada-002\",\n    vector_db_vendor: str | None = None,\n    connection_args: dict | None = None,\n    embedding_collection_name: str | None = None,\n    metadata_collection_name: str | None = None,\n    base_url: str | None = None,\n    embeddings: OpenAIEmbeddings | XinferenceEmbeddings | OllamaEmbeddings | AzureOpenAIEmbeddings | None = None,\n    documentids_workspace: list[str] | None = None,\n) -&gt; None:\n    r\"\"\"Initialize the DocumentEmbedder with the specified configuration.\n\n    Args:\n    ----\n        used (bool, optional): whether RAG has been used (frontend setting).\n            Defaults to False.\n\n        online (bool, optional): whether we are running the frontend online.\n            Defaults to False.\n\n        chunk_size (int, optional): size of chunks to split text into.\n            Defaults to 1000.\n\n        chunk_overlap (int, optional): overlap between chunks.\n            Defaults to 0.\n\n        split_by_characters (bool, optional): whether to split by characters\n            or tokens. Defaults to True.\n\n        separators (Optional[list], optional): list of separators to use\n            when splitting by characters. Defaults to [\" \", \",\", \"\\n\"].\n\n        n_results (int, optional): number of results to return from\n            similarity search. Defaults to 3.\n\n        model (Optional[str], optional): name of model to use for\n            embeddings. Defaults to 'text-embedding-ada-002'.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n            to use. Defaults to Milvus.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection. Defaults to None.\n\n        base_url (Optional[str], optional): base url of OpenAI API.\n\n        embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n            optional): Embeddings object to use. Defaults to OpenAI.\n\n        documentids_workspace (Optional[List[str]], optional): a list of\n            document IDs that defines the scope within which RAG operations\n            (remove, similarity search, and get all) occur. Defaults to\n            None, which means the operations will be performed across all\n            documents in the database.\n\n    \"\"\"\n    self.used = used\n    self.online = online\n    self.chunk_size = chunk_size\n    self.chunk_overlap = chunk_overlap\n    self.separators = separators or [\" \", \",\", \"\\n\"]\n    self.n_results = n_results\n    self.split_by_characters = split_by_characters\n    self.model_name = model\n\n    # TODO API Key handling to central config?\n    if base_url:\n        openai.api_base = base_url\n\n    self.embeddings = embeddings\n\n    # connection arguments\n    self.connection_args = connection_args or {\n        \"host\": \"127.0.0.1\",\n        \"port\": \"19530\",\n    }\n    self.embedding_collection_name = embedding_collection_name\n    self.metadata_collection_name = metadata_collection_name\n    self.documentids_workspace = documentids_workspace\n\n    # TODO: vector db selection\n    self.vector_db_vendor = vector_db_vendor or \"milvus\"\n    # instantiate VectorDatabaseHost\n    self.database_host = None\n    self._init_database_host()\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.connect","title":"<code>connect()</code>","text":"<p>Connect to the vector database.</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"Connect to the vector database.\"\"\"\n    self.database_host.connect()\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.get_all_documents","title":"<code>get_all_documents()</code>","text":"<p>Get all documents from the vector database.</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def get_all_documents(self) -&gt; list[dict]:\n    \"\"\"Get all documents from the vector database.\"\"\"\n    return self.database_host.get_all_documents(\n        doc_ids=self.documentids_workspace,\n    )\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.remove_document","title":"<code>remove_document(doc_id)</code>","text":"<p>Remove a document from the vector database.</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def remove_document(self, doc_id: str) -&gt; None:\n    \"\"\"Remove a document from the vector database.\"\"\"\n    return self.database_host.remove_document(\n        doc_id,\n        self.documentids_workspace,\n    )\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.save_document","title":"<code>save_document(doc)</code>","text":"<p>Save a list of documents to the vector database.</p> <pre><code>doc (List[Document]): document content, read with `DocumentReader`\n    functions `load_document()`, `document_from_pdf()`, or\n    `document_from_txt()`\n</code></pre> <pre><code>str: document id, which can be used to remove an uploaded document\n    with `remove_document()`\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def save_document(self, doc: list[Document]) -&gt; str:\n    \"\"\"Save a list of documents to the vector database.\n\n    Args:\n    ----\n        doc (List[Document]): document content, read with `DocumentReader`\n            functions `load_document()`, `document_from_pdf()`, or\n            `document_from_txt()`\n\n    Returns:\n    -------\n        str: document id, which can be used to remove an uploaded document\n            with `remove_document()`\n\n    \"\"\"\n    splitted = self._split_document(doc)\n    return self._store_embeddings(splitted)\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.set_chunk_overlap","title":"<code>set_chunk_overlap(chunk_overlap)</code>","text":"<p>Set the chunk overlap for the text splitter.</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def set_chunk_overlap(self, chunk_overlap: int) -&gt; None:\n    \"\"\"Set the chunk overlap for the text splitter.\"\"\"\n    self.chunk_overlap = chunk_overlap\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.set_chunk_size","title":"<code>set_chunk_size(chunk_size)</code>","text":"<p>Set the chunk size for the text splitter.</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def set_chunk_size(self, chunk_size: int) -&gt; None:\n    \"\"\"Set the chunk size for the text splitter.\"\"\"\n    self.chunk_size = chunk_size\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentEmbedder.set_separators","title":"<code>set_separators(separators)</code>","text":"<p>Set the separators for the text splitter.</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def set_separators(self, separators: list) -&gt; None:\n    \"\"\"Set the separators for the text splitter.\"\"\"\n    self.separators = separators\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentReader","title":"<code>DocumentReader</code>","text":"<p>Class for reading documents from various sources.</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>class DocumentReader:\n    \"\"\"Class for reading documents from various sources.\"\"\"\n\n    def load_document(self, path: str) -&gt; list[Document]:\n        \"\"\"Load a document from a path; accept txt and pdf files.\n\n        Txt files are loaded as-is, pdf files are converted to text using\n        `fitz`.\n\n        Args:\n        ----\n            path (str): path to document\n\n        Returns:\n        -------\n            List[Document]: list of documents\n\n        Raises:\n        ------\n            ValueError: If file extension is not supported\n\n        \"\"\"\n        if path.endswith(\".txt\"):\n            loader = TextLoader(path)\n            return loader.load()\n\n        if path.endswith(\".pdf\"):\n            doc = fitz.open(path)\n            text = \"\"\n            for page in doc:\n                text += page.get_text()\n\n            meta = {k: v for k, v in doc.metadata.items() if v}\n            meta.update({\"source\": path})\n\n            return [\n                Document(\n                    page_content=text,\n                    metadata=meta,\n                ),\n            ]\n\n        err_msg = f\"Unsupported file extension in {path}. File must be .txt or .pdf\"\n        raise ValueError(err_msg)\n\n    def document_from_pdf(self, pdf: bytes) -&gt; list[Document]:\n        \"\"\"Return a list of Documents from a pdf file byte representation.\n\n        Receive a byte representation of a pdf file and return a list of\n        Documents with metadata.\n\n        Args:\n        ----\n            pdf (bytes): byte representation of pdf file\n\n        Returns:\n        -------\n            List[Document]: list of documents\n\n        \"\"\"\n        doc = fitz.open(stream=pdf, filetype=\"pdf\")\n        text = \"\"\n        for page in doc:\n            text += page.get_text()\n\n        meta = {k: v for k, v in doc.metadata.items() if v}\n        meta.update({\"source\": \"pdf\"})\n\n        return [\n            Document(\n                page_content=text,\n                metadata=meta,\n            ),\n        ]\n\n    def document_from_txt(self, txt: bytes) -&gt; list[Document]:\n        \"\"\"Return a list of Documents from a txt file byte representation.\n\n        Receive a byte representation of a txt file and return a list of\n        Documents with metadata.\n\n        Args:\n        ----\n            txt (bytes): byte representation of txt file\n\n        Returns:\n        -------\n            List[Document]: list of documents\n\n        \"\"\"\n        meta = {\"source\": \"txt\"}\n        return [\n            Document(\n                page_content=txt,\n                metadata=meta,\n            ),\n        ]\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentReader.document_from_pdf","title":"<code>document_from_pdf(pdf)</code>","text":"<p>Return a list of Documents from a pdf file byte representation.</p> <p>Receive a byte representation of a pdf file and return a list of Documents with metadata.</p> <pre><code>pdf (bytes): byte representation of pdf file\n</code></pre> <pre><code>List[Document]: list of documents\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def document_from_pdf(self, pdf: bytes) -&gt; list[Document]:\n    \"\"\"Return a list of Documents from a pdf file byte representation.\n\n    Receive a byte representation of a pdf file and return a list of\n    Documents with metadata.\n\n    Args:\n    ----\n        pdf (bytes): byte representation of pdf file\n\n    Returns:\n    -------\n        List[Document]: list of documents\n\n    \"\"\"\n    doc = fitz.open(stream=pdf, filetype=\"pdf\")\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n\n    meta = {k: v for k, v in doc.metadata.items() if v}\n    meta.update({\"source\": \"pdf\"})\n\n    return [\n        Document(\n            page_content=text,\n            metadata=meta,\n        ),\n    ]\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentReader.document_from_txt","title":"<code>document_from_txt(txt)</code>","text":"<p>Return a list of Documents from a txt file byte representation.</p> <p>Receive a byte representation of a txt file and return a list of Documents with metadata.</p> <pre><code>txt (bytes): byte representation of txt file\n</code></pre> <pre><code>List[Document]: list of documents\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def document_from_txt(self, txt: bytes) -&gt; list[Document]:\n    \"\"\"Return a list of Documents from a txt file byte representation.\n\n    Receive a byte representation of a txt file and return a list of\n    Documents with metadata.\n\n    Args:\n    ----\n        txt (bytes): byte representation of txt file\n\n    Returns:\n    -------\n        List[Document]: list of documents\n\n    \"\"\"\n    meta = {\"source\": \"txt\"}\n    return [\n        Document(\n            page_content=txt,\n            metadata=meta,\n        ),\n    ]\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.DocumentReader.load_document","title":"<code>load_document(path)</code>","text":"<p>Load a document from a path; accept txt and pdf files.</p> <p>Txt files are loaded as-is, pdf files are converted to text using <code>fitz</code>.</p> <pre><code>path (str): path to document\n</code></pre> <pre><code>List[Document]: list of documents\n</code></pre> <pre><code>ValueError: If file extension is not supported\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def load_document(self, path: str) -&gt; list[Document]:\n    \"\"\"Load a document from a path; accept txt and pdf files.\n\n    Txt files are loaded as-is, pdf files are converted to text using\n    `fitz`.\n\n    Args:\n    ----\n        path (str): path to document\n\n    Returns:\n    -------\n        List[Document]: list of documents\n\n    Raises:\n    ------\n        ValueError: If file extension is not supported\n\n    \"\"\"\n    if path.endswith(\".txt\"):\n        loader = TextLoader(path)\n        return loader.load()\n\n    if path.endswith(\".pdf\"):\n        doc = fitz.open(path)\n        text = \"\"\n        for page in doc:\n            text += page.get_text()\n\n        meta = {k: v for k, v in doc.metadata.items() if v}\n        meta.update({\"source\": path})\n\n        return [\n            Document(\n                page_content=text,\n                metadata=meta,\n            ),\n        ]\n\n    err_msg = f\"Unsupported file extension in {path}. File must be .txt or .pdf\"\n    raise ValueError(err_msg)\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.OllamaDocumentEmbedder","title":"<code>OllamaDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> <p>Extension of the DocumentEmbedder class to Ollama.</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>class OllamaDocumentEmbedder(DocumentEmbedder):\n    \"\"\"Extension of the DocumentEmbedder class to Ollama.\"\"\"\n\n    def __init__(\n        self,\n        used: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: list | None = None,\n        n_results: int = 3,\n        model: str | None = \"nomic-embed-text\",\n        vector_db_vendor: str | None = None,\n        connection_args: dict | None = None,\n        embedding_collection_name: str | None = None,\n        metadata_collection_name: str | None = None,\n        base_url: str | None = None,\n        documentids_workspace: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize with the specified configuration.\n\n        Args:\n        ----\n            used (bool, optional): whether RAG has been used (frontend setting).\n\n            chunk_size (int, optional): size of chunks to split text into.\n\n            chunk_overlap (int, optional): overlap between chunks.\n\n            split_by_characters (bool, optional): whether to split by characters\n                or tokens.\n\n            separators (Optional[list], optional): list of separators to use\n                when splitting by characters.\n\n            n_results (int, optional): number of results to return from\n                similarity search.\n\n            model (Optional[str], optional): name of model to use for\n                embeddings. Can be \"auto\" to use the first available model.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n                to use.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n                vector database connection.\n\n            embedding_collection_name (Optional[str], optional): name of\n                collection to store embeddings in.\n\n            metadata_collection_name (Optional[str], optional): name of\n                collection to store metadata in.\n\n            base_url (Optional[str], optional): base url of Xinference API.\n\n            documentids_workspace (Optional[List[str]], optional): a list of\n                document IDs that defines the scope within which RAG operations\n                (remove, similarity search, and get all) occur. Defaults to\n                None, which means the operations will be performed across all\n                documents in the database.\n\n        \"\"\"\n        from langchain_community.embeddings import OllamaEmbeddings\n\n        self.model_name = model\n\n        super().__init__(\n            used=used,\n            online=True,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            split_by_characters=split_by_characters,\n            separators=separators,\n            n_results=n_results,\n            model=model,\n            vector_db_vendor=vector_db_vendor,\n            connection_args=connection_args,\n            embedding_collection_name=embedding_collection_name,\n            metadata_collection_name=metadata_collection_name,\n            base_url=base_url,\n            embeddings=OllamaEmbeddings(\n                base_url=base_url,\n                model=self.model_name,\n            ),\n            documentids_workspace=documentids_workspace,\n        )\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.OllamaDocumentEmbedder.__init__","title":"<code>__init__(used=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='nomic-embed-text', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, base_url=None, documentids_workspace=None)</code>","text":"<p>Initialize with the specified configuration.</p> <pre><code>used (bool, optional): whether RAG has been used (frontend setting).\n\nchunk_size (int, optional): size of chunks to split text into.\n\nchunk_overlap (int, optional): overlap between chunks.\n\nsplit_by_characters (bool, optional): whether to split by characters\n    or tokens.\n\nseparators (Optional[list], optional): list of separators to use\n    when splitting by characters.\n\nn_results (int, optional): number of results to return from\n    similarity search.\n\nmodel (Optional[str], optional): name of model to use for\n    embeddings. Can be \"auto\" to use the first available model.\n\nvector_db_vendor (Optional[str], optional): name of vector database\n    to use.\n\nconnection_args (Optional[dict], optional): arguments to pass to\n    vector database connection.\n\nembedding_collection_name (Optional[str], optional): name of\n    collection to store embeddings in.\n\nmetadata_collection_name (Optional[str], optional): name of\n    collection to store metadata in.\n\nbase_url (Optional[str], optional): base url of Xinference API.\n\ndocumentids_workspace (Optional[List[str]], optional): a list of\n    document IDs that defines the scope within which RAG operations\n    (remove, similarity search, and get all) occur. Defaults to\n    None, which means the operations will be performed across all\n    documents in the database.\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    used: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: list | None = None,\n    n_results: int = 3,\n    model: str | None = \"nomic-embed-text\",\n    vector_db_vendor: str | None = None,\n    connection_args: dict | None = None,\n    embedding_collection_name: str | None = None,\n    metadata_collection_name: str | None = None,\n    base_url: str | None = None,\n    documentids_workspace: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Initialize with the specified configuration.\n\n    Args:\n    ----\n        used (bool, optional): whether RAG has been used (frontend setting).\n\n        chunk_size (int, optional): size of chunks to split text into.\n\n        chunk_overlap (int, optional): overlap between chunks.\n\n        split_by_characters (bool, optional): whether to split by characters\n            or tokens.\n\n        separators (Optional[list], optional): list of separators to use\n            when splitting by characters.\n\n        n_results (int, optional): number of results to return from\n            similarity search.\n\n        model (Optional[str], optional): name of model to use for\n            embeddings. Can be \"auto\" to use the first available model.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n            to use.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection.\n\n        embedding_collection_name (Optional[str], optional): name of\n            collection to store embeddings in.\n\n        metadata_collection_name (Optional[str], optional): name of\n            collection to store metadata in.\n\n        base_url (Optional[str], optional): base url of Xinference API.\n\n        documentids_workspace (Optional[List[str]], optional): a list of\n            document IDs that defines the scope within which RAG operations\n            (remove, similarity search, and get all) occur. Defaults to\n            None, which means the operations will be performed across all\n            documents in the database.\n\n    \"\"\"\n    from langchain_community.embeddings import OllamaEmbeddings\n\n    self.model_name = model\n\n    super().__init__(\n        used=used,\n        online=True,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        split_by_characters=split_by_characters,\n        separators=separators,\n        n_results=n_results,\n        model=model,\n        vector_db_vendor=vector_db_vendor,\n        connection_args=connection_args,\n        embedding_collection_name=embedding_collection_name,\n        metadata_collection_name=metadata_collection_name,\n        base_url=base_url,\n        embeddings=OllamaEmbeddings(\n            base_url=base_url,\n            model=self.model_name,\n        ),\n        documentids_workspace=documentids_workspace,\n    )\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.XinferenceDocumentEmbedder","title":"<code>XinferenceDocumentEmbedder</code>","text":"<p>               Bases: <code>DocumentEmbedder</code></p> <p>Extension of the DocumentEmbedder class to Xinference.</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>class XinferenceDocumentEmbedder(DocumentEmbedder):\n    \"\"\"Extension of the DocumentEmbedder class to Xinference.\"\"\"\n\n    def __init__(\n        self,\n        used: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: list | None = None,\n        n_results: int = 3,\n        model: str | None = \"auto\",\n        vector_db_vendor: str | None = None,\n        connection_args: dict | None = None,\n        embedding_collection_name: str | None = None,\n        metadata_collection_name: str | None = None,\n        base_url: str | None = None,\n        documentids_workspace: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize with the specified configuration.\n\n        Args:\n        ----\n            used (bool, optional): whether RAG has been used (frontend setting).\n\n            chunk_size (int, optional): size of chunks to split text into.\n\n            chunk_overlap (int, optional): overlap between chunks.\n\n            split_by_characters (bool, optional): whether to split by characters\n                or tokens.\n\n            separators (Optional[list], optional): list of separators to use\n                when splitting by characters.\n\n            n_results (int, optional): number of results to return from\n                similarity search.\n\n            model (Optional[str], optional): name of model to use for\n                embeddings. Can be \"auto\" to use the first available model.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n                to use.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n                vector database connection.\n\n            embedding_collection_name (Optional[str], optional): name of\n                collection to store embeddings in.\n\n            metadata_collection_name (Optional[str], optional): name of\n                collection to store metadata in.\n\n            base_url (Optional[str], optional): base url of Xinference API.\n\n            documentids_workspace (Optional[List[str]], optional): a list of\n                document IDs that defines the scope within which RAG operations\n                (remove, similarity search, and get all) occur. Defaults to\n                None, which means the operations will be performed across all\n                documents in the database.\n\n        \"\"\"\n        from xinference.client import Client\n\n        self.model_name = model\n        self.client = Client(base_url=base_url)\n        self.models = {}\n        self.load_models()\n\n        if self.model_name is None or self.model_name == \"auto\":\n            self.model_name = self.list_models_by_type(\"embedding\")[0]\n        self.model_uid = self.models[self.model_name][\"id\"]\n\n        super().__init__(\n            used=used,\n            online=True,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            split_by_characters=split_by_characters,\n            separators=separators,\n            n_results=n_results,\n            model=model,\n            vector_db_vendor=vector_db_vendor,\n            connection_args=connection_args,\n            embedding_collection_name=embedding_collection_name,\n            metadata_collection_name=metadata_collection_name,\n            base_url=base_url,\n            embeddings=XinferenceEmbeddings(\n                server_url=base_url,\n                model_uid=self.model_uid,\n            ),\n            documentids_workspace=documentids_workspace,\n        )\n\n    def load_models(self) -&gt; None:\n        \"\"\"Get all models that are currently available.\n\n        Connect to the Xinference server and write the running models to\n        `self.models`.\n        \"\"\"\n        for _id, model in self.client.list_models().items():\n            model[\"id\"] = _id\n            self.models[model[\"model_name\"]] = model\n\n    def list_models_by_type(self, model_type: str) -&gt; list[str]:\n        \"\"\"Return all models of a certain type.\n\n        Connect to the Xinference server and return all models of a certain\n        type.\n\n        Args:\n        ----\n            model_type (str): type of model to list (e.g. \"embedding\", \"chat\")\n\n        Returns:\n        -------\n            List[str]: list of model names\n\n        \"\"\"\n        names = []\n        for name, model in self.models.items():\n            if \"model_ability\" in model:\n                if model_type in model[\"model_ability\"]:\n                    names.append(name)\n            elif model[\"model_type\"] == model_type:\n                names.append(name)\n        return names\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.XinferenceDocumentEmbedder.__init__","title":"<code>__init__(used=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='auto', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, base_url=None, documentids_workspace=None)</code>","text":"<p>Initialize with the specified configuration.</p> <pre><code>used (bool, optional): whether RAG has been used (frontend setting).\n\nchunk_size (int, optional): size of chunks to split text into.\n\nchunk_overlap (int, optional): overlap between chunks.\n\nsplit_by_characters (bool, optional): whether to split by characters\n    or tokens.\n\nseparators (Optional[list], optional): list of separators to use\n    when splitting by characters.\n\nn_results (int, optional): number of results to return from\n    similarity search.\n\nmodel (Optional[str], optional): name of model to use for\n    embeddings. Can be \"auto\" to use the first available model.\n\nvector_db_vendor (Optional[str], optional): name of vector database\n    to use.\n\nconnection_args (Optional[dict], optional): arguments to pass to\n    vector database connection.\n\nembedding_collection_name (Optional[str], optional): name of\n    collection to store embeddings in.\n\nmetadata_collection_name (Optional[str], optional): name of\n    collection to store metadata in.\n\nbase_url (Optional[str], optional): base url of Xinference API.\n\ndocumentids_workspace (Optional[List[str]], optional): a list of\n    document IDs that defines the scope within which RAG operations\n    (remove, similarity search, and get all) occur. Defaults to\n    None, which means the operations will be performed across all\n    documents in the database.\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    used: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: list | None = None,\n    n_results: int = 3,\n    model: str | None = \"auto\",\n    vector_db_vendor: str | None = None,\n    connection_args: dict | None = None,\n    embedding_collection_name: str | None = None,\n    metadata_collection_name: str | None = None,\n    base_url: str | None = None,\n    documentids_workspace: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Initialize with the specified configuration.\n\n    Args:\n    ----\n        used (bool, optional): whether RAG has been used (frontend setting).\n\n        chunk_size (int, optional): size of chunks to split text into.\n\n        chunk_overlap (int, optional): overlap between chunks.\n\n        split_by_characters (bool, optional): whether to split by characters\n            or tokens.\n\n        separators (Optional[list], optional): list of separators to use\n            when splitting by characters.\n\n        n_results (int, optional): number of results to return from\n            similarity search.\n\n        model (Optional[str], optional): name of model to use for\n            embeddings. Can be \"auto\" to use the first available model.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n            to use.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection.\n\n        embedding_collection_name (Optional[str], optional): name of\n            collection to store embeddings in.\n\n        metadata_collection_name (Optional[str], optional): name of\n            collection to store metadata in.\n\n        base_url (Optional[str], optional): base url of Xinference API.\n\n        documentids_workspace (Optional[List[str]], optional): a list of\n            document IDs that defines the scope within which RAG operations\n            (remove, similarity search, and get all) occur. Defaults to\n            None, which means the operations will be performed across all\n            documents in the database.\n\n    \"\"\"\n    from xinference.client import Client\n\n    self.model_name = model\n    self.client = Client(base_url=base_url)\n    self.models = {}\n    self.load_models()\n\n    if self.model_name is None or self.model_name == \"auto\":\n        self.model_name = self.list_models_by_type(\"embedding\")[0]\n    self.model_uid = self.models[self.model_name][\"id\"]\n\n    super().__init__(\n        used=used,\n        online=True,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        split_by_characters=split_by_characters,\n        separators=separators,\n        n_results=n_results,\n        model=model,\n        vector_db_vendor=vector_db_vendor,\n        connection_args=connection_args,\n        embedding_collection_name=embedding_collection_name,\n        metadata_collection_name=metadata_collection_name,\n        base_url=base_url,\n        embeddings=XinferenceEmbeddings(\n            server_url=base_url,\n            model_uid=self.model_uid,\n        ),\n        documentids_workspace=documentids_workspace,\n    )\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.XinferenceDocumentEmbedder.list_models_by_type","title":"<code>list_models_by_type(model_type)</code>","text":"<p>Return all models of a certain type.</p> <p>Connect to the Xinference server and return all models of a certain type.</p> <pre><code>model_type (str): type of model to list (e.g. \"embedding\", \"chat\")\n</code></pre> <pre><code>List[str]: list of model names\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def list_models_by_type(self, model_type: str) -&gt; list[str]:\n    \"\"\"Return all models of a certain type.\n\n    Connect to the Xinference server and return all models of a certain\n    type.\n\n    Args:\n    ----\n        model_type (str): type of model to list (e.g. \"embedding\", \"chat\")\n\n    Returns:\n    -------\n        List[str]: list of model names\n\n    \"\"\"\n    names = []\n    for name, model in self.models.items():\n        if \"model_ability\" in model:\n            if model_type in model[\"model_ability\"]:\n                names.append(name)\n        elif model[\"model_type\"] == model_type:\n            names.append(name)\n    return names\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore.XinferenceDocumentEmbedder.load_models","title":"<code>load_models()</code>","text":"<p>Get all models that are currently available.</p> <p>Connect to the Xinference server and write the running models to <code>self.models</code>.</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def load_models(self) -&gt; None:\n    \"\"\"Get all models that are currently available.\n\n    Connect to the Xinference server and write the running models to\n    `self.models`.\n    \"\"\"\n    for _id, model in self.client.list_models().items():\n        model[\"id\"] = _id\n        self.models[model[\"model_name\"]] = model\n</code></pre>"},{"location":"api-docs/vectorstore/#vectorstore-agent","title":"Vectorstore Agent","text":""},{"location":"api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus","title":"<code>VectorDatabaseAgentMilvus</code>","text":"<p>The VectorDatabaseAgentMilvus class manages vector databases in a connected host database. It manages an embedding collection <code>_col_embeddings:langchain.vectorstores.Milvus</code>, which is the main information on the embedded text fragments and the basis for similarity search, and a metadata collection <code>_col_metadata:pymilvus.Collection</code>, which stores the metadata of the embedded text fragments. A typical workflow includes the following operations:</p> <ol> <li>connect to a host using <code>connect()</code></li> <li>get all documents in the active database using <code>get_all_documents()</code></li> <li>save a number of fragments, usually from a specific document, using     <code>store_embeddings()</code></li> <li>do similarity search among all fragments of the currently active database     using <code>similarity_search()</code></li> <li>remove a document from the currently active database using     <code>remove_document()</code></li> </ol> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>class VectorDatabaseAgentMilvus:\n    \"\"\"The VectorDatabaseAgentMilvus class manages vector databases in a connected\n    host database. It manages an embedding collection\n    `_col_embeddings:langchain.vectorstores.Milvus`, which is the main\n    information on the embedded text fragments and the basis for similarity\n    search, and a metadata collection `_col_metadata:pymilvus.Collection`, which\n    stores the metadata of the embedded text fragments. A typical workflow\n    includes the following operations:\n\n    1. connect to a host using `connect()`\n    2. get all documents in the active database using `get_all_documents()`\n    3. save a number of fragments, usually from a specific document, using\n        `store_embeddings()`\n    4. do similarity search among all fragments of the currently active database\n        using `similarity_search()`\n    5. remove a document from the currently active database using\n        `remove_document()`\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_func: OpenAIEmbeddings,\n        connection_args: dict | None = None,\n        embedding_collection_name: str | None = None,\n        metadata_collection_name: str | None = None,\n    ):\n        \"\"\"Args:\n        ----\n            embedding_func OpenAIEmbeddings: Function used to embed the text\n\n            connection_args Optional dict: args to connect Vector Database\n\n            embedding_collection_name Optional str: exposed for test\n\n            metadata_collection_name Optional str: exposed for test\n\n        \"\"\"\n        self._embedding_func = embedding_func\n        self._col_embeddings: Milvus | None = None\n        self._col_metadata: Collection | None = None\n        self._connection_args = validate_connection_args(connection_args)\n        self._embedding_name = embedding_collection_name or DOCUMENT_EMBEDDINGS_COLLECTION_NAME\n        self._metadata_name = metadata_collection_name or DOCUMENT_METADATA_COLLECTION_NAME\n\n    def connect(self) -&gt; None:\n        \"\"\"Connect to a host and read two document collections (the default names\n        are `DocumentEmbeddings` and `DocumentMetadata`) in the currently active\n        database (default database name is `default`); if those document\n        collections don't exist, create the two collections.\n        \"\"\"\n        self._connect(**self._connection_args)\n        self._init_host()\n\n    def _connect(self, host: str, port: str, user: str, password: str) -&gt; None:\n        self.alias = self._create_connection_alias(host, port, user, password)\n\n    def _init_host(self) -&gt; None:\n        \"\"\"Initialize host. Will read/create document collection inside currently\n        active database.\n        \"\"\"\n        self._create_collections()\n\n    def _create_connection_alias(\n        self,\n        host: str,\n        port: str,\n        user: str,\n        password: str,\n    ) -&gt; str:\n        \"\"\"Connect to host and create a connection alias for metadata collection\n        using a random uuid.\n\n        Args:\n        ----\n            host (str): host ip address\n            port (str): host port\n\n        Returns:\n        -------\n            str: connection alias\n\n        \"\"\"\n        alias = uuid.uuid4().hex\n        try:\n            connections.connect(\n                host=host,\n                port=port,\n                user=user,\n                password=password,\n                alias=alias,\n            )\n            logger.debug(f\"Created new connection using: {alias}\")\n            return alias\n        except MilvusException as e:\n            logger.error(f\"Failed to create  new connection using: {alias}\")\n            raise e\n\n    def _create_collections(self) -&gt; None:\n        \"\"\"Create or load the embedding and metadata collections from the currently\n        active database.\n        \"\"\"\n        embedding_exists = utility.has_collection(\n            self._embedding_name,\n            using=self.alias,\n        )\n        meta_exists = utility.has_collection(\n            self._metadata_name,\n            using=self.alias,\n        )\n\n        if embedding_exists:\n            self._load_embeddings_collection()\n        else:\n            self._create_embeddings_collection()\n\n        if meta_exists:\n            self._load_metadata_collection()\n        else:\n            self._create_metadata_collection()\n\n        self._create_metadata_collection_index()\n        self._col_metadata.load()\n\n    def _load_embeddings_collection(self) -&gt; None:\n        \"\"\"Load embeddings collection from currently active database.\"\"\"\n        try:\n            self._col_embeddings = Milvus(\n                embedding_function=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n            )\n        except MilvusException as e:\n            logger.error(\n                f\"Failed to load embeddings collection {self._embedding_name}.\",\n            )\n            raise e\n\n    def _create_embeddings_collection(self) -&gt; None:\n        \"\"\"Create embedding collection.\n        All fields: \"meta_id\", \"vector\"\n        \"\"\"\n        try:\n            self._col_embeddings = Milvus(\n                embedding_function=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n            )\n        except MilvusException as e:\n            logger.error(\n                f\"Failed to create embeddings collection {self._embedding_name}\",\n            )\n            raise e\n\n    def _load_metadata_collection(self) -&gt; None:\n        \"\"\"Load metadata collection from currently active database.\"\"\"\n        self._col_metadata = Collection(\n            self._metadata_name,\n            using=self.alias,\n        )\n        self._col_metadata.load()\n\n    def _create_metadata_collection(self) -&gt; None:\n        \"\"\"Create metadata collection.\n\n        All fields: \"id\", \"name\", \"author\", \"title\", \"format\", \"subject\",\n        \"creator\", \"producer\", \"creationDate\", \"modDate\", \"source\", \"embedding\",\n        \"isDeleted\".\n\n        As the vector database requires a vector field, we will create a fake\n        vector \"embedding\". The field \"isDeleted\" is used to specify if the\n        document is deleted.\n        \"\"\"\n        MAX_LENGTH = 10000\n        doc_id = FieldSchema(\n            name=\"id\",\n            dtype=DataType.INT64,\n            is_primary=True,\n            auto_id=True,\n        )\n        doc_name = FieldSchema(\n            name=\"name\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        doc_author = FieldSchema(\n            name=\"author\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        doc_title = FieldSchema(\n            name=\"title\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        doc_format = FieldSchema(\n            name=\"format\",\n            dtype=DataType.VARCHAR,\n            max_length=255,\n        )\n        doc_subject = FieldSchema(\n            name=\"subject\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        doc_creator = FieldSchema(\n            name=\"creator\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        doc_producer = FieldSchema(\n            name=\"producer\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        doc_creationDate = FieldSchema(\n            name=\"creationDate\",\n            dtype=DataType.VARCHAR,\n            max_length=1024,\n        )\n        doc_modDate = FieldSchema(\n            name=\"modDate\",\n            dtype=DataType.VARCHAR,\n            max_length=1024,\n        )\n        doc_source = FieldSchema(\n            name=\"source\",\n            dtype=DataType.VARCHAR,\n            max_length=MAX_LENGTH,\n        )\n        embedding = FieldSchema(\n            name=\"embedding\",\n            dtype=DataType.FLOAT_VECTOR,\n            dim=METADATA_VECTOR_DIM,\n        )\n        isDeleted = FieldSchema(\n            name=\"isDeleted\",\n            dtype=DataType.BOOL,\n        )\n        fields = [\n            doc_id,\n            doc_name,\n            doc_author,\n            doc_title,\n            doc_format,\n            doc_subject,\n            doc_creator,\n            doc_producer,\n            doc_creationDate,\n            doc_modDate,\n            doc_source,\n            embedding,\n            isDeleted,\n        ]\n        schema = CollectionSchema(fields=fields)\n        try:\n            self._col_metadata = Collection(\n                name=self._metadata_name,\n                schema=schema,\n                using=self.alias,\n            )\n        except MilvusException as e:\n            logger.error(f\"Failed to create collection {self._metadata_name}\")\n            raise e\n\n    def _create_metadata_collection_index(self) -&gt; None:\n        \"\"\"Create index for metadata collection in currently active database.\"\"\"\n        if not isinstance(self._col_metadata, Collection) or len(self._col_metadata.indexes) &gt; 0:\n            return\n\n        index_params = {\n            \"metric_type\": \"L2\",\n            \"index_type\": \"HNSW\",\n            \"params\": {\"M\": 8, \"efConstruction\": 64},\n        }\n\n        try:\n            self._col_metadata.create_index(\n                field_name=\"embedding\",\n                index_params=index_params,\n                using=self.alias,\n            )\n        except MilvusException as e:\n            logger.error(\n                \"Failed to create index for meta collection \" f\"{self._metadata_name}.\",\n            )\n            raise e\n\n    def _insert_data(self, documents: list[Document]) -&gt; str:\n        \"\"\"Insert documents into the currently active database.\n\n        Args:\n        ----\n            documents (List[Documents]): documents array, usually from\n                DocumentReader.load_document, DocumentReader.document_from_pdf,\n                DocumentReader.document_from_txt\n\n        Returns:\n        -------\n            str: document id\n\n        \"\"\"\n        if len(documents) == 0:\n            return None\n        metadata = [documents[0].metadata]\n        aligned_metadata = align_metadata(metadata)\n        try:\n            result = self._col_metadata.insert(aligned_metadata)\n            meta_id = str(result.primary_keys[0])\n            self._col_metadata.flush()\n        except MilvusException as e:\n            logger.error(\"Failed to insert meta data\")\n            raise e\n        aligned_docs = align_embeddings(documents, meta_id)\n        try:\n            # As we passed collection_name, documents will be added to existed collection\n            self._col_embeddings = Milvus.from_documents(\n                embedding=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n                documents=aligned_docs,\n            )\n        except MilvusException as e:\n            logger.error(\n                \"Failed to insert data to embedding collection \" f\"{self._embedding_name}.\",\n            )\n            raise e\n        return meta_id\n\n    def store_embeddings(self, documents: list[Document]) -&gt; str:\n        \"\"\"Store documents in the currently active database.\n\n        Args:\n        ----\n            documents (List[Documents]): documents array, usually from\n                DocumentReader.load_document, DocumentReader.document_from_pdf,\n                DocumentReader.document_from_txt\n\n        Returns:\n        -------\n            str: document id\n\n        \"\"\"\n        if len(documents) == 0:\n            return None\n        return self._insert_data(documents)\n\n    def _build_embedding_search_expression(\n        self,\n        meta_ids: list[dict],\n    ) -&gt; str | None:\n        \"\"\"Build search expression for embedding collection. The generated\n        expression follows the pattern: \"meta_id in [{id1}, {id2}, ...]\n\n        Args:\n        ----\n            meta_ids: the array of metadata id in metadata collection\n\n        Returns:\n        -------\n            str: search expression or None\n\n        \"\"\"\n        if len(meta_ids) == 0:\n            return \"meta_id in []\"\n        built_expr = \"\"\"meta_id in [\"\"\"\n        for item in meta_ids:\n            id = f'\"{item[\"id\"]}\",'\n            built_expr += id\n        built_expr = built_expr[:-1]\n        built_expr += \"\"\"]\"\"\"\n        return built_expr\n\n    def _join_embedding_and_metadata_results(\n        self,\n        result_embedding: list[Document],\n        result_meta: list[dict],\n    ) -&gt; list[Document]:\n        \"\"\"Join the search results of embedding collection and results of metadata.\n\n        Args:\n        ----\n            result_embedding (List[Document]): search result of embedding\n                collection\n\n            result_meta (List[Dict]): search result of metadata collection\n\n        Returns:\n        -------\n            List[Document]: combined results like\n                [{page_content: str, metadata: {...}}]\n\n        \"\"\"\n\n        def _find_metadata_by_id(\n            metadata: list[dict],\n            id: str,\n        ) -&gt; dict | None:\n            for d in metadata:\n                if str(d[\"id\"]) == id:\n                    return d\n            return None\n\n        joined_docs = []\n        for res in result_embedding:\n            found = _find_metadata_by_id(result_meta, res.metadata[\"meta_id\"])\n            if found is None:  # discard\n                logger.error(\n                    f\"Failed to join meta_id {res.metadata['meta_id']}\",\n                )\n                continue\n            joined_docs.append(\n                Document(page_content=res.page_content, metadata=found),\n            )\n        return joined_docs\n\n    @staticmethod\n    def _build_meta_col_query_expr_for_all_documents(\n        doc_ids: list[str] | None = None,\n    ) -&gt; str:\n        \"\"\"Build metadata collection query expression to obtain all documents.\n\n        Args:\n        ----\n            doc_ids: the list of document ids (metadata ids), if thie argument is None,\n                     that is, the query is to get all undeleted documents in metadata collection.\n                     Otherwise, the query is to getr all undeleted documents form provided doc_ids\n\n        Returns:\n        -------\n            query: str\n\n        \"\"\"\n        expr = f\"id in {doc_ids} and isDeleted == false\" if doc_ids is not None else \"isDeleted == false\"\n        return expr.replace('\"', \"\").replace(\"'\", \"\")\n\n    def similarity_search(\n        self,\n        query: str,\n        k: int = 3,\n        doc_ids: list[str] | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"Perform similarity search insider the currently active database\n        according to the input query.\n\n        This method will:\n        1. get all non-deleted meta_id and build to search expression for\n            the currently active embedding collection\n        2. do similarity search in the embedding collection\n        3. combine metadata and embeddings\n\n        Args:\n        ----\n            query (str): query string\n\n            k (int): the number of results to return\n\n            doc_ids (Optional[list[str]]): the list of document ids, do\n                similarity search across the specified documents\n\n        Returns:\n        -------\n            List[Document]: search results\n\n        \"\"\"\n        result_metadata = []\n        expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n            doc_ids,\n        )\n        result_metadata = self._col_metadata.query(\n            expr=expr,\n            output_fields=METADATA_FIELDS,\n        )\n        expr = self._build_embedding_search_expression(result_metadata)\n        result_embedding = self._col_embeddings.similarity_search(\n            query=query,\n            k=k,\n            expr=expr,\n        )\n        return self._join_embedding_and_metadata_results(\n            result_embedding,\n            result_metadata,\n        )\n\n    def remove_document(\n        self,\n        doc_id: str,\n        doc_ids: list[str] | None = None,\n    ) -&gt; bool:\n        \"\"\"Remove the document include meta data and its embeddings.\n\n        Args:\n        ----\n            doc_id (str): the document to be deleted\n\n            doc_ids (Optional[list[str]]): the list of document ids, defines\n                documents scope within which remove operation occurs.\n\n        Returns:\n        -------\n            bool: True if the document is deleted, False otherwise\n\n        \"\"\"\n        if not self._col_metadata:\n            return False\n        if doc_ids is not None and (len(doc_ids) == 0 or (len(doc_ids) &gt; 0 and doc_id not in doc_ids)):\n            return False\n        try:\n            expr = f\"id in [{doc_id}]\"\n            res = self._col_metadata.query(\n                expr=expr,\n                output_fields=METADATA_FIELDS,\n            )\n            if len(res) == 0:\n                return False\n            del_res = self._col_metadata.delete(expr)\n            self._col_metadata.flush()\n\n            res = self._col_embeddings.col.query(f'meta_id in [\"{doc_id}\"]')\n            if len(res) == 0:\n                return True\n            ids = [item[\"pk\"] for item in res]\n            embedding_expr = f\"pk in {ids}\"\n            del_res = self._col_embeddings.col.delete(expr=embedding_expr)\n            self._col_embeddings.col.flush()\n            return True\n        except MilvusException as e:\n            logger.error(e)\n            raise e\n\n    def get_all_documents(\n        self,\n        doc_ids: list[str] | None = None,\n    ) -&gt; list[dict]:\n        \"\"\"Get all non-deleted documents from the currently active database.\n\n        Args:\n        ----\n            doc_ids (List[str] optional): the list of document ids, defines\n                documents scope within which the operation of obtaining all\n                documents occurs\n\n        Returns:\n        -------\n            List[Dict]: the metadata of all non-deleted documents in the form\n                [{{id}, {author}, {source}, ...}]\n\n        \"\"\"\n        try:\n            expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n                doc_ids,\n            )\n            result_metadata = self._col_metadata.query(\n                expr=expr,\n                output_fields=METADATA_FIELDS,\n            )\n            return result_metadata\n        except MilvusException as e:\n            logger.error(e)\n            raise e\n\n    def get_description(self, doc_ids: list[str] | None = None):\n        def get_name(meta: dict[str, str]):\n            name_col = [\"title\", \"name\", \"subject\", \"source\"]\n            for col in name_col:\n                if meta[col] is not None and len(meta[col]) &gt; 0:\n                    return meta[col]\n            return \"\"\n\n        expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n            doc_ids,\n        )\n        result = self._col_metadata.query(\n            expr=expr,\n            output_fields=METADATA_FIELDS,\n        )\n        names = list(map(get_name, result))\n        names_set = set(names)\n        desc = f\"This vector store contains the following articles: {names_set}\"\n        return desc[:MAX_AGENT_DESC_LENGTH]\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.__init__","title":"<code>__init__(embedding_func, connection_args=None, embedding_collection_name=None, metadata_collection_name=None)</code>","text":"<pre><code>embedding_func OpenAIEmbeddings: Function used to embed the text\n\nconnection_args Optional dict: args to connect Vector Database\n\nembedding_collection_name Optional str: exposed for test\n\nmetadata_collection_name Optional str: exposed for test\n</code></pre> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def __init__(\n    self,\n    embedding_func: OpenAIEmbeddings,\n    connection_args: dict | None = None,\n    embedding_collection_name: str | None = None,\n    metadata_collection_name: str | None = None,\n):\n    \"\"\"Args:\n    ----\n        embedding_func OpenAIEmbeddings: Function used to embed the text\n\n        connection_args Optional dict: args to connect Vector Database\n\n        embedding_collection_name Optional str: exposed for test\n\n        metadata_collection_name Optional str: exposed for test\n\n    \"\"\"\n    self._embedding_func = embedding_func\n    self._col_embeddings: Milvus | None = None\n    self._col_metadata: Collection | None = None\n    self._connection_args = validate_connection_args(connection_args)\n    self._embedding_name = embedding_collection_name or DOCUMENT_EMBEDDINGS_COLLECTION_NAME\n    self._metadata_name = metadata_collection_name or DOCUMENT_METADATA_COLLECTION_NAME\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.connect","title":"<code>connect()</code>","text":"<p>Connect to a host and read two document collections (the default names are <code>DocumentEmbeddings</code> and <code>DocumentMetadata</code>) in the currently active database (default database name is <code>default</code>); if those document collections don't exist, create the two collections.</p> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"Connect to a host and read two document collections (the default names\n    are `DocumentEmbeddings` and `DocumentMetadata`) in the currently active\n    database (default database name is `default`); if those document\n    collections don't exist, create the two collections.\n    \"\"\"\n    self._connect(**self._connection_args)\n    self._init_host()\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.get_all_documents","title":"<code>get_all_documents(doc_ids=None)</code>","text":"<p>Get all non-deleted documents from the currently active database.</p> <pre><code>doc_ids (List[str] optional): the list of document ids, defines\n    documents scope within which the operation of obtaining all\n    documents occurs\n</code></pre> <pre><code>List[Dict]: the metadata of all non-deleted documents in the form\n    [{{id}, {author}, {source}, ...}]\n</code></pre> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def get_all_documents(\n    self,\n    doc_ids: list[str] | None = None,\n) -&gt; list[dict]:\n    \"\"\"Get all non-deleted documents from the currently active database.\n\n    Args:\n    ----\n        doc_ids (List[str] optional): the list of document ids, defines\n            documents scope within which the operation of obtaining all\n            documents occurs\n\n    Returns:\n    -------\n        List[Dict]: the metadata of all non-deleted documents in the form\n            [{{id}, {author}, {source}, ...}]\n\n    \"\"\"\n    try:\n        expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n            doc_ids,\n        )\n        result_metadata = self._col_metadata.query(\n            expr=expr,\n            output_fields=METADATA_FIELDS,\n        )\n        return result_metadata\n    except MilvusException as e:\n        logger.error(e)\n        raise e\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.remove_document","title":"<code>remove_document(doc_id, doc_ids=None)</code>","text":"<p>Remove the document include meta data and its embeddings.</p> <pre><code>doc_id (str): the document to be deleted\n\ndoc_ids (Optional[list[str]]): the list of document ids, defines\n    documents scope within which remove operation occurs.\n</code></pre> <pre><code>bool: True if the document is deleted, False otherwise\n</code></pre> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def remove_document(\n    self,\n    doc_id: str,\n    doc_ids: list[str] | None = None,\n) -&gt; bool:\n    \"\"\"Remove the document include meta data and its embeddings.\n\n    Args:\n    ----\n        doc_id (str): the document to be deleted\n\n        doc_ids (Optional[list[str]]): the list of document ids, defines\n            documents scope within which remove operation occurs.\n\n    Returns:\n    -------\n        bool: True if the document is deleted, False otherwise\n\n    \"\"\"\n    if not self._col_metadata:\n        return False\n    if doc_ids is not None and (len(doc_ids) == 0 or (len(doc_ids) &gt; 0 and doc_id not in doc_ids)):\n        return False\n    try:\n        expr = f\"id in [{doc_id}]\"\n        res = self._col_metadata.query(\n            expr=expr,\n            output_fields=METADATA_FIELDS,\n        )\n        if len(res) == 0:\n            return False\n        del_res = self._col_metadata.delete(expr)\n        self._col_metadata.flush()\n\n        res = self._col_embeddings.col.query(f'meta_id in [\"{doc_id}\"]')\n        if len(res) == 0:\n            return True\n        ids = [item[\"pk\"] for item in res]\n        embedding_expr = f\"pk in {ids}\"\n        del_res = self._col_embeddings.col.delete(expr=embedding_expr)\n        self._col_embeddings.col.flush()\n        return True\n    except MilvusException as e:\n        logger.error(e)\n        raise e\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.similarity_search","title":"<code>similarity_search(query, k=3, doc_ids=None)</code>","text":"<p>Perform similarity search insider the currently active database according to the input query.</p> <p>This method will: 1. get all non-deleted meta_id and build to search expression for     the currently active embedding collection 2. do similarity search in the embedding collection 3. combine metadata and embeddings</p> <pre><code>query (str): query string\n\nk (int): the number of results to return\n\ndoc_ids (Optional[list[str]]): the list of document ids, do\n    similarity search across the specified documents\n</code></pre> <pre><code>List[Document]: search results\n</code></pre> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def similarity_search(\n    self,\n    query: str,\n    k: int = 3,\n    doc_ids: list[str] | None = None,\n) -&gt; list[Document]:\n    \"\"\"Perform similarity search insider the currently active database\n    according to the input query.\n\n    This method will:\n    1. get all non-deleted meta_id and build to search expression for\n        the currently active embedding collection\n    2. do similarity search in the embedding collection\n    3. combine metadata and embeddings\n\n    Args:\n    ----\n        query (str): query string\n\n        k (int): the number of results to return\n\n        doc_ids (Optional[list[str]]): the list of document ids, do\n            similarity search across the specified documents\n\n    Returns:\n    -------\n        List[Document]: search results\n\n    \"\"\"\n    result_metadata = []\n    expr = VectorDatabaseAgentMilvus._build_meta_col_query_expr_for_all_documents(\n        doc_ids,\n    )\n    result_metadata = self._col_metadata.query(\n        expr=expr,\n        output_fields=METADATA_FIELDS,\n    )\n    expr = self._build_embedding_search_expression(result_metadata)\n    result_embedding = self._col_embeddings.similarity_search(\n        query=query,\n        k=k,\n        expr=expr,\n    )\n    return self._join_embedding_and_metadata_results(\n        result_embedding,\n        result_metadata,\n    )\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore_agent.VectorDatabaseAgentMilvus.store_embeddings","title":"<code>store_embeddings(documents)</code>","text":"<p>Store documents in the currently active database.</p> <pre><code>documents (List[Documents]): documents array, usually from\n    DocumentReader.load_document, DocumentReader.document_from_pdf,\n    DocumentReader.document_from_txt\n</code></pre> <pre><code>str: document id\n</code></pre> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def store_embeddings(self, documents: list[Document]) -&gt; str:\n    \"\"\"Store documents in the currently active database.\n\n    Args:\n    ----\n        documents (List[Documents]): documents array, usually from\n            DocumentReader.load_document, DocumentReader.document_from_pdf,\n            DocumentReader.document_from_txt\n\n    Returns:\n    -------\n        str: document id\n\n    \"\"\"\n    if len(documents) == 0:\n        return None\n    return self._insert_data(documents)\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore_agent.align_embeddings","title":"<code>align_embeddings(docs, meta_id)</code>","text":"<p>Ensure that the metadata id is present in each document.</p> <pre><code>docs (List[Document]): List of documents\n\nmeta_id (int): Metadata id to assign to the documents\n</code></pre> <pre><code>List[Document]: List of documents, with each document having a metadata\n    id.\n</code></pre> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def align_embeddings(docs: list[Document], meta_id: int) -&gt; list[Document]:\n    \"\"\"Ensure that the metadata id is present in each document.\n\n    Args:\n    ----\n        docs (List[Document]): List of documents\n\n        meta_id (int): Metadata id to assign to the documents\n\n    Returns:\n    -------\n        List[Document]: List of documents, with each document having a metadata\n            id.\n\n    \"\"\"\n    ret = []\n    for doc in docs:\n        ret.append(\n            Document(\n                page_content=doc.page_content,\n                metadata={\"meta_id\": meta_id},\n            ),\n        )\n    return ret\n</code></pre>"},{"location":"api-docs/vectorstore/#biochatter.vectorstore_agent.align_metadata","title":"<code>align_metadata(metadata, isDeleted=False)</code>","text":"<p>Ensure that specific metadata fields are present; if not provided, fill with \"unknown\". Also, add a random vector to each metadata item to simulate an embedding.</p> <pre><code>metadata (List[Dict]): List of metadata items\n\nisDeleted (Optional[bool], optional): Whether the document is deleted.\n    Defaults to False.\n</code></pre> <pre><code>List[List]: List of metadata items, with each item being a list of\n    metadata fields.\n</code></pre> Source code in <code>biochatter/vectorstore_agent.py</code> <pre><code>def align_metadata(\n    metadata: list[dict],\n    isDeleted: bool | None = False,\n) -&gt; list[list]:\n    \"\"\"Ensure that specific metadata fields are present; if not provided, fill with\n    \"unknown\". Also, add a random vector to each metadata item to simulate an\n    embedding.\n\n    Args:\n    ----\n        metadata (List[Dict]): List of metadata items\n\n        isDeleted (Optional[bool], optional): Whether the document is deleted.\n            Defaults to False.\n\n    Returns:\n    -------\n        List[List]: List of metadata items, with each item being a list of\n            metadata fields.\n\n    \"\"\"\n    ret = []\n    fields = METADATA_FIELDS.copy()\n    fields.pop(0)\n    for ix, k in enumerate(fields):\n        ret.append([item[k] if k in item else \"unknown\" for item in metadata])\n\n    ret.append(\n        [[random.random() for _ in range(METADATA_VECTOR_DIM)] for _ in range(len(metadata))],\n    )\n    ret.append([isDeleted for _ in metadata])\n    return ret\n</code></pre>"},{"location":"benchmark/developer/","title":"Benchmark - Developer Notes","text":"<p>To understand the benchmarking procedure, you should be familiar with Pytest. The benchmark test matrix is executed via Pytest fixtures that iterate through the combinations of test parameters such as model name and size.  This basic setup happens in the <code>conftest.py</code> file in the <code>benchmark</code> directory.  The benchmark Pytest setup is distinct from the Pytest setup we use for our continuous integration (in the <code>test</code> folder).</p> <p>You can imagine the benchmark as a set of unit tests, with the only difference being that the test subject is not our codebase, but the behaviour and performance of the various LLMs, prompts, etc. These tests are defined in two parts: the data and the method. Data are collected in a set of YAML files in the <code>benchmark/data</code> directory, while the benchmark methods are implemented in the Pytest functions in the individual Python modules (file names starting with <code>test_</code>). We also have some Python modules for auxiliary functions, such as <code>load_dataset.py</code>.</p> <p>In the following, we will describe a walkthrough of how to implement your own tests according to our benchmark philosophy.</p>"},{"location":"benchmark/developer/#test-setup","title":"Test setup","text":"<p>Tests are collected in the typical Pytest manner at the start of the procedure. In <code>conftest.py</code>, we define the model combinations we want to use in the benchmark.  We distinguish between closed-source and open-source models, since open-source models offer more flexibility, e.g., by setting their size and quantisation. In contrast, for OpenAI models, all we need is the name.</p>"},{"location":"benchmark/developer/#quickstart","title":"Quickstart","text":"<p>For getting started with developing your own benchmark, OpenAI models offer the simplest way, only requiring an API key and an internet connection. If you don't want to run open-source models right away, which is tied to setting up an external service for deploying the models, we can remove the <code>XINFERENCE_MODEL_NAMES</code> from the list of models to be benchmarked in <code>conftest.py</code> (by deleting or commenting them out):</p> <pre><code>BENCHMARKED_MODELS = OPENAI_MODEL_NAMES\n</code></pre> <p>In addition, we can reduce the number of OpenAI models to call to one for development; <code>gpt-3.5-turbo-0125</code> is a well-performing and economical initial choice (in <code>conftest.py</code>).</p> <pre><code>OPENAI_MODEL_NAMES = [\n    \"gpt-3.5-turbo-0125\",\n]\n</code></pre> <p>The last thing to look out for when running the benchmark is to reduce the number of iterations for each test to one. We run iterations to account for stochasticity in LLM responses when we run the benchmark for real, but in development, this iteration brings no benefit and just increases computational cost. Set <code>N_ITERATIONS</code> to 1 in <code>conftest.py</code>.</p> <pre><code># how often should each benchmark be run?\nN_ITERATIONS = 1\n</code></pre> <p>This setup should allow you to run and debug a newly developed benchmark dataset or method effectively. For more explanation on how to do this, please read on.</p>"},{"location":"benchmark/developer/#debugging-the-benchmark","title":"Debugging the benchmark","text":"<p>You can get some insight into how the benchmark works by debugging the existing test cases and stepping through the code line-by-line. For this, it is necessary that you are familiar with the debugging procedure in your programming environment of choice, for instance, VSCode. You can set breakpoints in the initial setup (e.g., in <code>conftest.py</code> and <code>load_dataset.py</code>) as well as the test functions (e.g., <code>test_rag_interpretation.py</code>). Stepping through the code will give you insights into how the benchmark is designed and also how the LLMs respond in detail to each specific task. This is particularly helpful for ensuring that your newly developed benchmark test cases behave as expected and test accurately the functionality you aim to test.</p>"},{"location":"benchmark/developer/#creating-new-test-cases-for-existing-tests","title":"Creating new test cases for existing tests","text":"<p>Our test cases are collected in YAML files that follow a simple formalism for defining each test. These files are found in <code>benchmark/data</code> and need to end in <code>_data.yaml</code> in order to be loaded in the test procedure. They include test cases and auxiliary materials, such as knowledge graph schemata. A test case consists of</p> <ul> <li> <p>a descriptive name</p> </li> <li> <p>a set of input data to simulate a real-world task (e.g., the question of a user and some auxiliary information)</p> </li> <li> <p>a set of expected results to assess the performance of the model's response, akin to assert statements in regular unit tests</p> </li> </ul> <p>Here is a simple example test case:</p> <pre><code>rag_interpretation:\n# test simple relevance judgement\n  - case: explicit_relevance_yes\n    input:\n      prompt: Which molecular pathways are associated with cancer?\n      system_messages:\n        [\n          \"You will receive a text fragment to help answer the user's question. Your task is to judge this text fragment for  relevance to the user's question, and return either 'yes' or 'no'; only respond with one word, do not offer explanation  or justification! Here is the fragment: \",\n          \"The EGFR pathway is deregulated in a number of cancers.\",\n        ]\n    expected:\n      answer: \"yes\"\n</code></pre> <p>In this test, we benchmark the model's ability to judge the relevance of text fragments to be used in a Retrieval-Augmented Generation (RAG) scenario in an explicit fashion (i.e., we directly ask the model to judge the relevance of the given fragments). Thus, we choose the descriptive name (<code>case</code>) <code>explicit_relevance_yes</code>. The <code>input</code>s are a simulated user question (<code>prompt</code>) and the <code>system_messages</code> that simulate the prompt engineering and RAG fragments (that in the real application would be retrieved by some suitable mechanism). Finally, we define the <code>expected</code> output, which in this case is only the <code>answer</code> we expect from the LLM.</p> <p>The way that these test cases are evaluated is defined in the Pytest functions, which are tuned to the specific fields we define in the YAML. We can freely define test definitions and testing implementation by adjusting the YAML alongside the test Python code. The coordination between cases and Pytest functions is done by name of the test category, in this example <code>rag_interpretation</code> in the YAML and <code>test_rag_interpretation.py</code> in the <code>benchmark</code> directory.</p> <p>New tests can be arbitrarily complex as long as the test function is synchronised with the content of the YAML test dataset's fields (see below).</p>"},{"location":"benchmark/developer/#combinatorial-expansion-of-tests","title":"Combinatorial expansion of tests","text":"<p>Sometimes, it can be efficient to run the same test with slightly different variations of input data. For instance, LLMs are very sensitive to the prompt setup, and thus, we can run the same test with varying approaches to the prompt engineering. To simplify this, we can define input data as dictionaries (with keys being a descriptive name of the sub-test, and values being the content), which will lead to expansion of these test cases into full cases according to the definition. For instance, we can define a RAG interpretation test with prompts at three different levels of detail (<code>simple</code>, <code>more_explicit</code>, and <code>repeat_instruction</code>):</p> <pre><code>rag_interpretation:\n  # test simple irrelevance judgement\n  - case: explicit_relevance_no\n    input:\n      prompt: Which molecular pathways are associated with cancer?\n      system_messages:\n        simple:\n          [\n            \"You will receive a text fragment to help answer the user's question. Your task is to judge this text fragment for relevance to the user's question, and return either 'yes' or 'no'! Here is the fragment: \",\n            \"The earth is a globe.\",\n          ]\n        more_explicit:\n          [\n            \"You will receive a text fragment to help answer the user's question. Your task is to judge this text fragment for relevance to the user's question, and return either 'yes' or 'no'; only respond with one word, do not offer explanation or justification! Here is the fragment: \",\n            \"The earth is a globe.\",\n          ]\n        repeat_instruction:\n          [\n            \"You will receive a text fragment to help answer the user's question. You should only respond with 'yes' or 'no' without additional words. Your task is to judge this text fragment for relevance to the user's question, and return either 'yes' or 'no'; only respond with one word, do not offer explanation or justification! Here is the fragment: \",\n            \"The earth is a globe.\",\n          ]\n    expected:\n      answer: \"no\"\n</code></pre> <p>Upon instantiation of the test matrix, this definition will be expanded into three full tests, each with their respective prompt setup. You can define as many combinations as you like (for instance, you could also define a list of prompts in this example), but be aware that the number of tests will grow exponentially with the number of combinations.</p>"},{"location":"benchmark/developer/#setting-up-the-test-data-pipeline","title":"Setting up the test data pipeline","text":"<p>Test data are provided to the test functions via fixtures. The fixtures are defined in the <code>conftest.py</code> file and are used to load the test data from the YAML files. If you add a new test module or a function with a new kind of test data, you need to add the corresponding fixture to the <code>pytest_generate_tests</code> function in <code>conftest.py</code>. This function is responsible for loading the test data and providing appropriately named fixtures to the test functions. For the tests defined above, this equates to:</p> <pre><code>def pytest_generate_tests(metafunc):\n    data = BENCHMARK_DATASET\n    if \"test_data_rag_interpretation\" in metafunc.fixturenames:\n        metafunc.parametrize(\n            \"test_data_rag_interpretation\",\n            data[\"rag_interpretation\"],\n        )\n</code></pre> <p>We prepend the fixtures with <code>test_data_</code> for consistency and higher code readability. For more information, see the Pytest Documentation.</p>"},{"location":"benchmark/developer/#creating-new-test-procedures","title":"Creating new test procedures","text":"<p>If a new kind of test requires a bespoke procedure, such as evaluating a newly introduced functionality or calculating a score in a distinct way, we can introduce new methods to the test modules or even entire new modules. Following the layout of the existing tests, the newly created test functions should refer to fixtures for their data inputs.  Such a test function typically has as parameters:</p> <ul> <li> <p>the <code>model_name</code> fixture, to be able to record a model-specific benchmark metric;</p> </li> <li> <p>a <code>test_data</code> object that is generated from the benchmark dataset according to the name of the test module (e.g., <code>test_data_rag_interpretation</code>). This is the fixture you defined above in <code>conftest.py</code>;</p> </li> <li> <p>a <code>conversation</code> instance (the connection to the LLM to be tested);</p> </li> <li> <p>the <code>multiple_testing</code> fixture that implements running the test multiple times and averaging the results;</p> </li> <li> <p>any number of additional inputs that may be required for the tests.</p> </li> </ul> <p>For instance, the knowledge graph query generation tests acquire additional tests inputs from the YAML definition (the schema of the BioCypher knowledge graph underlying the test) and additional functionality from BioChatter (an instance of the prompt engine class that generates the knowledge graph query using the aforementioned schema).</p>"},{"location":"benchmark/developer/#running-the-benchmark","title":"Running the benchmark","text":"<p>If everything is set up correctly, you can run the benchmark by executing the following command in the root directory of the repository:</p> <pre><code>poetry run pytest benchmark\n</code></pre> <p>We need to specify the <code>benchmark</code> directory to run the benchmark tests, because we also have regular tests in the <code>test</code> directory. If you want to run only a specific test module, you can specify the file name (or use any other Pytest workflow).</p> <p>Skipping tests</p> <p>For efficiency reasons, we by default do not rerun tests that have already been executed for a given model and test case. For this purpose, we store the results in the <code>benchmark/results</code> directory, including, for every test case, an md5 hash of the input data. If you want to rerun a test, you can delete the corresponding line (or entire file) in the <code>results</code> directory.</p> <p>We re-run the benchmark automatically if a test case has changed (reflected in a different md5 hash) or if there is a new <code>biochatter</code> version (potentially introducing changes in the behaviour of the framework). If a test case has changed, the old result is automatically removed from the result files. You can also force a rerun of all tests by using the <code>--run-all</code> flag:</p> <pre><code>poetry run pytest benchmark --run-all\n</code></pre>"},{"location":"benchmark/developer/#processing-benchmark-results","title":"Processing benchmark results","text":"<p>The benchmark results are processed to generate visualizations and statistics that are displayed in the documentation. This processing used to happen during the documentation build but has been moved to a separate workflow for better efficiency:</p> <ol> <li> <p>The processing script is located at <code>docs/scripts/hooks.py</code> and can be run directly:</p> <pre><code>python docs/scripts/hooks.py\n</code></pre> </li> <li> <p>The processing is automated through a GitHub Action that runs when:</p> <ul> <li> <p>Changes are pushed to the <code>main</code> branch that affect benchmark results</p> </li> <li> <p>Pull requests targeting <code>main</code> include benchmark result changes</p> </li> <li> <p>Manual triggers via workflow dispatch in GitHub Actions</p> </li> </ul> </li> </ol> <p>This separation means that the heavy processing of benchmark results only happens when the results actually change, rather than on every documentation build.</p>"},{"location":"benchmark/developer/#running-open-source-models","title":"Running open-source models","text":"<p>To execute the benchmark on any of the open-source models in the test matrix, you need to deploy an Xorbits Inference server at an arbitrary IP, either via Docker (available on Linux machines with dedicated Nvidia GPU) or natively (e.g., on Apple machines). Please refer to the Xinference documentation for details.</p> <p>When you have deployed the Xinference server, you can point the benchmark to the server by setting the <code>BENCHMARK_URL</code> parameter in <code>conftest.py</code>:</p> <pre><code># Xinference IP and port\nBENCHMARK_URL = \"http://localhost:9997\"\n</code></pre>"},{"location":"benchmark/overview/","title":"Benchmark Results - Overview","text":"<p>Here we collect the results of the living BioChatter benchmark. For an explanation, see the benchmarking documentation and the developer docs for further reading.</p>"},{"location":"benchmark/overview/#scores-per-model","title":"Scores per model","text":"<p>Table sorted by median score in descending order. Click the column names to reorder.</p> Model name Size Median Accuracy SD claude-3-opus-20240229 Unknown 0.77 0.28 gpt-3.5-turbo-0125 175 0.77 0.19 gpt-3.5-turbo-0613 175 0.76 0.21 claude-3-5-sonnet-20240620 Unknown 0.76 0.28 gpt-4-0613 Unknown 0.73 0.17 gpt-4o-2024-08-06 Unknown 0.73 0.24 llama-3.1-instruct 70 0.73 0.29 llama-3.1-instruct 8 0.72 0.28 gpt-4-0125-preview Unknown 0.69 0.26 gpt-4o-mini-2024-07-18 Unknown 0.69 0.22 gpt-4-1106-preview Unknown 0.69 0.06 gpt-4o-2024-05-13 Unknown 0.68 0.31 gpt-4-turbo-2024-04-09 Unknown 0.68 0.27 llama-3-instruct 8 0.65 0.36 openhermes-2.5 7 0.6 0.3 gpt-4o-2024-11-20 Unknown 0.55 0.14 chatglm3 6 0.44 0.26 llama-2-chat 70 0.42 0.34 mistral-instruct-v0.2 7 0.4 0.33 code-llama-instruct 7 0.4 0.35 code-llama-instruct 34 0.38 0.35 code-llama-instruct 13 0.38 0.33 llama-2-chat 13 0.38 0.33 mixtral-instruct-v0.1 46,7 0.34 0.28 llama-2-chat 7 0.31 0.3 <p> </p>"},{"location":"benchmark/overview/#scores-per-quantisation","title":"Scores per quantisation","text":"<p>Table sorted by median score in descending order. Click the column names to reorder.</p> Model name Size Version Quantisation Median Accuracy SD claude-3-opus-20240229 Unknown nan nan 0.77 0.28 gpt-3.5-turbo-0125 175 nan nan 0.77 0.19 claude-3-5-sonnet-20240620 Unknown nan nan 0.76 0.28 gpt-3.5-turbo-0613 175 nan nan 0.76 0.21 llama-3.1-instruct 8 ggufv2 Q6_K 0.74 0.28 llama-3.1-instruct 70 ggufv2 IQ2_M 0.74 0.29 llama-3.1-instruct 8 ggufv2 Q5_K_M 0.74 0.28 gpt-4-0613 Unknown nan nan 0.73 0.17 llama-3.1-instruct 70 ggufv2 IQ4_XS 0.73 0.29 gpt-4o-2024-08-06 Unknown nan nan 0.73 0.24 llama-3.1-instruct 8 ggufv2 Q8_0 0.72 0.3 llama-3.1-instruct 8 ggufv2 Q3_K_L 0.71 0.28 llama-3.1-instruct 8 ggufv2 Q4_K_M 0.7 0.26 llama-3.1-instruct 8 ggufv2 IQ4_XS 0.69 0.28 gpt-4-1106-preview Unknown nan nan 0.69 0.06 gpt-4-0125-preview Unknown nan nan 0.69 0.26 gpt-4o-mini-2024-07-18 Unknown nan nan 0.69 0.22 gpt-4-turbo-2024-04-09 Unknown nan nan 0.68 0.27 gpt-4o-2024-05-13 Unknown nan nan 0.68 0.31 llama-3.1-instruct 70 ggufv2 Q3_K_S 0.67 0.28 llama-3-instruct 8 ggufv2 Q8_0 0.65 0.35 llama-3-instruct 8 ggufv2 Q4_K_M 0.65 0.38 llama-3-instruct 8 ggufv2 Q6_K 0.65 0.36 openhermes-2.5 7 ggufv2 Q6_K 0.62 0.3 llama-3-instruct 8 ggufv2 Q5_K_M 0.62 0.36 openhermes-2.5 7 ggufv2 Q5_K_M 0.6 0.29 openhermes-2.5 7 ggufv2 Q8_0 0.6 0.3 openhermes-2.5 7 ggufv2 Q4_K_M 0.6 0.3 openhermes-2.5 7 ggufv2 Q3_K_M 0.56 0.3 gpt-4o-2024-11-20 Unknown nan nan 0.55 0.14 code-llama-instruct 34 ggufv2 Q2_K 0.5 0.33 openhermes-2.5 7 ggufv2 Q2_K 0.5 0.28 code-llama-instruct 7 ggufv2 Q3_K_M 0.49 0.31 code-llama-instruct 7 ggufv2 Q4_K_M 0.47 0.39 mistral-instruct-v0.2 7 ggufv2 Q5_K_M 0.46 0.34 mistral-instruct-v0.2 7 ggufv2 Q6_K 0.45 0.34 code-llama-instruct 34 ggufv2 Q3_K_M 0.45 0.31 chatglm3 6 ggmlv3 q4_0 0.44 0.26 llama-2-chat 70 ggufv2 Q5_K_M 0.44 0.35 llama-2-chat 70 ggufv2 Q4_K_M 0.44 0.35 code-llama-instruct 13 ggufv2 Q6_K 0.44 0.35 code-llama-instruct 13 ggufv2 Q8_0 0.44 0.33 code-llama-instruct 13 ggufv2 Q5_K_M 0.43 0.32 llama-2-chat 70 ggufv2 Q3_K_M 0.41 0.33 mistral-instruct-v0.2 7 ggufv2 Q3_K_M 0.41 0.34 mistral-instruct-v0.2 7 ggufv2 Q8_0 0.4 0.33 llama-2-chat 13 ggufv2 Q8_0 0.4 0.34 code-llama-instruct 7 ggufv2 Q8_0 0.4 0.37 code-llama-instruct 7 ggufv2 Q5_K_M 0.39 0.34 llama-2-chat 13 ggufv2 Q3_K_M 0.39 0.33 llama-2-chat 13 ggufv2 Q5_K_M 0.39 0.33 code-llama-instruct 7 ggufv2 Q2_K 0.38 0.29 code-llama-instruct 34 ggufv2 Q4_K_M 0.38 0.35 code-llama-instruct 7 ggufv2 Q6_K 0.38 0.39 code-llama-instruct 34 ggufv2 Q5_K_M 0.38 0.38 llama-2-chat 70 ggufv2 Q2_K 0.38 0.35 llama-2-chat 13 ggufv2 Q6_K 0.37 0.34 code-llama-instruct 34 ggufv2 Q8_0 0.37 0.35 mistral-instruct-v0.2 7 ggufv2 Q2_K 0.37 0.29 mistral-instruct-v0.2 7 ggufv2 Q4_K_M 0.37 0.35 code-llama-instruct 34 ggufv2 Q6_K 0.37 0.36 llama-2-chat 13 ggufv2 Q4_K_M 0.36 0.34 mixtral-instruct-v0.1 46,7 ggufv2 Q4_K_M 0.35 0.3 mixtral-instruct-v0.1 46,7 ggufv2 Q5_K_M 0.34 0.31 mixtral-instruct-v0.1 46,7 ggufv2 Q6_K 0.34 0.29 llama-2-chat 7 ggufv2 Q5_K_M 0.34 0.29 mixtral-instruct-v0.1 46,7 ggufv2 Q3_K_M 0.33 0.28 llama-2-chat 7 ggufv2 Q2_K 0.33 0.32 code-llama-instruct 13 ggufv2 Q4_K_M 0.33 0.31 mixtral-instruct-v0.1 46,7 ggufv2 Q8_0 0.33 0.25 mixtral-instruct-v0.1 46,7 ggufv2 Q2_K 0.32 0.27 llama-2-chat 7 ggufv2 Q8_0 0.31 0.28 llama-2-chat 7 ggufv2 Q6_K 0.31 0.29 llama-2-chat 7 ggufv2 Q4_K_M 0.3 0.29 llama-2-chat 7 ggufv2 Q3_K_M 0.3 0.33 llama-2-chat 13 ggufv2 Q2_K 0.28 0.29 code-llama-instruct 13 ggufv2 Q2_K 0.17 0.34 code-llama-instruct 13 ggufv2 Q3_K_M 0.15 0.34 <p></p>"},{"location":"benchmark/overview/#scores-of-all-tasks","title":"Scores of all tasks","text":"<p>Wide table; you may need to scroll horizontally to see all columns. Table sorted by median score in descending order. Click the column names to reorder.</p> Full model name naive_query_generation_using_schema entity_selection sourcedata_info_extraction explicit_relevance_of_single_fragments property_selection relationship_selection api_calling query_generation medical_exam end_to_end_query_generation implicit_relevance_of_multiple_fragments multimodal_answer python_api_calling property_exists python_api_calling_reduced Mean Accuracy Median Accuracy SD claude-3-opus-20240229 0.733333 1 0.691235 0.833333 0.421875 0 nan 0.944444 0.805556 0.655556 1 nan nan 1 nan 0.73503 0.770293 0.276411 gpt-3.5-turbo-0125 0.514451 0.978261 0.510032 1 0.36747 1 0.746479 0.953757 0.671322 0.919075 0.9 nan 0.615789 0.789474 0.759615 0.766123 0.766123 0.191717 claude-3-5-sonnet-20240620 0.633333 1 0.756088 1 0.375 0 nan 0.966667 0.7737 0.733333 1 nan nan 0.866667 nan 0.736799 0.764894 0.283502 gpt-3.5-turbo-0613 0.5 0.888889 0.575381 1 0.3625 0.5 nan 0.946667 nan 0.833333 1 nan nan 0.755556 nan 0.736233 0.755556 0.211926 llama-3.1-instruct:8:ggufv2:Q6_K 0.633333 1 0.394469 1 0.46875 0 nan 0.955556 0.751748 0.733333 0.833333 nan nan 0.833333 nan 0.69126 0.742541 0.278015 llama-3.1-instruct:70:ggufv2:IQ2_M 0.633333 1 0.626498 1 0.328125 0 nan 0.955556 0.772881 0.6 1 nan nan 0.916667 nan 0.712096 0.742489 0.293676 llama-3.1-instruct:8:ggufv2:Q5_K_M 0.7 1 0.380477 1 0.4375 0 nan 0.933333 0.749117 0.733333 0.833333 nan nan 0.833333 nan 0.690948 0.741225 0.279278 gpt-4-0613 0.682081 0.920635 0.668903 1 0.38253 0.695652 0.619048 0.959538 0.730912 0.878613 1 nan 0.489474 0.666667 0.769231 0.747377 0.730912 0.174471 llama-3.1-instruct:70:ggufv2:IQ4_XS 0.566667 1 0.699238 1 0.375 0 nan 0.955556 0.822581 0.6 1 nan nan 0.75 nan 0.706276 0.728138 0.285226 gpt-4o-2024-08-06 0.528302 1 0.711185 1 0.425439 1 nan 0.874214 0.850211 0.830189 0.666667 nan 0.363158 0.179894 0.740385 0.705357 0.725785 0.243719 llama-3.1-instruct:8:ggufv2:Q8_0 0.660377 1 0.38907 1 0.565789 0.142857 nan 0.937107 0.765734 0.773585 1 nan nan 0.161765 nan 0.67239 0.719062 0.295232 llama-3.1-instruct:8:ggufv2:Q3_K_L 0.622642 1 0.360379 1 0.486842 0.142857 nan 0.943396 0.768421 0.811321 0.833333 nan nan 0.24 nan 0.655381 0.711901 0.280463 llama-3.1-instruct:8:ggufv2:Q4_K_M 0.660377 0.92 0.382027 1 0.513158 0.285714 nan 0.924528 0.741935 0.735849 0.833333 nan nan 0.150538 nan 0.649769 0.698113 0.257418 llama-3.1-instruct:8:ggufv2:IQ4_XS 0.646018 0.893939 0.414621 1 0.387255 0 nan 0.946903 0.756944 0.743363 0.833333 nan nan 0.439394 nan 0.641979 0.69469 0.276449 gpt-4-1106-preview nan nan nan nan nan nan nan nan nan nan nan nan 0.616216 nan 0.767677 0.691946 0.691946 0.0618335 gpt-4-0125-preview 0.456647 0.824561 0.689705 1 0.0301205 0.782609 0.793651 0.83815 0.777159 0.109827 0.5 nan 0.508772 0.619048 0.769231 0.621391 0.689705 0.259268 gpt-4o-mini-2024-07-18 0.537572 0.921053 0.684553 0.833333 0.388554 0.130435 0.714286 0.953757 0.840796 0.687861 0.5 0.98 0.578947 0.52439 0.730769 0.667087 0.686207 0.216242 gpt-4-turbo-2024-04-09 0.508671 1 0.650369 1 0.325301 0.130435 nan 0.83237 0.83701 0.635838 1 0.99 0.259649 0.657143 0.762821 0.684972 0.684972 0.270175 gpt-4o-2024-05-13 0.537572 1 0.653946 1 0.0301205 0.130435 0.809524 0.809249 0.763501 0.115607 0.7 0.96 nan 0.526316 nan 0.618175 0.676973 0.312259 llama-3.1-instruct:70:ggufv2:Q3_K_S 0.633333 1 0.642336 1 0.375 0 nan 0.966667 0.8 0.6 1 nan nan 0.625 nan 0.694758 0.668547 0.28438 llama-3-instruct:8:ggufv2:Q8_0 0.666667 0.875 0.188555 1 0.28125 0 nan 0.92 0.640669 0 1 nan nan 0.725 nan 0.572467 0.653668 0.35454 llama-3-instruct:8:ggufv2:Q4_K_M 0.666667 0.861111 0.116871 1 0.109375 0 nan 0.92 0.624884 0 1 nan nan 0.775 nan 0.552173 0.645775 0.376754 llama-3-instruct:8:ggufv2:Q6_K 0.666667 0.875 0.162657 1 0.28125 0 nan 0.926667 0.623955 0 1 nan nan 0.775 nan 0.573745 0.645311 0.359165 openhermes-2.5:7:ggufv2:Q6_K 0.557078 0.7675 0.619167 1 0.126404 0.896552 nan 0.890411 0.57423 0.273973 1 nan nan 0.113379 nan 0.619881 0.619524 0.300867 llama-3-instruct:8:ggufv2:Q5_K_M 0.6 0.875 0.166434 1 0.1875 0 nan 0.926667 0.635097 0 1 nan nan 0.65 nan 0.549154 0.617549 0.360565 openhermes-2.5:7:ggufv2:Q5_K_M 0.56621 0.758808 0.579916 1 0.196629 0.896552 nan 0.917808 0.571429 0.26484 1 nan nan 0.120988 nan 0.624835 0.602375 0.293641 openhermes-2.5:7:ggufv2:Q8_0 0.497717 0.628118 0.600829 1 0.196629 0.896552 nan 0.90411 0.577031 0.237443 1 nan nan 0.100629 nan 0.603551 0.60219 0.296591 openhermes-2.5:7:ggufv2:Q4_K_M 0.479452 0.66967 0.597281 1 0.126404 0.896552 nan 0.894977 0.586368 0.246575 1 nan nan 0.140921 nan 0.603473 0.600377 0.299216 openhermes-2.5:7:ggufv2:Q3_K_M 0.47032 1 0.554488 1 0.171348 1 nan 0.917808 0.563959 0.287671 0.5 nan nan 0.156805 nan 0.602036 0.559223 0.301462 gpt-4o-2024-11-20 nan nan nan nan nan nan nan nan nan nan nan nan 0.377193 nan 0.730769 0.553981 0.553981 0.144347 code-llama-instruct:34:ggufv2:Q2_K 0.566667 0 nan 0.5 0 0.5 nan 0.686667 nan 0 1 nan nan 0.75 nan 0.444815 0.5 0.328199 openhermes-2.5:7:ggufv2:Q2_K 0.420091 0.604444 0.444054 1 0.0168539 0.655172 nan 0.917808 0.539106 0.159817 0.5 nan nan 0.18638 nan 0.494884 0.497442 0.27656 code-llama-instruct:7:ggufv2:Q3_K_M 0.426667 0.5 nan 0.833333 0 0.25 nan 0.873333 nan 0 0.7 nan nan 0.8 nan 0.487037 0.493519 0.307716 code-llama-instruct:7:ggufv2:Q4_K_M 0.653333 0.333333 0.138732 1 0 0 nan 0.966667 nan 0 1 nan nan 0.6 nan 0.469207 0.469207 0.38731 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0.466667 0.444444 0.385754 1 0 0 nan 0.826667 0.364146 0 1 nan nan 0.688889 nan 0.470597 0.455556 0.34385 mistral-instruct-v0.2:7:ggufv2:Q6_K 0.433333 0.5 0.367412 1 0.046875 0 nan 0.833333 0.366947 0 1 nan nan 0.65 nan 0.472536 0.452935 0.337974 code-llama-instruct:34:ggufv2:Q3_K_M 0.6 0 nan 0.5 0 0.25 nan 0.786667 nan 0 0.5 nan nan 0.875 nan 0.390185 0.445093 0.306514 chatglm3:6:ggmlv3:q4_0 0.48 0.75 0.188284 0.733333 0.2875 0.4 nan 0.553333 0.426704 0 1 nan nan 0.275 nan 0.463105 0.444905 0.260423 llama-2-chat:70:ggufv2:Q5_K_M 0.36 0.444444 0.210166 1 0 0.25 nan 0.906667 nan 0 0.9 nan nan 0.777778 nan 0.484905 0.444444 0.346535 llama-2-chat:70:ggufv2:Q4_K_M 0.42 0.444444 0.240936 1 0 0.25 nan 0.92 nan 0 1 nan nan 0.755556 nan 0.503094 0.444444 0.354692 code-llama-instruct:13:ggufv2:Q6_K 0.54 0 nan 0.833333 0 0 nan 0.793333 nan 0 0.5 nan nan 0.825 nan 0.387963 0.443981 0.345581 code-llama-instruct:13:ggufv2:Q8_0 0.566667 0 nan 0.833333 0 0 nan 0.766667 nan 0 0.5 nan nan 0.75 nan 0.37963 0.439815 0.334971 code-llama-instruct:13:ggufv2:Q5_K_M 0.566667 0 nan 0.666667 0 0 nan 0.78 nan 0 0.5 nan nan 0.775 nan 0.36537 0.432685 0.320506 llama-2-chat:70:ggufv2:Q3_K_M 0.413333 0.333333 0.197898 1 0.171875 0 nan 0.906667 nan 0 0.5 nan nan 0.777778 nan 0.430088 0.413333 0.327267 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 0.466667 0.333333 0.368974 1 0.046875 0 nan 0.773333 0.360411 0 1 nan nan 0.666667 nan 0.456024 0.412499 0.335885 mistral-instruct-v0.2:7:ggufv2:Q8_0 0.433333 0.333333 0.351684 1 0.0375 0 nan 0.846667 0.366947 0 0.9 nan nan 0.644444 nan 0.446719 0.40014 0.330107 llama-2-chat:13:ggufv2:Q8_0 0.48 0 0.0762457 1 0 0 nan 0.786667 0.431373 0 0.5 nan nan 0.711111 nan 0.362309 0.396841 0.335904 code-llama-instruct:7:ggufv2:Q8_0 0.4 0 nan 1 0 0 nan 0.96 nan 0 0.5 nan nan 0.666667 nan 0.391852 0.395926 0.37338 code-llama-instruct:7:ggufv2:Q5_K_M 0.4 0.111111 nan 0.833333 0 0 nan 0.96 nan 0 0.5 nan nan 0.688889 nan 0.388148 0.394074 0.340156 llama-2-chat:13:ggufv2:Q3_K_M 0.48 0 0.112631 1 0 0 nan 0.68 0.428571 0 0.5 nan nan 0.733333 nan 0.357685 0.393128 0.325419 llama-2-chat:13:ggufv2:Q5_K_M 0.433333 0 0.0766167 1 0 0 nan 0.746667 0.431373 0 0.5 nan nan 0.644444 nan 0.348403 0.389888 0.32518 code-llama-instruct:7:ggufv2:Q2_K 0.533333 0.25 nan 0.333333 0.0625 0.25 nan 0.92 nan 0 0.7 nan nan 0.8 nan 0.427685 0.380509 0.292686 code-llama-instruct:34:ggufv2:Q4_K_M 0.466667 0 nan 0.5 0 0 nan 0.906667 nan 0 0.4 nan nan 0.975 nan 0.360926 0.380463 0.350483 code-llama-instruct:7:ggufv2:Q6_K 0.333333 0 nan 0.833333 0 0 nan 0.96 nan 0 0.9 nan nan 0.775 nan 0.422407 0.37787 0.391629 code-llama-instruct:34:ggufv2:Q5_K_M 0.466667 0.125 nan 0.333333 0 0 nan 0.9 nan 0 1 nan nan 0.95 nan 0.419444 0.376389 0.384096 llama-2-chat:70:ggufv2:Q2_K 0.473333 0 0.215047 1 0 0 nan 0.9 nan 0 0.5 nan nan 0.666667 nan 0.375505 0.375505 0.352226 llama-2-chat:13:ggufv2:Q6_K 0.386667 0 0.0781337 1 0 0 nan 0.813333 0.428571 0 0.5 nan nan 0.775 nan 0.361973 0.37432 0.342819 code-llama-instruct:34:ggufv2:Q8_0 0.466667 0.25 nan 0.333333 0 0 nan 0.86 nan 0 0.9 nan nan 0.925 nan 0.415 0.374167 0.353285 mistral-instruct-v0.2:7:ggufv2:Q2_K 0.573333 0.222222 0.331261 1 0 0 nan 0.693333 0.352941 0 0.5 nan nan 0.6 nan 0.388463 0.370702 0.294881 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0.366667 0.333333 0.347025 1 0 0 nan 0.826667 0.365079 0 1 nan nan 0.688889 nan 0.447969 0.365873 0.348328 code-llama-instruct:34:ggufv2:Q6_K 0.473333 0.125 nan 0.333333 0 0 nan 0.853333 nan 0 0.9 nan nan 0.9 nan 0.398333 0.365833 0.356636 llama-2-chat:13:ggufv2:Q4_K_M 0.366667 0 0.0888675 1 0 0 nan 0.76 0.428571 0 0.5 nan nan 0.777778 nan 0.356535 0.361601 0.336686 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 0.426667 0.333333 0.193786 0.166667 0.1625 0 nan 0.76 0.368814 0 1 nan nan 0.755556 nan 0.378848 0.351074 0.301567 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0.333333 0.422222 0.235659 0 0 0.25 nan 0.84 0.352941 0 1 nan nan 0.711111 nan 0.376842 0.343137 0.313874 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0.333333 0.475 0.225524 0 0 0.25 nan 0.826667 0.34267 0 0.7 nan nan 0.85 nan 0.363927 0.338002 0.289705 llama-2-chat:7:ggufv2:Q5_K_M 0.3379 0.57037 0.0697591 1 0.0337079 0 nan 0.547945 0.40056 0 0.6 nan nan 0.155556 nan 0.3378 0.33785 0.293711 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 0.38 0.333333 0.229622 0 0.065625 0.25 nan 0.893333 nan 0 0.5 nan nan 0.777778 nan 0.342969 0.333333 0.278279 llama-2-chat:7:ggufv2:Q2_K 0.164384 0.410256 0.0361865 0.833333 0 0 nan 0.611872 0.369748 0 1 nan nan 0.324786 nan 0.340961 0.332873 0.320018 code-llama-instruct:13:ggufv2:Q4_K_M 0.533333 0 nan 0.333333 0 0 nan 0.833333 nan 0 0.5 nan nan 0.775 nan 0.330556 0.331944 0.30939 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0.386667 0.311111 0.189177 0.133333 0 0.25 nan 0.846667 0.358543 0 0.6 nan nan 0.666667 nan 0.340197 0.325654 0.248216 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0.48 0 0.157514 0.333333 0 0 nan 0.726667 0.329599 0 0.6 nan nan 0.733333 nan 0.305495 0.317547 0.269925 llama-2-chat:7:ggufv2:Q8_0 0.292237 0.481481 0.0847297 1 0 0.103448 nan 0.589041 0.40056 0 0.5 nan nan 0.163399 nan 0.328627 0.310432 0.278624 llama-2-chat:7:ggufv2:Q6_K 0.292237 0.553846 0.0614608 1 0 0 nan 0.561644 0.406162 0 0.5 nan nan 0.177778 nan 0.323012 0.307625 0.290181 llama-2-chat:7:ggufv2:Q4_K_M 0.273973 0.57037 0.0852494 1 0 0 nan 0.611872 0.40056 0 0.5 nan nan 0.251852 nan 0.335807 0.30489 0.291028 llama-2-chat:7:ggufv2:Q3_K_M 0.228311 0.435897 0.0650717 1 0.0898876 0.103448 nan 0.589041 0.394958 0 1 nan nan 0.207407 nan 0.374002 0.301156 0.326263 llama-2-chat:13:ggufv2:Q2_K 0.366667 0 0.0649389 1 0 0 nan 0.433333 0.414566 0 0.5 nan nan 0.288889 nan 0.278945 0.283917 0.285171 code-llama-instruct:13:ggufv2:Q2_K 0.566667 0 nan 0.0333333 0 0 nan 0.82 nan 0 0.4 nan nan 0.875 nan 0.299444 0.166389 0.336056 code-llama-instruct:13:ggufv2:Q3_K_M 0.533333 0.45 nan 0 0 0 nan 0.833333 nan 0 0 nan nan 0.85 nan 0.296296 0.148148 0.336707"},{"location":"benchmark/results/","title":"Benchmark - All Results","text":""},{"location":"benchmark/results/#biocypher-query-generation","title":"BioCypher query generation","text":"<p>In this set of tasks, we test LLM abilities to generate queries for a BioCypher Knowledge Graph using BioChatter. The <code>schema_config.yaml</code> of the BioCypher Knowledge Graph and a natural language query are passed to BioChatter.</p> <p>Individual steps of the query generation process are tested separately, as well as the end-to-end performance of the process.</p> Entity selectionRelationship selectionProperty selectionProperty confabulationNaive query generationBioChatter query generationEnd to end query generation Full model name Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 22 22 0 1 3 claude-3-opus-20240229 24 24 0 1 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 24 24 0 1 3 openhermes-2.5:7:ggufv2:Q3_K_M 279 279 0 1 5 llama-3.1-instruct:70:ggufv2:IQ4_XS 24 24 0 1 3 llama-3.1-instruct:70:ggufv2:Q3_K_S 24 24 0 1 3 llama-3.1-instruct:70:ggufv2:IQ2_M 24 24 0 1 3 llama-3.1-instruct:8:ggufv2:Q3_K_L 132 132 0 1 3 llama-3.1-instruct:8:ggufv2:Q6_K 24 24 0 1 3 gpt-4o-2024-05-13 70 70 0 1 5 gpt-4-turbo-2024-04-09 64 64 0 1 5 gpt-4o-2024-08-06 42 42 0 1 3 llama-3.1-instruct:8:ggufv2:Q8_0 186 186 0 1 3 gpt-3.5-turbo-0125 45 46 0 0.978261 5 gpt-4o-mini-2024-07-18 70 76 0 0.921053 5 gpt-4-0613 58 63 0 0.920635 5 llama-3.1-instruct:8:ggufv2:Q4_K_M 138 150 0 0.92 3 llama-3.1-instruct:8:ggufv2:IQ4_XS 59 66 0 0.893939 3 gpt-3.5-turbo-0613 40 45 0 0.888889 5 llama-3-instruct:8:ggufv2:Q8_0 35 40 0 0.875 5 llama-3-instruct:8:ggufv2:Q6_K 35 40 0 0.875 5 llama-3-instruct:8:ggufv2:Q5_K_M 35 40 0 0.875 5 llama-3-instruct:8:ggufv2:Q4_K_M 31 36 0 0.861111 5 gpt-4-0125-preview 47 57 0 0.824561 5 openhermes-2.5:7:ggufv2:Q6_K 307 400 0 0.7675 5 openhermes-2.5:7:ggufv2:Q5_K_M 280 369 0 0.758808 5 chatglm3:6:ggmlv3:q4_0 30 40 0 0.75 5 openhermes-2.5:7:ggufv2:Q4_K_M 223 333 0 0.66967 5 openhermes-2.5:7:ggufv2:Q8_0 277 441 0 0.628118 5 openhermes-2.5:7:ggufv2:Q2_K 136 225 0 0.604444 5 llama-2-chat:7:ggufv2:Q4_K_M 77 135 0 0.57037 5 llama-2-chat:7:ggufv2:Q5_K_M 77 135 0 0.57037 5 llama-2-chat:7:ggufv2:Q6_K 72 130 0 0.553846 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 20 40 0 0.5 5 code-llama-instruct:7:ggufv2:Q3_K_M 20 40 0 0.5 5 llama-2-chat:7:ggufv2:Q8_0 65 135 0 0.481481 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 19 40 0 0.475 5 code-llama-instruct:13:ggufv2:Q3_K_M 18 40 0 0.45 5 llama-2-chat:70:ggufv2:Q4_K_M 20 45 0 0.444444 5 llama-2-chat:70:ggufv2:Q5_K_M 20 45 0 0.444444 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 20 45 0 0.444444 5 llama-2-chat:7:ggufv2:Q3_K_M 51 117 0 0.435897 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 19 45 0 0.422222 5 llama-2-chat:7:ggufv2:Q2_K 48 117 0 0.410256 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 15 45 0 0.333333 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 15 45 0 0.333333 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 15 45 0 0.333333 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 15 45 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 15 45 0 0.333333 5 llama-2-chat:70:ggufv2:Q3_K_M 15 45 0 0.333333 5 code-llama-instruct:7:ggufv2:Q4_K_M 15 45 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 14 45 0 0.311111 5 code-llama-instruct:34:ggufv2:Q8_0 10 40 0 0.25 5 code-llama-instruct:7:ggufv2:Q2_K 10 40 0 0.25 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 10 45 0 0.222222 5 code-llama-instruct:34:ggufv2:Q5_K_M 5 40 0 0.125 5 code-llama-instruct:34:ggufv2:Q6_K 5 40 0 0.125 5 code-llama-instruct:7:ggufv2:Q5_K_M 5 45 0 0.111111 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 40 0 0 5 code-llama-instruct:13:ggufv2:Q2_K 0 40 0 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 40 0 0 5 code-llama-instruct:34:ggufv2:Q3_K_M 0 40 0 0 5 code-llama-instruct:34:ggufv2:Q2_K 0 40 0 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 40 0 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 40 0 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 40 0 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 45 0 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 45 0 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 40 0 0 5 llama-2-chat:13:ggufv2:Q2_K 0 45 0 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 45 0 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 45 0 0 5 llama-2-chat:13:ggufv2:Q8_0 0 45 0 0 5 llama-2-chat:13:ggufv2:Q6_K 0 40 0 0 5 llama-2-chat:70:ggufv2:Q2_K 0 45 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 45 0 0 5 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-3.5-turbo-0125 69 69 0 1 5 openhermes-2.5:7:ggufv2:Q3_K_M 87 87 0 1 5 gpt-4o-2024-08-06 63 63 0 1 3 openhermes-2.5:7:ggufv2:Q5_K_M 78 87 0 0.896552 5 openhermes-2.5:7:ggufv2:Q4_K_M 78 87 0 0.896552 5 openhermes-2.5:7:ggufv2:Q8_0 78 87 0 0.896552 5 openhermes-2.5:7:ggufv2:Q6_K 78 87 0 0.896552 5 gpt-4-0125-preview 54 69 0 0.782609 5 gpt-4-0613 48 69 0 0.695652 5 openhermes-2.5:7:ggufv2:Q2_K 57 87 0 0.655172 5 code-llama-instruct:34:ggufv2:Q2_K 30 60 0 0.5 5 gpt-3.5-turbo-0613 30 60 0 0.5 5 chatglm3:6:ggmlv3:q4_0 24 60 0 0.4 5 llama-3.1-instruct:8:ggufv2:Q4_K_M 18 63 0 0.285714 3 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 15 60 0 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 15 60 0 0.25 5 llama-2-chat:70:ggufv2:Q4_K_M 15 60 0 0.25 5 code-llama-instruct:7:ggufv2:Q2_K 15 60 0 0.25 5 code-llama-instruct:7:ggufv2:Q3_K_M 15 60 0 0.25 5 llama-2-chat:70:ggufv2:Q5_K_M 15 60 0 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 15 60 0 0.25 5 code-llama-instruct:34:ggufv2:Q3_K_M 15 60 0 0.25 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 15 60 0 0.25 5 llama-3.1-instruct:8:ggufv2:Q3_K_L 9 63 0 0.142857 3 llama-3.1-instruct:8:ggufv2:Q8_0 9 63 0 0.142857 3 gpt-4-turbo-2024-04-09 9 69 0 0.130435 5 gpt-4o-2024-05-13 9 69 0 0.130435 5 gpt-4o-mini-2024-07-18 9 69 0 0.130435 5 llama-2-chat:7:ggufv2:Q8_0 9 87 0 0.103448 5 llama-2-chat:7:ggufv2:Q3_K_M 9 87 0 0.103448 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 60 0 0 5 claude-3-opus-20240229 0 36 0 0 3 code-llama-instruct:13:ggufv2:Q2_K 0 60 0 0 5 claude-3-5-sonnet-20240620 0 36 0 0 3 code-llama-instruct:13:ggufv2:Q8_0 0 60 0 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 60 0 0 5 code-llama-instruct:7:ggufv2:Q4_K_M 0 60 0 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 60 0 0 5 code-llama-instruct:7:ggufv2:Q5_K_M 0 60 0 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 60 0 0 5 code-llama-instruct:34:ggufv2:Q8_0 0 60 0 0 5 llama-2-chat:7:ggufv2:Q6_K 0 87 0 0 5 llama-2-chat:7:ggufv2:Q5_K_M 0 87 0 0 5 llama-2-chat:7:ggufv2:Q4_K_M 0 87 0 0 5 llama-2-chat:7:ggufv2:Q2_K 0 87 0 0 5 llama-2-chat:70:ggufv2:Q3_K_M 0 60 0 0 5 llama-2-chat:13:ggufv2:Q6_K 0 60 0 0 5 llama-2-chat:13:ggufv2:Q8_0 0 60 0 0 5 llama-2-chat:70:ggufv2:Q2_K 0 60 0 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 60 0 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 60 0 0 5 llama-2-chat:13:ggufv2:Q2_K 0 60 0 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 60 0 0 5 code-llama-instruct:34:ggufv2:Q5_K_M 0 60 0 0 5 code-llama-instruct:34:ggufv2:Q6_K 0 60 0 0 5 llama-3-instruct:8:ggufv2:Q8_0 0 60 0 0 5 llama-3-instruct:8:ggufv2:Q4_K_M 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 0 60 0 0 5 llama-3.1-instruct:8:ggufv2:Q6_K 0 36 0 0 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 0 36 0 0 3 llama-3.1-instruct:8:ggufv2:IQ4_XS 0 45 0 0 3 llama-3.1-instruct:70:ggufv2:IQ2_M 0 36 0 0 3 llama-3.1-instruct:70:ggufv2:IQ4_XS 0 36 0 0 3 llama-3.1-instruct:70:ggufv2:Q3_K_S 0 36 0 0 3 llama-3-instruct:8:ggufv2:Q6_K 0 60 0 0 5 llama-3-instruct:8:ggufv2:Q5_K_M 0 60 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 0 60 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 0 60 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 0 60 0 0 5 Full model name Score achieved Score possible Score SD Accuracy Iterations llama-3.1-instruct:8:ggufv2:Q8_0 129 228 0 0.565789 3 llama-3.1-instruct:8:ggufv2:Q4_K_M 117 228 0 0.513158 3 llama-3.1-instruct:8:ggufv2:Q3_K_L 111 228 0 0.486842 3 llama-3.1-instruct:8:ggufv2:Q6_K 90 192 0 0.46875 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 84 192 0 0.4375 3 gpt-4o-2024-08-06 97 228 1.1547 0.425439 3 claude-3-opus-20240229 81 192 0 0.421875 3 gpt-4o-mini-2024-07-18 129 332 0.547723 0.388554 5 llama-3.1-instruct:8:ggufv2:IQ4_XS 79 204 0 0.387255 3 gpt-4-0613 127 332 0 0.38253 5 llama-3.1-instruct:70:ggufv2:IQ4_XS 72 192 0 0.375 3 claude-3-5-sonnet-20240620 72 192 0 0.375 3 llama-3.1-instruct:70:ggufv2:Q3_K_S 72 192 0 0.375 3 gpt-3.5-turbo-0125 122 332 0 0.36747 5 gpt-3.5-turbo-0613 116 320 0 0.3625 5 llama-3.1-instruct:70:ggufv2:IQ2_M 63 192 0 0.328125 3 gpt-4-turbo-2024-04-09 108 332 0.894427 0.325301 5 chatglm3:6:ggmlv3:q4_0 92 320 0 0.2875 5 llama-3-instruct:8:ggufv2:Q8_0 90 320 0 0.28125 5 llama-3-instruct:8:ggufv2:Q6_K 90 320 0 0.28125 5 openhermes-2.5:7:ggufv2:Q8_0 70 356 0 0.196629 5 openhermes-2.5:7:ggufv2:Q5_K_M 70 356 0 0.196629 5 llama-3-instruct:8:ggufv2:Q5_K_M 60 320 0 0.1875 5 llama-2-chat:70:ggufv2:Q3_K_M 55 320 0 0.171875 5 openhermes-2.5:7:ggufv2:Q3_K_M 61 356 0 0.171348 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 52 320 0 0.1625 5 openhermes-2.5:7:ggufv2:Q6_K 45 356 0 0.126404 5 openhermes-2.5:7:ggufv2:Q4_K_M 45 356 0 0.126404 5 llama-3-instruct:8:ggufv2:Q4_K_M 35 320 0 0.109375 5 llama-2-chat:7:ggufv2:Q3_K_M 32 356 0 0.0898876 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 21 320 0 0.065625 5 code-llama-instruct:7:ggufv2:Q2_K 20 320 0 0.0625 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 15 320 0 0.046875 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 15 320 0 0.046875 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 12 320 0 0.0375 5 llama-2-chat:7:ggufv2:Q5_K_M 12 356 0 0.0337079 5 gpt-4o-2024-05-13 10 332 0 0.0301205 5 gpt-4-0125-preview 10 332 0 0.0301205 5 openhermes-2.5:7:ggufv2:Q2_K 6 356 0 0.0168539 5 code-llama-instruct:13:ggufv2:Q2_K 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q3_K_M 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q2_K 0 320 0 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 320 0 0 5 llama-2-chat:7:ggufv2:Q6_K 0 356 0 0 5 llama-2-chat:7:ggufv2:Q4_K_M 0 356 0 0 5 llama-2-chat:70:ggufv2:Q5_K_M 0 320 0 0 5 llama-2-chat:7:ggufv2:Q2_K 0 356 0 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 320 0 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 320 0 0 5 llama-2-chat:13:ggufv2:Q2_K 0 320 0 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q8_0 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q6_K 0 320 0 0 5 code-llama-instruct:34:ggufv2:Q5_K_M 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q3_K_M 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q4_K_M 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q5_K_M 0 320 0 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 320 0 0 5 llama-2-chat:70:ggufv2:Q4_K_M 0 320 0 0 5 llama-2-chat:13:ggufv2:Q6_K 0 320 0 0 5 llama-2-chat:13:ggufv2:Q8_0 0 320 0 0 5 llama-2-chat:70:ggufv2:Q2_K 0 320 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 0 320 0 0 5 llama-2-chat:7:ggufv2:Q8_0 0 356 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0 320 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0 320 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 320 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 320 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 320 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0 320 0 0 5 Full model name Score achieved Score possible Score SD Accuracy Iterations claude-3-opus-20240229 24 24 0 1 3 code-llama-instruct:34:ggufv2:Q4_K_M 39 40 0 0.975 5 code-llama-instruct:34:ggufv2:Q5_K_M 38 40 0 0.95 5 code-llama-instruct:34:ggufv2:Q8_0 37 40 0 0.925 5 llama-3.1-instruct:70:ggufv2:IQ2_M 22 24 0.57735 0.916667 3 code-llama-instruct:34:ggufv2:Q6_K 36 40 0 0.9 5 code-llama-instruct:34:ggufv2:Q3_K_M 35 40 0 0.875 5 code-llama-instruct:13:ggufv2:Q2_K 35 40 0 0.875 5 claude-3-5-sonnet-20240620 26 30 0.57735 0.866667 3 code-llama-instruct:13:ggufv2:Q3_K_M 34 40 0 0.85 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 34 40 0 0.85 5 llama-3.1-instruct:8:ggufv2:Q6_K 20 24 1.1547 0.833333 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 20 24 1.1547 0.833333 3 code-llama-instruct:13:ggufv2:Q6_K 33 40 0 0.825 5 code-llama-instruct:7:ggufv2:Q3_K_M 36 45 0 0.8 5 code-llama-instruct:7:ggufv2:Q2_K 32 40 0 0.8 5 gpt-3.5-turbo-0125 45 57 0 0.789474 5 llama-2-chat:70:ggufv2:Q5_K_M 35 45 0 0.777778 5 llama-2-chat:70:ggufv2:Q3_K_M 35 45 0 0.777778 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 35 45 0 0.777778 5 llama-2-chat:13:ggufv2:Q4_K_M 35 45 0 0.777778 5 llama-3-instruct:8:ggufv2:Q4_K_M 31 40 0 0.775 5 code-llama-instruct:7:ggufv2:Q6_K 31 40 0 0.775 5 code-llama-instruct:13:ggufv2:Q4_K_M 31 40 0 0.775 5 code-llama-instruct:13:ggufv2:Q5_K_M 31 40 0 0.775 5 llama-3-instruct:8:ggufv2:Q6_K 31 40 0 0.775 5 llama-2-chat:13:ggufv2:Q6_K 31 40 0 0.775 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 34 45 0 0.755556 5 gpt-3.5-turbo-0613 34 45 0 0.755556 5 llama-2-chat:70:ggufv2:Q4_K_M 34 45 0 0.755556 5 code-llama-instruct:34:ggufv2:Q2_K 30 40 0 0.75 5 code-llama-instruct:13:ggufv2:Q8_0 30 40 0 0.75 5 llama-3.1-instruct:70:ggufv2:IQ4_XS 18 24 0 0.75 3 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 33 45 0 0.733333 5 llama-2-chat:13:ggufv2:Q3_K_M 33 45 0 0.733333 5 llama-3-instruct:8:ggufv2:Q8_0 29 40 0 0.725 5 llama-2-chat:13:ggufv2:Q8_0 32 45 0 0.711111 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 32 45 0 0.711111 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 31 45 0 0.688889 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 31 45 0 0.688889 5 code-llama-instruct:7:ggufv2:Q5_K_M 31 45 0 0.688889 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 30 45 0 0.666667 5 code-llama-instruct:7:ggufv2:Q8_0 30 45 0 0.666667 5 gpt-4-0613 46 69 0 0.666667 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 30 45 0 0.666667 5 llama-2-chat:70:ggufv2:Q2_K 30 45 0 0.666667 5 gpt-4-turbo-2024-04-09 46 70 0 0.657143 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 26 40 0 0.65 5 llama-3-instruct:8:ggufv2:Q5_K_M 26 40 0 0.65 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 29 45 0 0.644444 5 llama-2-chat:13:ggufv2:Q5_K_M 29 45 0 0.644444 5 llama-3.1-instruct:70:ggufv2:Q3_K_S 15 24 0 0.625 3 gpt-4-0125-preview 39 63 0 0.619048 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 27 45 0 0.6 5 code-llama-instruct:7:ggufv2:Q4_K_M 27 45 0 0.6 5 gpt-4o-2024-05-13 40 76 0 0.526316 5 gpt-4o-mini-2024-07-18 43 82 0.547723 0.52439 5 llama-3.1-instruct:8:ggufv2:IQ4_XS 29 66 0 0.439394 3 llama-2-chat:7:ggufv2:Q2_K 38 117 0.57735 0.324786 5 llama-2-chat:13:ggufv2:Q2_K 13 45 0 0.288889 5 chatglm3:6:ggmlv3:q4_0 11 40 0 0.275 5 llama-2-chat:7:ggufv2:Q4_K_M 34 135 1.1547 0.251852 5 llama-3.1-instruct:8:ggufv2:Q3_K_L 36 150 2.88675 0.24 3 llama-2-chat:7:ggufv2:Q3_K_M 28 135 1.1547 0.207407 5 openhermes-2.5:7:ggufv2:Q2_K 52 279 0.57735 0.18638 5 gpt-4o-2024-08-06 34 189 0.57735 0.179894 3 llama-2-chat:7:ggufv2:Q6_K 24 135 0 0.177778 5 llama-2-chat:7:ggufv2:Q8_0 25 153 1 0.163399 5 llama-3.1-instruct:8:ggufv2:Q8_0 33 204 1.73205 0.161765 3 openhermes-2.5:7:ggufv2:Q3_K_M 53 338 0.57735 0.156805 5 llama-2-chat:7:ggufv2:Q5_K_M 21 135 0.57735 0.155556 5 llama-3.1-instruct:8:ggufv2:Q4_K_M 28 186 0.57735 0.150538 3 openhermes-2.5:7:ggufv2:Q4_K_M 52 369 0 0.140921 5 openhermes-2.5:7:ggufv2:Q5_K_M 49 405 0.57735 0.120988 5 openhermes-2.5:7:ggufv2:Q6_K 50 441 0.57735 0.113379 5 openhermes-2.5:7:ggufv2:Q8_0 48 477 1.1547 0.100629 5 Full model name Score achieved Score possible Score SD Accuracy Iterations claude-3-opus-20240229 66 90 0 0.733333 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 63 90 0 0.7 3 gpt-4-0613 118 173 0 0.682081 5 llama-3-instruct:8:ggufv2:Q4_K_M 100 150 0 0.666667 5 llama-3-instruct:8:ggufv2:Q8_0 100 150 0 0.666667 5 llama-3-instruct:8:ggufv2:Q6_K 100 150 0 0.666667 5 llama-3.1-instruct:8:ggufv2:Q4_K_M 105 159 0 0.660377 3 llama-3.1-instruct:8:ggufv2:Q8_0 105 159 0 0.660377 3 code-llama-instruct:7:ggufv2:Q4_K_M 98 150 0 0.653333 5 llama-3.1-instruct:8:ggufv2:IQ4_XS 73 113 0 0.646018 3 llama-3.1-instruct:70:ggufv2:IQ2_M 57 90 0 0.633333 3 claude-3-5-sonnet-20240620 57 90 0 0.633333 3 llama-3.1-instruct:70:ggufv2:Q3_K_S 57 90 0 0.633333 3 llama-3.1-instruct:8:ggufv2:Q6_K 57 90 0 0.633333 3 llama-3.1-instruct:8:ggufv2:Q3_K_L 99 159 0 0.622642 3 code-llama-instruct:34:ggufv2:Q3_K_M 90 150 0 0.6 5 llama-3-instruct:8:ggufv2:Q5_K_M 90 150 0 0.6 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 86 150 0 0.573333 5 code-llama-instruct:13:ggufv2:Q2_K 85 150 0 0.566667 5 code-llama-instruct:13:ggufv2:Q8_0 85 150 0 0.566667 5 code-llama-instruct:13:ggufv2:Q5_K_M 85 150 0 0.566667 5 code-llama-instruct:34:ggufv2:Q2_K 85 150 0 0.566667 5 llama-3.1-instruct:70:ggufv2:IQ4_XS 51 90 0 0.566667 3 openhermes-2.5:7:ggufv2:Q5_K_M 124 219 0 0.56621 5 openhermes-2.5:7:ggufv2:Q6_K 122 219 0 0.557078 5 code-llama-instruct:13:ggufv2:Q6_K 81 150 0 0.54 5 gpt-4o-2024-05-13 93 173 0 0.537572 5 gpt-4o-mini-2024-07-18 93 173 0 0.537572 5 code-llama-instruct:7:ggufv2:Q2_K 80 150 0 0.533333 5 code-llama-instruct:13:ggufv2:Q3_K_M 80 150 0 0.533333 5 code-llama-instruct:13:ggufv2:Q4_K_M 80 150 0 0.533333 5 gpt-4o-2024-08-06 84 159 0 0.528302 3 gpt-3.5-turbo-0125 89 173 0 0.514451 5 gpt-4-turbo-2024-04-09 88 173 0 0.508671 5 gpt-3.5-turbo-0613 75 150 0 0.5 5 openhermes-2.5:7:ggufv2:Q8_0 109 219 0 0.497717 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 72 150 0 0.48 5 chatglm3:6:ggmlv3:q4_0 72 150 0 0.48 5 llama-2-chat:13:ggufv2:Q8_0 72 150 0 0.48 5 llama-2-chat:13:ggufv2:Q3_K_M 72 150 0 0.48 5 openhermes-2.5:7:ggufv2:Q4_K_M 105 219 0.57735 0.479452 5 llama-2-chat:70:ggufv2:Q2_K 71 150 0 0.473333 5 code-llama-instruct:34:ggufv2:Q6_K 71 150 0 0.473333 5 openhermes-2.5:7:ggufv2:Q3_K_M 103 219 0 0.47032 5 code-llama-instruct:34:ggufv2:Q4_K_M 70 150 0 0.466667 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 70 150 0 0.466667 5 code-llama-instruct:34:ggufv2:Q5_K_M 70 150 0 0.466667 5 code-llama-instruct:34:ggufv2:Q8_0 70 150 0 0.466667 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 70 150 0 0.466667 5 gpt-4-0125-preview 79 173 0 0.456647 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 65 150 0 0.433333 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 65 150 0 0.433333 5 llama-2-chat:13:ggufv2:Q5_K_M 65 150 0 0.433333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 64 150 0 0.426667 5 code-llama-instruct:7:ggufv2:Q3_K_M 64 150 0 0.426667 5 openhermes-2.5:7:ggufv2:Q2_K 92 219 0 0.420091 5 llama-2-chat:70:ggufv2:Q4_K_M 63 150 0 0.42 5 llama-2-chat:70:ggufv2:Q3_K_M 62 150 0 0.413333 5 code-llama-instruct:7:ggufv2:Q8_0 60 150 0 0.4 5 code-llama-instruct:7:ggufv2:Q5_K_M 60 150 0 0.4 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 58 150 0 0.386667 5 llama-2-chat:13:ggufv2:Q6_K 58 150 0 0.386667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 57 150 0 0.38 5 llama-2-chat:13:ggufv2:Q2_K 55 150 0 0.366667 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 55 150 0 0.366667 5 llama-2-chat:13:ggufv2:Q4_K_M 55 150 0 0.366667 5 llama-2-chat:70:ggufv2:Q5_K_M 54 150 0 0.36 5 llama-2-chat:7:ggufv2:Q5_K_M 74 219 0 0.3379 5 code-llama-instruct:7:ggufv2:Q6_K 50 150 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 50 150 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 50 150 0 0.333333 5 llama-2-chat:7:ggufv2:Q6_K 64 219 0 0.292237 5 llama-2-chat:7:ggufv2:Q8_0 64 219 0 0.292237 5 llama-2-chat:7:ggufv2:Q4_K_M 60 219 0 0.273973 5 llama-2-chat:7:ggufv2:Q3_K_M 50 219 0 0.228311 5 llama-2-chat:7:ggufv2:Q2_K 36 219 0 0.164384 5 Full model name Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 87 90 0 0.966667 3 code-llama-instruct:7:ggufv2:Q4_K_M 145 150 0 0.966667 5 llama-3.1-instruct:70:ggufv2:Q3_K_S 87 90 0 0.966667 3 code-llama-instruct:7:ggufv2:Q6_K 144 150 0 0.96 5 code-llama-instruct:7:ggufv2:Q5_K_M 144 150 0 0.96 5 code-llama-instruct:7:ggufv2:Q8_0 144 150 0 0.96 5 gpt-4-0613 166 173 0 0.959538 5 llama-3.1-instruct:70:ggufv2:IQ2_M 86 90 1.1547 0.955556 3 llama-3.1-instruct:70:ggufv2:IQ4_XS 86 90 1.1547 0.955556 3 llama-3.1-instruct:8:ggufv2:Q6_K 86 90 0.57735 0.955556 3 gpt-3.5-turbo-0125 165 173 0 0.953757 5 gpt-4o-mini-2024-07-18 165 173 0 0.953757 5 llama-3.1-instruct:8:ggufv2:IQ4_XS 107 113 0.57735 0.946903 3 gpt-3.5-turbo-0613 142 150 0 0.946667 5 claude-3-opus-20240229 85 90 0.57735 0.944444 3 llama-3.1-instruct:8:ggufv2:Q3_K_L 150 159 0 0.943396 3 llama-3.1-instruct:8:ggufv2:Q8_0 149 159 0.57735 0.937107 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 84 90 0 0.933333 3 llama-3-instruct:8:ggufv2:Q5_K_M 139 150 0 0.926667 5 llama-3-instruct:8:ggufv2:Q6_K 139 150 0 0.926667 5 llama-3.1-instruct:8:ggufv2:Q4_K_M 147 159 0 0.924528 3 code-llama-instruct:7:ggufv2:Q2_K 138 150 0 0.92 5 llama-3-instruct:8:ggufv2:Q4_K_M 138 150 0 0.92 5 llama-2-chat:70:ggufv2:Q4_K_M 138 150 0 0.92 5 llama-3-instruct:8:ggufv2:Q8_0 138 150 0 0.92 5 openhermes-2.5:7:ggufv2:Q5_K_M 201 219 0.57735 0.917808 5 openhermes-2.5:7:ggufv2:Q2_K 201 219 0 0.917808 5 openhermes-2.5:7:ggufv2:Q3_K_M 201 219 3 0.917808 5 llama-2-chat:70:ggufv2:Q5_K_M 136 150 0 0.906667 5 code-llama-instruct:34:ggufv2:Q4_K_M 136 150 0 0.906667 5 llama-2-chat:70:ggufv2:Q3_K_M 136 150 0 0.906667 5 openhermes-2.5:7:ggufv2:Q8_0 198 219 0 0.90411 5 llama-2-chat:70:ggufv2:Q2_K 135 150 0 0.9 5 code-llama-instruct:34:ggufv2:Q5_K_M 135 150 0 0.9 5 openhermes-2.5:7:ggufv2:Q4_K_M 196 219 0.57735 0.894977 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 134 150 0 0.893333 5 openhermes-2.5:7:ggufv2:Q6_K 195 219 0 0.890411 5 gpt-4o-2024-08-06 139 159 2.3094 0.874214 3 code-llama-instruct:7:ggufv2:Q3_K_M 131 150 0 0.873333 5 code-llama-instruct:34:ggufv2:Q8_0 129 150 0 0.86 5 code-llama-instruct:34:ggufv2:Q6_K 128 150 0 0.853333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 127 150 0 0.846667 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 127 150 0 0.846667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 126 150 0 0.84 5 gpt-4-0125-preview 145 173 0 0.83815 5 code-llama-instruct:13:ggufv2:Q3_K_M 125 150 0 0.833333 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 125 150 0 0.833333 5 code-llama-instruct:13:ggufv2:Q4_K_M 125 150 0 0.833333 5 gpt-4-turbo-2024-04-09 144 173 0.447214 0.83237 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 124 150 0 0.826667 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 124 150 0 0.826667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 124 150 0 0.826667 5 code-llama-instruct:13:ggufv2:Q2_K 123 150 0 0.82 5 llama-2-chat:13:ggufv2:Q6_K 122 150 0 0.813333 5 gpt-4o-2024-05-13 140 173 0 0.809249 5 code-llama-instruct:13:ggufv2:Q6_K 119 150 0 0.793333 5 code-llama-instruct:34:ggufv2:Q3_K_M 118 150 0 0.786667 5 llama-2-chat:13:ggufv2:Q8_0 118 150 0 0.786667 5 code-llama-instruct:13:ggufv2:Q5_K_M 117 150 0 0.78 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 116 150 0 0.773333 5 code-llama-instruct:13:ggufv2:Q8_0 115 150 0 0.766667 5 llama-2-chat:13:ggufv2:Q4_K_M 114 150 0 0.76 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 114 150 0 0.76 5 llama-2-chat:13:ggufv2:Q5_K_M 112 150 0 0.746667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 109 150 0 0.726667 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 104 150 0 0.693333 5 code-llama-instruct:34:ggufv2:Q2_K 103 150 0 0.686667 5 llama-2-chat:13:ggufv2:Q3_K_M 102 150 0 0.68 5 llama-2-chat:7:ggufv2:Q2_K 134 219 1.1547 0.611872 5 llama-2-chat:7:ggufv2:Q4_K_M 134 219 2.68223 0.611872 5 llama-2-chat:7:ggufv2:Q8_0 129 219 0 0.589041 5 llama-2-chat:7:ggufv2:Q3_K_M 129 219 1.1547 0.589041 5 llama-2-chat:7:ggufv2:Q6_K 123 219 0 0.561644 5 chatglm3:6:ggmlv3:q4_0 83 150 0 0.553333 5 llama-2-chat:7:ggufv2:Q5_K_M 120 219 0.57735 0.547945 5 llama-2-chat:13:ggufv2:Q2_K 65 150 0 0.433333 5 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-3.5-turbo-0125 159 173 0 0.919075 5 gpt-4-0613 152 173 0 0.878613 5 gpt-3.5-turbo-0613 125 150 0 0.833333 5 gpt-4o-2024-08-06 132 159 1.1547 0.830189 3 llama-3.1-instruct:8:ggufv2:Q3_K_L 129 159 0 0.811321 3 llama-3.1-instruct:8:ggufv2:Q8_0 123 159 0 0.773585 3 llama-3.1-instruct:8:ggufv2:IQ4_XS 84 113 0 0.743363 3 llama-3.1-instruct:8:ggufv2:Q4_K_M 117 159 0 0.735849 3 claude-3-5-sonnet-20240620 66 90 0 0.733333 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 66 90 0 0.733333 3 llama-3.1-instruct:8:ggufv2:Q6_K 66 90 0 0.733333 3 gpt-4o-mini-2024-07-18 119 173 1.54266 0.687861 5 claude-3-opus-20240229 59 90 3.21455 0.655556 3 gpt-4-turbo-2024-04-09 110 173 0 0.635838 5 llama-3.1-instruct:70:ggufv2:Q3_K_S 54 90 3.4641 0.6 3 llama-3.1-instruct:70:ggufv2:IQ4_XS 54 90 0 0.6 3 llama-3.1-instruct:70:ggufv2:IQ2_M 54 90 0 0.6 3 openhermes-2.5:7:ggufv2:Q3_K_M 63 219 1 0.287671 5 openhermes-2.5:7:ggufv2:Q6_K 60 219 1.73205 0.273973 5 openhermes-2.5:7:ggufv2:Q5_K_M 58 219 0.57735 0.26484 5 openhermes-2.5:7:ggufv2:Q4_K_M 54 219 1.1547 0.246575 5 openhermes-2.5:7:ggufv2:Q8_0 52 219 1.1547 0.237443 5 openhermes-2.5:7:ggufv2:Q2_K 35 219 2.3094 0.159817 5 gpt-4o-2024-05-13 20 173 0 0.115607 5 gpt-4-0125-preview 19 173 0 0.109827 5 chatglm3:6:ggmlv3:q4_0 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q2_K 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q4_K_M 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q3_K_M 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q2_K 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q8_0 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q6_K 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q5_K_M 0 150 0 0 5 code-llama-instruct:13:ggufv2:Q4_K_M 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q8_0 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q6_K 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q5_K_M 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q4_K_M 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q3_K_M 0 150 0 0 5 code-llama-instruct:7:ggufv2:Q2_K 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q8_0 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q6_K 0 150 0 0 5 code-llama-instruct:34:ggufv2:Q5_K_M 0 150 0 0 5 llama-2-chat:7:ggufv2:Q6_K 0 219 0 0 5 llama-2-chat:7:ggufv2:Q5_K_M 0 219 0 0 5 llama-2-chat:7:ggufv2:Q4_K_M 0 219 0 0 5 llama-2-chat:7:ggufv2:Q3_K_M 0 219 0 0 5 llama-2-chat:7:ggufv2:Q2_K 0 219 0 0 5 llama-2-chat:70:ggufv2:Q5_K_M 0 150 0 0 5 llama-2-chat:70:ggufv2:Q4_K_M 0 150 0 0 5 llama-2-chat:70:ggufv2:Q3_K_M 0 150 0 0 5 llama-2-chat:70:ggufv2:Q2_K 0 150 0 0 5 llama-2-chat:13:ggufv2:Q8_0 0 150 0 0 5 llama-2-chat:13:ggufv2:Q6_K 0 150 0 0 5 llama-2-chat:13:ggufv2:Q5_K_M 0 150 0 0 5 llama-2-chat:13:ggufv2:Q4_K_M 0 150 0 0 5 llama-2-chat:13:ggufv2:Q3_K_M 0 150 0 0 5 llama-2-chat:13:ggufv2:Q2_K 0 150 0 0 5 llama-3-instruct:8:ggufv2:Q8_0 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 0 150 0 0 5 llama-3-instruct:8:ggufv2:Q4_K_M 0 150 0 0 5 llama-2-chat:7:ggufv2:Q8_0 0 219 0 0 5 llama-3-instruct:8:ggufv2:Q6_K 0 150 0 0 5 llama-3-instruct:8:ggufv2:Q5_K_M 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 0 150 0 0 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 150 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 0 150 0 0 5"},{"location":"benchmark/results/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":"<p>In this set of tasks, we test LLM abilities to generate answers to a given question using a RAG agent, or to judge the relevance of a RAG fragment to a given question. Instructions can be explicit (\"is this fragment relevant to the question?\") or implicit (just asking the question without instructions and evaluating whether the model responds with 'not enough information given').</p> Explicit relevance of single fragmentsImplicit relevance of single fragments Full model name Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 18 18 0 1 3 llama-2-chat:13:ggufv2:Q6_K 30 30 0 1 5 llama-2-chat:70:ggufv2:Q2_K 30 30 0 1 5 gpt-4o-2024-08-06 18 18 0 1 3 code-llama-instruct:7:ggufv2:Q8_0 30 30 0 1 5 gpt-3.5-turbo-0125 30 30 0 1 5 gpt-3.5-turbo-0613 30 30 0 1 5 gpt-4-0125-preview 30 30 0 1 5 gpt-4-0613 30 30 0 1 5 gpt-4-turbo-2024-04-09 30 30 0 1 5 gpt-4o-2024-05-13 30 30 0 1 5 code-llama-instruct:7:ggufv2:Q4_K_M 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 30 30 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q4_K_M 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q5_K_M 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q2_K 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q3_K_M 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q6_K 30 30 0 1 5 llama-3.1-instruct:8:ggufv2:Q3_K_L 18 18 0 1 3 llama-3.1-instruct:8:ggufv2:Q4_K_M 18 18 0 1 3 llama-2-chat:13:ggufv2:Q8_0 30 30 0 1 5 llama-2-chat:13:ggufv2:Q5_K_M 30 30 0 1 5 llama-2-chat:13:ggufv2:Q4_K_M 30 30 0 1 5 llama-2-chat:13:ggufv2:Q3_K_M 30 30 0 1 5 llama-2-chat:13:ggufv2:Q2_K 30 30 0 1 5 llama-2-chat:7:ggufv2:Q5_K_M 30 30 0 1 5 llama-2-chat:70:ggufv2:Q4_K_M 30 30 0 1 5 llama-2-chat:70:ggufv2:Q5_K_M 30 30 0 1 5 llama-2-chat:7:ggufv2:Q3_K_M 30 30 0 1 5 llama-2-chat:7:ggufv2:Q6_K 30 30 0 1 5 llama-2-chat:7:ggufv2:Q4_K_M 30 30 0 1 5 llama-2-chat:70:ggufv2:Q3_K_M 30 30 0 1 5 llama-3.1-instruct:8:ggufv2:IQ4_XS 18 18 0 1 3 llama-3.1-instruct:70:ggufv2:Q3_K_S 18 18 0 1 3 llama-3.1-instruct:70:ggufv2:IQ4_XS 18 18 0 1 3 llama-3.1-instruct:70:ggufv2:IQ2_M 18 18 0 1 3 llama-3-instruct:8:ggufv2:Q8_0 30 30 0 1 5 llama-3-instruct:8:ggufv2:Q6_K 30 30 0 1 5 llama-3-instruct:8:ggufv2:Q5_K_M 30 30 0 1 5 llama-3-instruct:8:ggufv2:Q4_K_M 30 30 0 1 5 llama-2-chat:7:ggufv2:Q8_0 30 30 0 1 5 openhermes-2.5:7:ggufv2:Q8_0 30 30 0 1 5 llama-3.1-instruct:8:ggufv2:Q8_0 18 18 0 1 3 mistral-instruct-v0.2:7:ggufv2:Q2_K 30 30 0 1 5 llama-3.1-instruct:8:ggufv2:Q5_K_M 18 18 0 1 3 llama-3.1-instruct:8:ggufv2:Q6_K 18 18 0 1 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 30 30 0 1 5 code-llama-instruct:7:ggufv2:Q6_K 25 30 0 0.833333 5 code-llama-instruct:7:ggufv2:Q5_K_M 25 30 0 0.833333 5 code-llama-instruct:13:ggufv2:Q6_K 25 30 0 0.833333 5 code-llama-instruct:13:ggufv2:Q8_0 25 30 0 0.833333 5 claude-3-opus-20240229 15 18 0 0.833333 3 code-llama-instruct:7:ggufv2:Q3_K_M 25 30 0 0.833333 5 llama-2-chat:7:ggufv2:Q2_K 25 30 0 0.833333 5 gpt-4o-mini-2024-07-18 25 30 0 0.833333 5 chatglm3:6:ggmlv3:q4_0 22 30 0 0.733333 5 code-llama-instruct:13:ggufv2:Q5_K_M 20 30 0 0.666667 5 code-llama-instruct:34:ggufv2:Q2_K 15 30 0 0.5 5 code-llama-instruct:34:ggufv2:Q3_K_M 15 30 0 0.5 5 code-llama-instruct:34:ggufv2:Q4_K_M 15 30 0 0.5 5 code-llama-instruct:13:ggufv2:Q4_K_M 10 30 0 0.333333 5 code-llama-instruct:34:ggufv2:Q5_K_M 10 30 0 0.333333 5 code-llama-instruct:34:ggufv2:Q6_K 10 30 0 0.333333 5 code-llama-instruct:34:ggufv2:Q8_0 10 30 0 0.333333 5 code-llama-instruct:7:ggufv2:Q2_K 10 30 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 10 30 0 0.333333 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 5 30 0 0.166667 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 4 30 0 0.133333 5 code-llama-instruct:13:ggufv2:Q2_K 1 30 0 0.0333333 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 30 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 0 30 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 0 30 0 0 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 0 30 0 0 5 Full model name Score achieved Score possible Score SD Accuracy Iterations chatglm3:6:ggmlv3:q4_0 10 10 0 1 5 claude-3-5-sonnet-20240620 6 6 0 1 3 claude-3-opus-20240229 6 6 0 1 3 code-llama-instruct:34:ggufv2:Q2_K 10 10 0 1 5 llama-2-chat:7:ggufv2:Q2_K 10 10 0 1 5 llama-2-chat:7:ggufv2:Q3_K_M 10 10 0 1 5 llama-2-chat:70:ggufv2:Q4_K_M 10 10 0 1 5 gpt-4-turbo-2024-04-09 10 10 0 1 5 gpt-3.5-turbo-0613 10 10 0 1 5 gpt-4-0613 10 10 0 1 5 code-llama-instruct:7:ggufv2:Q4_K_M 10 10 0 1 5 code-llama-instruct:34:ggufv2:Q5_K_M 10 10 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 10 10 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 10 10 0 1 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 10 10 0 1 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 10 10 0 1 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 10 10 0 1 5 openhermes-2.5:7:ggufv2:Q5_K_M 10 10 0 1 5 openhermes-2.5:7:ggufv2:Q6_K 10 10 0 1 5 llama-3.1-instruct:8:ggufv2:Q8_0 6 6 0 1 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 10 10 0 1 5 llama-3.1-instruct:70:ggufv2:Q3_K_S 6 6 0 1 3 llama-3.1-instruct:70:ggufv2:IQ2_M 6 6 0 1 3 llama-3.1-instruct:70:ggufv2:IQ4_XS 6 6 0 1 3 llama-3-instruct:8:ggufv2:Q8_0 10 10 0 1 5 llama-3-instruct:8:ggufv2:Q6_K 10 10 0 1 5 openhermes-2.5:7:ggufv2:Q8_0 10 10 0 1 5 openhermes-2.5:7:ggufv2:Q4_K_M 10 10 0 1 5 llama-3-instruct:8:ggufv2:Q5_K_M 10 10 0 1 5 llama-3-instruct:8:ggufv2:Q4_K_M 10 10 0 1 5 gpt-3.5-turbo-0125 9 10 0 0.9 5 code-llama-instruct:7:ggufv2:Q6_K 9 10 0 0.9 5 llama-2-chat:70:ggufv2:Q5_K_M 9 10 0 0.9 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 9 10 0 0.9 5 code-llama-instruct:34:ggufv2:Q8_0 9 10 0 0.9 5 code-llama-instruct:34:ggufv2:Q6_K 9 10 0 0.9 5 llama-3.1-instruct:8:ggufv2:Q3_K_L 5 6 0.57735 0.833333 3 llama-3.1-instruct:8:ggufv2:IQ4_XS 5 6 0.57735 0.833333 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 5 6 0.57735 0.833333 3 llama-3.1-instruct:8:ggufv2:Q4_K_M 5 6 0.57735 0.833333 3 llama-3.1-instruct:8:ggufv2:Q6_K 5 6 0.57735 0.833333 3 code-llama-instruct:7:ggufv2:Q3_K_M 7 10 0 0.7 5 gpt-4o-2024-05-13 7 10 0 0.7 5 code-llama-instruct:7:ggufv2:Q2_K 7 10 0 0.7 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 7 10 0 0.7 5 gpt-4o-2024-08-06 4 6 0.57735 0.666667 3 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 6 10 0 0.6 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 6 10 0 0.6 5 llama-2-chat:7:ggufv2:Q5_K_M 6 10 0 0.6 5 code-llama-instruct:13:ggufv2:Q6_K 5 10 0 0.5 5 code-llama-instruct:13:ggufv2:Q4_K_M 5 10 0 0.5 5 code-llama-instruct:34:ggufv2:Q3_K_M 5 10 0 0.5 5 llama-2-chat:7:ggufv2:Q6_K 5 10 0 0.5 5 llama-2-chat:7:ggufv2:Q4_K_M 5 10 0 0.5 5 llama-2-chat:70:ggufv2:Q3_K_M 5 10 0 0.5 5 code-llama-instruct:13:ggufv2:Q8_0 5 10 0 0.5 5 code-llama-instruct:13:ggufv2:Q5_K_M 5 10 0 0.5 5 code-llama-instruct:7:ggufv2:Q5_K_M 5 10 0 0.5 5 gpt-4-0125-preview 5 10 0 0.5 5 code-llama-instruct:7:ggufv2:Q8_0 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q2_K 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q3_K_M 5 10 0 0.5 5 gpt-4o-mini-2024-07-18 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q4_K_M 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q5_K_M 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q6_K 5 10 0 0.5 5 llama-2-chat:13:ggufv2:Q8_0 5 10 0 0.5 5 llama-2-chat:70:ggufv2:Q2_K 5 10 0 0.5 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 5 10 0 0.5 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 5 10 0 0.5 5 openhermes-2.5:7:ggufv2:Q3_K_M 5 10 0 0.5 5 llama-2-chat:7:ggufv2:Q8_0 5 10 0 0.5 5 openhermes-2.5:7:ggufv2:Q2_K 5 10 0 0.5 5 code-llama-instruct:13:ggufv2:Q2_K 4 10 0 0.4 5 code-llama-instruct:34:ggufv2:Q4_K_M 4 10 0 0.4 5 code-llama-instruct:13:ggufv2:Q3_K_M 0 10 0 0 5"},{"location":"benchmark/results/#text-extraction","title":"Text Extraction","text":"<p>In this set of tasks, we test LLM abilities to extract text from a given document.</p> Overall PerformanceAssay extractionChemical extractionContext extractionDisease extractionEntity extractionExperiment extraction (yes or no)Hypothesis extractionIntervention extractionGene identifier mappingSignificance extractionStatistics extractionImage caption yes/no Full model name Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 224.558 297 0.0766981 0.756088 3 gpt-4o-2024-08-06 211.222 297 1.31903 0.711185 3 llama-3.1-instruct:70:ggufv2:IQ4_XS 207.674 297 1.29175e-15 0.699238 3 claude-3-opus-20240229 205.297 297 0.441227 0.691235 3 gpt-4-0125-preview 341.404 495 0 0.689705 5 gpt-4o-mini-2024-07-18 338.854 495 1.74807 0.684553 5 gpt-4-0613 331.107 495 0 0.668903 5 gpt-4o-2024-05-13 323.703 495 0 0.653946 5 gpt-4-turbo-2024-04-09 321.933 495 4.39682 0.650369 5 llama-3.1-instruct:70:ggufv2:Q3_K_S 190.774 297 9.26323e-16 0.642336 3 llama-3.1-instruct:70:ggufv2:IQ2_M 186.07 297 8.83831e-16 0.626498 3 openhermes-2.5:7:ggufv2:Q6_K 306.488 495 0 0.619167 5 openhermes-2.5:7:ggufv2:Q8_0 297.41 495 0 0.600829 5 openhermes-2.5:7:ggufv2:Q4_K_M 295.654 495 0 0.597281 5 openhermes-2.5:7:ggufv2:Q5_K_M 287.059 495 0 0.579916 5 gpt-3.5-turbo-0613 284.814 495 0 0.575381 5 openhermes-2.5:7:ggufv2:Q3_K_M 274.471 495 0 0.554488 5 gpt-3.5-turbo-0125 252.466 495 0 0.510032 5 openhermes-2.5:7:ggufv2:Q2_K 219.807 495 0 0.444054 5 llama-3.1-instruct:8:ggufv2:IQ4_XS 123.142 297 5.69391e-16 0.414621 3 llama-3.1-instruct:8:ggufv2:Q6_K 117.157 297 3.73928e-16 0.394469 3 llama-3.1-instruct:8:ggufv2:Q8_0 115.554 297 4.18545e-16 0.38907 3 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 190.948 495 0 0.385754 5 llama-3.1-instruct:8:ggufv2:Q4_K_M 113.462 297 0.300229 0.382027 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 113.002 297 0.00031952 0.380477 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 182.642 495 0 0.368974 5 mistral-instruct-v0.2:7:ggufv2:Q6_K 181.869 495 0 0.367412 5 llama-3.1-instruct:8:ggufv2:Q3_K_L 107.033 297 5.50801e-16 0.360379 3 mistral-instruct-v0.2:7:ggufv2:Q8_0 174.084 495 0 0.351684 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 171.777 495 0 0.347025 5 mistral-instruct-v0.2:7:ggufv2:Q2_K 163.974 495 0 0.331261 5 llama-2-chat:70:ggufv2:Q4_K_M 119.263 495 0 0.240936 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 116.651 495 0 0.235659 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M 113.663 495 0 0.229622 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 111.634 495 0 0.225524 5 llama-2-chat:70:ggufv2:Q2_K 106.448 495 0 0.215047 5 llama-2-chat:70:ggufv2:Q5_K_M 104.032 495 0 0.210166 5 llama-2-chat:70:ggufv2:Q3_K_M 97.9593 495 0 0.197898 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 95.9243 495 0 0.193786 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 93.6428 495 0 0.189177 5 llama-3-instruct:8:ggufv2:Q8_0 93.3345 495 0 0.188555 5 chatglm3:6:ggmlv3:q4_0 93.2008 495 0 0.188284 5 llama-3-instruct:8:ggufv2:Q5_K_M 82.3847 495 0 0.166434 5 llama-3-instruct:8:ggufv2:Q6_K 80.5152 495 0 0.162657 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 77.9693 495 0 0.157514 5 code-llama-instruct:7:ggufv2:Q4_K_M 68.6724 495 0 0.138732 5 llama-3-instruct:8:ggufv2:Q4_K_M 57.8514 495 0 0.116871 5 llama-2-chat:13:ggufv2:Q3_K_M 55.7521 495 0 0.112631 5 llama-2-chat:13:ggufv2:Q4_K_M 43.9894 495 0 0.0888675 5 llama-2-chat:7:ggufv2:Q4_K_M 42.1985 495 0 0.0852494 5 llama-2-chat:7:ggufv2:Q8_0 25.1647 297 1.46597e-16 0.0847297 3 llama-2-chat:13:ggufv2:Q6_K 23.2057 297 0.00246731 0.0781337 3 llama-2-chat:13:ggufv2:Q5_K_M 37.9252 495 0 0.0766167 5 llama-2-chat:13:ggufv2:Q8_0 37.7416 495 0 0.0762457 5 llama-2-chat:7:ggufv2:Q5_K_M 34.5308 495 0 0.0697591 5 llama-2-chat:7:ggufv2:Q3_K_M 32.2105 495 0 0.0650717 5 llama-2-chat:13:ggufv2:Q2_K 32.1447 495 0 0.0649389 5 llama-2-chat:7:ggufv2:Q6_K 18.2539 297 2.57076e-16 0.0614608 3 llama-2-chat:7:ggufv2:Q2_K 17.9123 495 0 0.0361865 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 assay 23.4242 27 0 0.867565 3 llama-3.1-instruct:70:ggufv2:IQ2_M assay 22.1667 27 0 0.820988 3 llama-3.1-instruct:70:ggufv2:Q3_K_S assay 22.1667 27 0 0.820988 3 claude-3-opus-20240229 assay 21.4909 27 1.51082e-17 0.79596 3 gpt-4o-2024-08-06 assay 20.3897 27 3.02164e-17 0.755176 3 llama-3.1-instruct:70:ggufv2:IQ4_XS assay 20.3238 27 3.02164e-17 0.752734 3 gpt-4o-mini-2024-07-18 assay 33.0217 45 0.00559644 0.733816 5 gpt-4-turbo-2024-04-09 assay 26.4233 45 0.0114637 0.587184 5 llama-3.1-instruct:8:ggufv2:Q3_K_L assay 12.9241 27 1.60525e-17 0.478671 3 llama-3.1-instruct:8:ggufv2:IQ4_XS assay 12.4373 27 8.49837e-18 0.460639 3 llama-3.1-instruct:8:ggufv2:Q4_K_M assay 12.4264 27 7.55411e-18 0.460235 3 llama-3.1-instruct:8:ggufv2:Q6_K assay 12.1618 27 8.49837e-18 0.450438 3 llama-3.1-instruct:8:ggufv2:Q5_K_M assay 11.2618 27 0 0.417103 3 llama-3.1-instruct:8:ggufv2:Q8_0 assay 10.4603 27 1.51082e-17 0.387417 3 gpt-4o-2024-05-13 assay 6.67307 45 0 0.148291 5 gpt-4-0125-preview assay 6.60264 45 0 0.146725 5 openhermes-2.5:7:ggufv2:Q6_K assay 6.45354 45 0 0.143412 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M assay 6.42156 45 0 0.142701 5 openhermes-2.5:7:ggufv2:Q8_0 assay 6.24141 45 0 0.138698 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 assay 5.8662 45 0 0.13036 5 mistral-instruct-v0.2:7:ggufv2:Q2_K assay 5.84165 45 0 0.129814 5 mistral-instruct-v0.2:7:ggufv2:Q6_K assay 5.83272 45 0 0.129616 5 openhermes-2.5:7:ggufv2:Q5_K_M assay 5.77475 45 0 0.128328 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M assay 5.72421 45 0 0.127205 5 gpt-3.5-turbo-0613 assay 5.71717 45 0 0.127048 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M assay 5.66084 45 0 0.125797 5 gpt-3.5-turbo-0125 assay 5.48324 45 0 0.12185 5 gpt-4-0613 assay 5.47238 45 0 0.121608 5 openhermes-2.5:7:ggufv2:Q4_K_M assay 5.40473 45 0 0.120105 5 openhermes-2.5:7:ggufv2:Q3_K_M assay 4.99329 45 0 0.110962 5 openhermes-2.5:7:ggufv2:Q2_K assay 4.35689 45 0 0.0968198 5 llama-2-chat:7:ggufv2:Q6_K assay 2.34166 27 7.55411e-18 0.0867281 3 llama-2-chat:13:ggufv2:Q6_K assay 2.19772 27 3.77706e-18 0.081397 3 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M assay 3.17543 45 0 0.070565 5 llama-2-chat:7:ggufv2:Q8_0 assay 1.62311 27 0 0.0601152 3 llama-2-chat:70:ggufv2:Q4_K_M assay 1.8509 45 0 0.041131 5 llama-2-chat:70:ggufv2:Q5_K_M assay 1.81844 45 0 0.0404097 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K assay 1.68419 45 0 0.0374265 5 chatglm3:6:ggmlv3:q4_0 assay 1.61672 45 0 0.0359271 5 code-llama-instruct:7:ggufv2:Q4_K_M assay 1.53778 45 0 0.0341728 5 llama-3-instruct:8:ggufv2:Q6_K assay 1.48103 45 0 0.0329118 5 llama-3-instruct:8:ggufv2:Q8_0 assay 1.37088 45 0 0.0304641 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 assay 1.16327 45 0 0.0258505 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M assay 1.15926 45 0 0.0257612 5 llama-2-chat:70:ggufv2:Q2_K assay 1.15095 45 0 0.0255768 5 llama-2-chat:70:ggufv2:Q3_K_M assay 1.07788 45 0 0.023953 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M assay 1.05347 45 0 0.0234104 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K assay 1.02909 45 0 0.0228686 5 llama-2-chat:13:ggufv2:Q2_K assay 0.974441 45 0 0.0216542 5 llama-3-instruct:8:ggufv2:Q5_K_M assay 0.922706 45 0 0.0205046 5 llama-2-chat:7:ggufv2:Q5_K_M assay 0.919259 45 0 0.020428 5 llama-2-chat:13:ggufv2:Q5_K_M assay 0.836349 45 0 0.0185855 5 llama-2-chat:13:ggufv2:Q8_0 assay 0.756302 45 0 0.0168067 5 llama-2-chat:13:ggufv2:Q3_K_M assay 0.750557 45 0 0.016679 5 llama-2-chat:13:ggufv2:Q4_K_M assay 0.647223 45 0 0.0143827 5 llama-2-chat:7:ggufv2:Q4_K_M assay 0.604799 45 0 0.01344 5 llama-3-instruct:8:ggufv2:Q4_K_M assay 0.522273 45 0 0.0116061 5 llama-2-chat:7:ggufv2:Q3_K_M assay 0.455699 45 0 0.0101266 5 llama-2-chat:7:ggufv2:Q2_K assay 0.233824 45 0 0.00519608 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-opus-20240229 chemical 24 27 0 0.888889 3 claude-3-5-sonnet-20240620 chemical 21.6667 27 0 0.802469 3 llama-3.1-instruct:70:ggufv2:IQ4_XS chemical 20 27 0 0.740741 3 gpt-4o-2024-08-06 chemical 18.6667 27 0 0.691358 3 llama-3.1-instruct:70:ggufv2:Q3_K_S chemical 18 27 0 0.666667 3 llama-3.1-instruct:70:ggufv2:IQ2_M chemical 18 27 0 0.666667 3 gpt-4-turbo-2024-04-09 chemical 29.188 45 0 0.648623 5 gpt-4o-mini-2024-07-18 chemical 27.7778 45 0 0.617284 5 llama-3.1-instruct:8:ggufv2:IQ4_XS chemical 12.3451 27 0 0.457227 3 llama-3.1-instruct:8:ggufv2:Q4_K_M chemical 12.0531 27 0 0.446411 3 llama-3.1-instruct:8:ggufv2:Q6_K chemical 11.0168 27 0 0.408029 3 llama-3.1-instruct:8:ggufv2:Q3_K_L chemical 10.8547 27 2.36066e-19 0.402026 3 llama-3.1-instruct:8:ggufv2:Q8_0 chemical 9.13698 27 0 0.338407 3 llama-3.1-instruct:8:ggufv2:Q5_K_M chemical 8.35802 27 0 0.309556 3 gpt-4-0613 chemical 6.38889 45 0 0.141975 5 gpt-4-0125-preview chemical 6.22222 45 0 0.138272 5 openhermes-2.5:7:ggufv2:Q6_K chemical 6.16667 45 0 0.137037 5 gpt-4o-2024-05-13 chemical 5.55556 45 0 0.123457 5 gpt-3.5-turbo-0613 chemical 5.44444 45 0 0.120988 5 openhermes-2.5:7:ggufv2:Q3_K_M chemical 5.23309 45 0 0.116291 5 openhermes-2.5:7:ggufv2:Q8_0 chemical 5.16667 45 0 0.114815 5 openhermes-2.5:7:ggufv2:Q5_K_M chemical 5.06667 45 0 0.112593 5 gpt-3.5-turbo-0125 chemical 5.06444 45 0 0.112543 5 openhermes-2.5:7:ggufv2:Q4_K_M chemical 4.95556 45 0 0.110123 5 openhermes-2.5:7:ggufv2:Q2_K chemical 4.66667 45 0 0.103704 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M chemical 4.02332 45 0 0.0894072 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M chemical 3.69824 45 0 0.0821832 5 mistral-instruct-v0.2:7:ggufv2:Q6_K chemical 3.5588 45 0 0.0790845 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M chemical 3.23175 45 0 0.0718166 5 mistral-instruct-v0.2:7:ggufv2:Q2_K chemical 2.9648 45 0 0.0658845 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M chemical 2.85926 45 0 0.0635392 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 chemical 2.80214 45 0 0.0622698 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K chemical 2.28839 45 0 0.050853 5 llama-2-chat:13:ggufv2:Q6_K chemical 1.33748 27 0 0.0495362 3 llama-3-instruct:8:ggufv2:Q6_K chemical 1.99259 45 0 0.0442798 5 llama-3-instruct:8:ggufv2:Q5_K_M chemical 1.98451 45 0 0.0441003 5 llama-3-instruct:8:ggufv2:Q8_0 chemical 1.98451 45 0 0.0441003 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M chemical 1.92687 45 0 0.0428194 5 llama-2-chat:70:ggufv2:Q2_K chemical 1.92403 45 0 0.0427562 5 llama-2-chat:70:ggufv2:Q4_K_M chemical 1.86594 45 0 0.0414653 5 llama-2-chat:7:ggufv2:Q8_0 chemical 1.11429 27 3.77706e-18 0.0412698 3 llama-2-chat:70:ggufv2:Q5_K_M chemical 1.7972 45 0 0.0399378 5 llama-2-chat:70:ggufv2:Q3_K_M chemical 1.65417 45 0 0.0367593 5 llama-2-chat:13:ggufv2:Q4_K_M chemical 1.60885 45 0 0.0357522 5 llama-2-chat:7:ggufv2:Q6_K chemical 0.85 27 1.88853e-18 0.0314815 3 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K chemical 1.37178 45 0 0.030484 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 chemical 1.02473 45 0 0.0227718 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M chemical 0.993896 45 0 0.0220866 5 llama-3-instruct:8:ggufv2:Q4_K_M chemical 0.920791 45 0 0.020462 5 chatglm3:6:ggmlv3:q4_0 chemical 0.839293 45 0 0.018651 5 llama-2-chat:7:ggufv2:Q5_K_M chemical 0.580952 45 0 0.0129101 5 llama-2-chat:13:ggufv2:Q5_K_M chemical 0.473978 45 0 0.0105328 5 llama-2-chat:13:ggufv2:Q8_0 chemical 0.473978 45 0 0.0105328 5 llama-2-chat:13:ggufv2:Q3_K_M chemical 0.447004 45 0 0.00993343 5 code-llama-instruct:7:ggufv2:Q4_K_M chemical 0.44189 45 0 0.00981978 5 llama-2-chat:13:ggufv2:Q2_K chemical 0.429118 45 0 0.00953595 5 llama-2-chat:7:ggufv2:Q4_K_M chemical 0.416702 45 0 0.00926004 5 llama-2-chat:7:ggufv2:Q3_K_M chemical 0.270151 45 0 0.00600336 5 llama-2-chat:7:ggufv2:Q2_K chemical 0.264943 45 0 0.00588762 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations llama-3.1-instruct:70:ggufv2:IQ2_M context 25.2426 27 1.51082e-17 0.93491 3 llama-3.1-instruct:70:ggufv2:IQ4_XS context 25.2195 27 3.02164e-17 0.934057 3 claude-3-5-sonnet-20240620 context 24.5401 27 6.04329e-17 0.908892 3 gpt-4o-2024-08-06 context 23.6991 27 0.00396564 0.877746 3 gpt-4-turbo-2024-04-09 context 38.9656 45 0.0255706 0.865903 5 claude-3-opus-20240229 context 23.2287 27 3.02164e-17 0.860323 3 gpt-4o-mini-2024-07-18 context 38.476 45 0.00134967 0.855023 5 llama-3.1-instruct:70:ggufv2:Q3_K_S context 22.2637 27 1.51082e-17 0.82458 3 llama-3.1-instruct:8:ggufv2:Q8_0 context 16.8902 27 9.44264e-19 0.625563 3 llama-3.1-instruct:8:ggufv2:IQ4_XS context 16.8016 27 1.51082e-17 0.622281 3 llama-3.1-instruct:8:ggufv2:Q4_K_M context 16.4994 27 2.83279e-18 0.611089 3 llama-3.1-instruct:8:ggufv2:Q6_K context 16.4093 27 1.88853e-18 0.607753 3 llama-3.1-instruct:8:ggufv2:Q5_K_M context 16.1817 27 1.88853e-18 0.599324 3 llama-3.1-instruct:8:ggufv2:Q3_K_L context 15.6028 27 1.88853e-18 0.57788 3 llama-2-chat:7:ggufv2:Q8_0 context 5.70797 27 0 0.211406 3 llama-2-chat:13:ggufv2:Q6_K context 4.88293 27 1.69967e-17 0.180849 3 gpt-4-0613 context 7.90663 45 0 0.175703 5 gpt-4-0125-preview context 7.85253 45 0 0.174501 5 gpt-4o-2024-05-13 context 7.82965 45 0 0.173992 5 gpt-3.5-turbo-0125 context 6.89247 45 0 0.153166 5 openhermes-2.5:7:ggufv2:Q4_K_M context 6.89055 45 0 0.153123 5 openhermes-2.5:7:ggufv2:Q6_K context 6.79989 45 0 0.151109 5 openhermes-2.5:7:ggufv2:Q3_K_M context 6.77271 45 0 0.150505 5 openhermes-2.5:7:ggufv2:Q8_0 context 6.67749 45 0 0.148389 5 gpt-3.5-turbo-0613 context 6.50472 45 0 0.144549 5 openhermes-2.5:7:ggufv2:Q5_K_M context 6.44769 45 0 0.143282 5 llama-2-chat:7:ggufv2:Q6_K context 3.73057 27 0 0.138169 3 mistral-instruct-v0.2:7:ggufv2:Q8_0 context 5.16754 45 0 0.114834 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M context 5.12599 45 0 0.113911 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M context 5.02844 45 0 0.111743 5 mistral-instruct-v0.2:7:ggufv2:Q6_K context 5.0158 45 0 0.111462 5 mistral-instruct-v0.2:7:ggufv2:Q2_K context 4.99362 45 0 0.110969 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K context 4.51314 45 0 0.100292 5 llama-2-chat:70:ggufv2:Q3_K_M context 4.22332 45 0 0.0938516 5 llama-2-chat:70:ggufv2:Q4_K_M context 4.10284 45 0 0.0911743 5 llama-2-chat:70:ggufv2:Q2_K context 4.08979 45 0 0.0908843 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M context 4.06318 45 0 0.090293 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M context 4.01117 45 0 0.0891372 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M context 3.90982 45 0 0.0868849 5 openhermes-2.5:7:ggufv2:Q2_K context 3.86897 45 0 0.0859772 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M context 3.79416 45 0 0.0843146 5 llama-2-chat:70:ggufv2:Q5_K_M context 3.74591 45 0 0.0832424 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 context 3.70126 45 0 0.0822502 5 code-llama-instruct:7:ggufv2:Q4_K_M context 3.32657 45 0 0.0739237 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K context 3.1452 45 0 0.0698933 5 chatglm3:6:ggmlv3:q4_0 context 2.85636 45 0 0.0634747 5 llama-2-chat:7:ggufv2:Q3_K_M context 2.10857 45 0 0.046857 5 llama-2-chat:7:ggufv2:Q4_K_M context 1.89605 45 0 0.0421345 5 llama-2-chat:13:ggufv2:Q3_K_M context 1.78868 45 0 0.0397484 5 llama-2-chat:13:ggufv2:Q5_K_M context 1.78618 45 0 0.0396929 5 llama-2-chat:13:ggufv2:Q4_K_M context 1.77351 45 0 0.0394113 5 llama-3-instruct:8:ggufv2:Q8_0 context 1.67334 45 0 0.0371853 5 llama-3-instruct:8:ggufv2:Q5_K_M context 1.64821 45 0 0.0366268 5 llama-2-chat:13:ggufv2:Q8_0 context 1.58821 45 0 0.0352936 5 llama-3-instruct:8:ggufv2:Q4_K_M context 1.57169 45 0 0.0349264 5 llama-2-chat:13:ggufv2:Q2_K context 1.34289 45 0 0.0298419 5 llama-2-chat:7:ggufv2:Q5_K_M context 1.23881 45 0 0.0275291 5 llama-2-chat:7:ggufv2:Q2_K context 1.12335 45 0 0.0249632 5 llama-3-instruct:8:ggufv2:Q6_K context 1.10292 45 0 0.0245094 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 disease 20.4 27 1.51082e-17 0.755556 3 gpt-4o-mini-2024-07-18 disease 32.3333 45 0 0.718519 5 gpt-4o-2024-08-06 disease 19.4 27 1.51082e-17 0.718519 3 gpt-4-turbo-2024-04-09 disease 32.2667 45 0.00331269 0.717037 5 llama-3.1-instruct:70:ggufv2:IQ4_XS disease 17.2 27 7.55411e-18 0.637037 3 llama-3.1-instruct:70:ggufv2:IQ2_M disease 13.2 27 7.55411e-18 0.488889 3 llama-3.1-instruct:70:ggufv2:Q3_K_S disease 13.2 27 7.55411e-18 0.488889 3 llama-3.1-instruct:8:ggufv2:Q5_K_M disease 11.7851 27 7.55411e-18 0.436484 3 llama-3.1-instruct:8:ggufv2:Q3_K_L disease 11.5883 27 2.36066e-19 0.429196 3 llama-3.1-instruct:8:ggufv2:Q6_K disease 11.5414 27 1.51082e-17 0.42746 3 llama-3.1-instruct:8:ggufv2:IQ4_XS disease 11.5414 27 1.51082e-17 0.42746 3 llama-3.1-instruct:8:ggufv2:Q4_K_M disease 10.2531 27 2.36066e-19 0.379746 3 claude-3-opus-20240229 disease 10 27 0 0.37037 3 llama-3.1-instruct:8:ggufv2:Q8_0 disease 9.31558 27 2.36066e-19 0.345022 3 openhermes-2.5:7:ggufv2:Q3_K_M disease 6.46667 45 0 0.143704 5 openhermes-2.5:7:ggufv2:Q4_K_M disease 6.46667 45 0 0.143704 5 openhermes-2.5:7:ggufv2:Q5_K_M disease 6.46667 45 0 0.143704 5 openhermes-2.5:7:ggufv2:Q6_K disease 6.46667 45 0 0.143704 5 openhermes-2.5:7:ggufv2:Q8_0 disease 6.46667 45 0 0.143704 5 gpt-4-0125-preview disease 6.21333 45 0 0.138074 5 gpt-4o-2024-05-13 disease 6.2 45 0 0.137778 5 gpt-4-0613 disease 6.13333 45 0 0.136296 5 gpt-3.5-turbo-0613 disease 6.06667 45 0 0.134815 5 gpt-3.5-turbo-0125 disease 4.75238 45 0 0.105608 5 openhermes-2.5:7:ggufv2:Q2_K disease 4.32493 45 0 0.0961096 5 mistral-instruct-v0.2:7:ggufv2:Q2_K disease 4.20708 45 0 0.0934906 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K disease 4.14674 45 0 0.0921497 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M disease 4.02927 45 0 0.0895392 5 mistral-instruct-v0.2:7:ggufv2:Q6_K disease 4.01581 45 0 0.0892402 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 disease 3.47244 45 0 0.0771654 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M disease 3.04532 45 0 0.0676737 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M disease 2.92854 45 0 0.0650787 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M disease 2.65437 45 0 0.0589859 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 disease 2.57657 45 0 0.057257 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M disease 2.44785 45 0 0.0543966 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M disease 2.29171 45 0 0.0509269 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K disease 2.29094 45 0 0.0509099 5 llama-3-instruct:8:ggufv2:Q8_0 disease 1.73452 45 0 0.0385449 5 llama-3-instruct:8:ggufv2:Q6_K disease 1.73452 45 0 0.0385449 5 llama-3-instruct:8:ggufv2:Q5_K_M disease 1.73452 45 0 0.0385449 5 llama-2-chat:13:ggufv2:Q6_K disease 0.827524 27 0 0.030649 3 code-llama-instruct:7:ggufv2:Q4_K_M disease 1.33093 45 0 0.0295762 5 chatglm3:6:ggmlv3:q4_0 disease 1.21669 45 0 0.0270376 5 llama-3-instruct:8:ggufv2:Q4_K_M disease 0.995894 45 0 0.022131 5 llama-2-chat:7:ggufv2:Q8_0 disease 0.444887 27 2.36066e-19 0.0164773 3 llama-2-chat:7:ggufv2:Q6_K disease 0.439254 27 0 0.0162687 3 llama-2-chat:13:ggufv2:Q5_K_M disease 0.306386 45 0 0.00680858 5 llama-2-chat:13:ggufv2:Q8_0 disease 0.26663 45 0 0.00592511 5 llama-2-chat:13:ggufv2:Q4_K_M disease 0.250053 45 0 0.00555673 5 llama-2-chat:70:ggufv2:Q5_K_M disease 0.235648 45 0 0.00523663 5 llama-2-chat:7:ggufv2:Q3_K_M disease 0.185035 45 0 0.0041119 5 llama-2-chat:70:ggufv2:Q2_K disease 0.182046 45 0 0.00404548 5 llama-2-chat:70:ggufv2:Q4_K_M disease 0.179398 45 0 0.00398663 5 llama-2-chat:7:ggufv2:Q5_K_M disease 0.150208 45 0 0.00333795 5 llama-2-chat:70:ggufv2:Q3_K_M disease 0.142957 45 0 0.00317683 5 llama-2-chat:13:ggufv2:Q3_K_M disease 0.103277 45 0 0.00229505 5 llama-2-chat:7:ggufv2:Q4_K_M disease 0.0898052 45 0 0.00199567 5 llama-2-chat:13:ggufv2:Q2_K disease 0.0874203 45 0 0.00194267 5 llama-2-chat:7:ggufv2:Q2_K disease 0.0587138 45 0 0.00130475 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-08-06 entity 17.9286 27 0 0.664021 3 llama-3.1-instruct:70:ggufv2:Q3_K_S entity 17.8096 27 1.51082e-17 0.659615 3 llama-3.1-instruct:70:ggufv2:IQ4_XS entity 17.6096 27 7.55411e-18 0.652208 3 claude-3-opus-20240229 entity 16.325 27 4.53247e-17 0.60463 3 llama-3.1-instruct:70:ggufv2:IQ2_M entity 16.225 27 1.51082e-17 0.600926 3 claude-3-5-sonnet-20240620 entity 15.625 27 1.51082e-17 0.578704 3 gpt-4o-mini-2024-07-18 entity 24.5545 45 0.0661682 0.545656 5 gpt-4-turbo-2024-04-09 entity 19.2455 45 0.0562119 0.427677 5 llama-3.1-instruct:8:ggufv2:Q8_0 entity 8.02772 27 0 0.297323 3 llama-3.1-instruct:8:ggufv2:Q6_K entity 7.84022 27 0 0.290379 3 llama-3.1-instruct:8:ggufv2:IQ4_XS entity 6.80681 27 1.51082e-17 0.252104 3 llama-3.1-instruct:8:ggufv2:Q3_K_L entity 6.16922 27 1.69967e-17 0.22849 3 llama-3.1-instruct:8:ggufv2:Q4_K_M entity 5.96677 27 7.55411e-18 0.220991 3 llama-3.1-instruct:8:ggufv2:Q5_K_M entity 5.58394 27 0 0.206813 3 gpt-4o-2024-05-13 entity 5.9909 45 0 0.133131 5 gpt-4-0125-preview entity 4.59502 45 0 0.102112 5 gpt-3.5-turbo-0613 entity 4.57972 45 0 0.101772 5 openhermes-2.5:7:ggufv2:Q4_K_M entity 4.22461 45 0 0.0938803 5 openhermes-2.5:7:ggufv2:Q8_0 entity 4.1344 45 0 0.0918755 5 gpt-4-0613 entity 4.12852 45 0 0.0917448 5 openhermes-2.5:7:ggufv2:Q6_K entity 4.09333 45 0 0.0909629 5 openhermes-2.5:7:ggufv2:Q5_K_M entity 4.02016 45 0 0.0893369 5 gpt-3.5-turbo-0125 entity 3.71195 45 0 0.0824877 5 openhermes-2.5:7:ggufv2:Q3_K_M entity 3.65819 45 0 0.0812932 5 llama-2-chat:13:ggufv2:Q6_K entity 2.14189 27 0 0.0793293 3 llama-2-chat:7:ggufv2:Q6_K entity 2.07106 27 9.44264e-19 0.0767059 3 llama-2-chat:7:ggufv2:Q8_0 entity 1.79733 27 0 0.0665678 3 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M entity 2.42313 45 0 0.0538473 5 openhermes-2.5:7:ggufv2:Q2_K entity 2.33413 45 0 0.0518696 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M entity 2.30597 45 0 0.0512437 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M entity 2.20283 45 0 0.0489518 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K entity 2.10077 45 0 0.0466838 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M entity 2.0607 45 0 0.0457934 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 entity 2.00802 45 0 0.0446226 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M entity 1.99809 45 0 0.044402 5 mistral-instruct-v0.2:7:ggufv2:Q6_K entity 1.99214 45 0 0.0442699 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 entity 1.79999 45 0 0.0399998 5 mistral-instruct-v0.2:7:ggufv2:Q2_K entity 1.77563 45 0 0.0394584 5 chatglm3:6:ggmlv3:q4_0 entity 1.22227 45 0 0.0271617 5 llama-2-chat:70:ggufv2:Q3_K_M entity 1.20851 45 0 0.0268558 5 llama-2-chat:70:ggufv2:Q2_K entity 1.16189 45 0 0.0258197 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M entity 1.10007 45 0 0.0244461 5 llama-2-chat:70:ggufv2:Q4_K_M entity 1.01555 45 0 0.0225677 5 code-llama-instruct:7:ggufv2:Q4_K_M entity 0.948961 45 0 0.021088 5 llama-2-chat:70:ggufv2:Q5_K_M entity 0.903324 45 0 0.0200739 5 llama-2-chat:13:ggufv2:Q2_K entity 0.807379 45 0 0.0179418 5 llama-2-chat:13:ggufv2:Q4_K_M entity 0.785233 45 0 0.0174496 5 llama-3-instruct:8:ggufv2:Q5_K_M entity 0.75253 45 0 0.0167229 5 llama-3-instruct:8:ggufv2:Q6_K entity 0.749495 45 0 0.0166554 5 llama-2-chat:7:ggufv2:Q3_K_M entity 0.699988 45 0 0.0155553 5 llama-3-instruct:8:ggufv2:Q8_0 entity 0.695524 45 0 0.0154561 5 llama-3-instruct:8:ggufv2:Q4_K_M entity 0.694377 45 0 0.0154306 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K entity 0.685368 45 0 0.0152304 5 llama-2-chat:7:ggufv2:Q4_K_M entity 0.685027 45 0 0.0152228 5 llama-2-chat:13:ggufv2:Q8_0 entity 0.629764 45 0 0.0139947 5 llama-2-chat:7:ggufv2:Q5_K_M entity 0.623851 45 0 0.0138634 5 llama-2-chat:13:ggufv2:Q5_K_M entity 0.623813 45 0 0.0138625 5 llama-2-chat:13:ggufv2:Q3_K_M entity 0.56502 45 0 0.012556 5 llama-2-chat:7:ggufv2:Q2_K entity 0.318196 45 0 0.00707101 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-opus-20240229 experiment_yes_or_no 27 27 0 1 3 gpt-4-turbo-2024-04-09 experiment_yes_or_no 45 45 0 1 5 llama-3.1-instruct:70:ggufv2:IQ4_XS experiment_yes_or_no 27 27 0 1 3 llama-3.1-instruct:70:ggufv2:Q3_K_S experiment_yes_or_no 27 27 0 1 3 claude-3-5-sonnet-20240620 experiment_yes_or_no 27 27 0 1 3 gpt-4o-2024-08-06 experiment_yes_or_no 25 27 0 0.925926 3 gpt-4o-mini-2024-07-18 experiment_yes_or_no 40 45 0 0.888889 5 llama-3.1-instruct:70:ggufv2:IQ2_M experiment_yes_or_no 22 27 0 0.814815 3 llama-3.1-instruct:8:ggufv2:Q6_K experiment_yes_or_no 18.0146 27 0 0.667206 3 llama-3.1-instruct:8:ggufv2:Q4_K_M experiment_yes_or_no 18.006 27 0 0.666888 3 llama-3.1-instruct:8:ggufv2:IQ4_XS experiment_yes_or_no 18.0059 27 0 0.666887 3 llama-3.1-instruct:8:ggufv2:Q8_0 experiment_yes_or_no 18.0059 27 0 0.666886 3 llama-3.1-instruct:8:ggufv2:Q5_K_M experiment_yes_or_no 18 27 0 0.666667 3 llama-3.1-instruct:8:ggufv2:Q3_K_L experiment_yes_or_no 18 27 0 0.666667 3 openhermes-2.5:7:ggufv2:Q2_K experiment_yes_or_no 9 45 0 0.2 5 gpt-4-0125-preview experiment_yes_or_no 9 45 0 0.2 5 llama-2-chat:70:ggufv2:Q4_K_M experiment_yes_or_no 9 45 0 0.2 5 chatglm3:6:ggmlv3:q4_0 experiment_yes_or_no 8.6 45 0 0.191111 5 openhermes-2.5:7:ggufv2:Q5_K_M experiment_yes_or_no 8.33333 45 0 0.185185 5 openhermes-2.5:7:ggufv2:Q6_K experiment_yes_or_no 8.33333 45 0 0.185185 5 openhermes-2.5:7:ggufv2:Q4_K_M experiment_yes_or_no 8.33333 45 0 0.185185 5 llama-2-chat:70:ggufv2:Q5_K_M experiment_yes_or_no 8.025 45 0 0.178333 5 openhermes-2.5:7:ggufv2:Q8_0 experiment_yes_or_no 8 45 0 0.177778 5 gpt-3.5-turbo-0613 experiment_yes_or_no 8 45 0 0.177778 5 gpt-4-0613 experiment_yes_or_no 8 45 0 0.177778 5 openhermes-2.5:7:ggufv2:Q3_K_M experiment_yes_or_no 8 45 0 0.177778 5 gpt-4o-2024-05-13 experiment_yes_or_no 8 45 0 0.177778 5 llama-2-chat:7:ggufv2:Q8_0 experiment_yes_or_no 4.67535 27 0 0.173161 3 llama-2-chat:70:ggufv2:Q2_K experiment_yes_or_no 7.05061 45 0 0.15668 5 llama-2-chat:70:ggufv2:Q3_K_M experiment_yes_or_no 6.07336 45 0 0.134964 5 gpt-3.5-turbo-0125 experiment_yes_or_no 6.03333 45 0 0.134074 5 llama-2-chat:13:ggufv2:Q6_K experiment_yes_or_no 3.25916 27 9.44264e-19 0.12071 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M experiment_yes_or_no 5.23564 45 0 0.116348 5 llama-2-chat:13:ggufv2:Q3_K_M experiment_yes_or_no 5.16593 45 0 0.114799 5 llama-3-instruct:8:ggufv2:Q8_0 experiment_yes_or_no 3.7 45 0 0.0822222 5 llama-3-instruct:8:ggufv2:Q5_K_M experiment_yes_or_no 3.68182 45 0 0.0818182 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 experiment_yes_or_no 3.32028 45 0 0.073784 5 llama-2-chat:7:ggufv2:Q6_K experiment_yes_or_no 1.97565 27 7.08198e-19 0.0731722 3 mistral-instruct-v0.2:7:ggufv2:Q5_K_M experiment_yes_or_no 3.26963 45 0 0.0726584 5 code-llama-instruct:7:ggufv2:Q4_K_M experiment_yes_or_no 3.0913 45 0 0.0686956 5 llama-3-instruct:8:ggufv2:Q6_K experiment_yes_or_no 2.36364 45 0 0.0525253 5 mistral-instruct-v0.2:7:ggufv2:Q6_K experiment_yes_or_no 2.36015 45 0 0.0524479 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M experiment_yes_or_no 2.2851 45 0 0.05078 5 mistral-instruct-v0.2:7:ggufv2:Q2_K experiment_yes_or_no 2.2802 45 0 0.0506711 5 llama-2-chat:7:ggufv2:Q4_K_M experiment_yes_or_no 2.06817 45 0 0.0459593 5 llama-3-instruct:8:ggufv2:Q4_K_M experiment_yes_or_no 1.89935 45 0 0.0422078 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M experiment_yes_or_no 1.45686 45 0 0.0323746 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M experiment_yes_or_no 1.29991 45 0 0.0288868 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M experiment_yes_or_no 1.1661 45 0 0.0259134 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K experiment_yes_or_no 1.15184 45 0 0.0255965 5 llama-2-chat:13:ggufv2:Q8_0 experiment_yes_or_no 1.06643 45 0 0.0236984 5 llama-2-chat:13:ggufv2:Q5_K_M experiment_yes_or_no 1.03147 45 0 0.0229215 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K experiment_yes_or_no 0.785587 45 0 0.0174575 5 llama-2-chat:7:ggufv2:Q3_K_M experiment_yes_or_no 0.726745 45 0 0.0161499 5 llama-2-chat:7:ggufv2:Q5_K_M experiment_yes_or_no 0.618798 45 0 0.0137511 5 llama-2-chat:13:ggufv2:Q4_K_M experiment_yes_or_no 0.468722 45 0 0.010416 5 llama-2-chat:13:ggufv2:Q2_K experiment_yes_or_no 0.267272 45 0 0.00593938 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 experiment_yes_or_no 0.201489 45 0 0.00447753 5 llama-2-chat:7:ggufv2:Q2_K experiment_yes_or_no 0.130285 45 0 0.00289522 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations gpt-4o-mini-2024-07-18 hypothesis 17.2242 45 0.0552888 0.382759 5 gpt-4-turbo-2024-04-09 hypothesis 16.3215 45 0.154249 0.3627 5 gpt-4o-2024-08-06 hypothesis 8.73977 27 0.0621005 0.323695 3 llama-3.1-instruct:70:ggufv2:IQ4_XS hypothesis 6.99218 27 0 0.25897 3 claude-3-opus-20240229 hypothesis 6.74202 27 0.0355843 0.249704 3 llama-3.1-instruct:70:ggufv2:Q3_K_S hypothesis 5.51964 27 4.72132e-18 0.204431 3 claude-3-5-sonnet-20240620 hypothesis 4.76919 27 0.000375971 0.176637 3 llama-3.1-instruct:8:ggufv2:IQ4_XS hypothesis 4.4134 27 0 0.163459 3 llama-3.1-instruct:70:ggufv2:IQ2_M hypothesis 4.27846 27 0 0.158462 3 llama-3.1-instruct:8:ggufv2:Q3_K_L hypothesis 4.14868 27 0 0.153655 3 llama-3.1-instruct:8:ggufv2:Q5_K_M hypothesis 3.48866 27 3.55023e-05 0.129209 3 llama-2-chat:7:ggufv2:Q8_0 hypothesis 2.854 27 3.77706e-18 0.105704 3 llama-3.1-instruct:8:ggufv2:Q4_K_M hypothesis 2.74519 27 1.88853e-18 0.101674 3 llama-3.1-instruct:8:ggufv2:Q8_0 hypothesis 2.70116 27 0 0.100043 3 llama-3.1-instruct:8:ggufv2:Q6_K hypothesis 2.64133 27 9.44264e-19 0.097827 3 mistral-instruct-v0.2:7:ggufv2:Q4_K_M hypothesis 3.67339 45 0 0.0816309 5 llama-2-chat:7:ggufv2:Q6_K hypothesis 2.01944 27 5.19345e-18 0.074794 3 mistral-instruct-v0.2:7:ggufv2:Q6_K hypothesis 3.33681 45 0 0.0741512 5 gpt-4-0613 hypothesis 3.29696 45 0 0.0732657 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 hypothesis 2.9272 45 0 0.0650489 5 gpt-4o-2024-05-13 hypothesis 2.89512 45 0 0.064336 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M hypothesis 2.75585 45 0 0.0612411 5 gpt-3.5-turbo-0125 hypothesis 2.72775 45 0 0.0606168 5 llama-2-chat:13:ggufv2:Q6_K hypothesis 1.61253 27 1.88853e-18 0.0597233 3 gpt-3.5-turbo-0613 hypothesis 2.64497 45 0 0.0587771 5 openhermes-2.5:7:ggufv2:Q4_K_M hypothesis 2.57382 45 0 0.0571961 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M hypothesis 2.47292 45 0 0.0549539 5 openhermes-2.5:7:ggufv2:Q8_0 hypothesis 2.37196 45 0 0.0527103 5 gpt-4-0125-preview hypothesis 2.33518 45 0 0.051893 5 openhermes-2.5:7:ggufv2:Q6_K hypothesis 2.29085 45 0 0.0509077 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M hypothesis 2.23255 45 0 0.0496122 5 openhermes-2.5:7:ggufv2:Q3_K_M hypothesis 2.09626 45 0 0.0465835 5 mistral-instruct-v0.2:7:ggufv2:Q2_K hypothesis 2.05375 45 0 0.045639 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M hypothesis 1.87442 45 0 0.0416537 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 hypothesis 1.83735 45 0 0.04083 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M hypothesis 1.71557 45 0 0.0381237 5 openhermes-2.5:7:ggufv2:Q5_K_M hypothesis 1.52181 45 0 0.033818 5 openhermes-2.5:7:ggufv2:Q2_K hypothesis 1.4915 45 0 0.0331444 5 llama-2-chat:70:ggufv2:Q3_K_M hypothesis 1.44143 45 0 0.0320317 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K hypothesis 1.44009 45 0 0.032002 5 llama-2-chat:70:ggufv2:Q2_K hypothesis 1.4389 45 0 0.0319755 5 llama-2-chat:70:ggufv2:Q4_K_M hypothesis 1.41421 45 0 0.0314268 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K hypothesis 1.39565 45 0 0.0310144 5 llama-3-instruct:8:ggufv2:Q4_K_M hypothesis 1.13596 45 0 0.0252436 5 chatglm3:6:ggmlv3:q4_0 hypothesis 0.98676 45 0 0.021928 5 llama-3-instruct:8:ggufv2:Q8_0 hypothesis 0.878406 45 0 0.0195201 5 llama-3-instruct:8:ggufv2:Q6_K hypothesis 0.876219 45 0 0.0194715 5 llama-2-chat:7:ggufv2:Q5_K_M hypothesis 0.68638 45 0 0.0152529 5 llama-2-chat:70:ggufv2:Q5_K_M hypothesis 0.623758 45 0 0.0138613 5 llama-2-chat:7:ggufv2:Q4_K_M hypothesis 0.62053 45 0 0.0137896 5 llama-3-instruct:8:ggufv2:Q5_K_M hypothesis 0.604423 45 0 0.0134316 5 code-llama-instruct:7:ggufv2:Q4_K_M hypothesis 0.572369 45 0 0.0127193 5 llama-2-chat:13:ggufv2:Q8_0 hypothesis 0.55524 45 0 0.0123387 5 llama-2-chat:7:ggufv2:Q2_K hypothesis 0.520453 45 0 0.0115656 5 llama-2-chat:13:ggufv2:Q2_K hypothesis 0.49279 45 0 0.0109509 5 llama-2-chat:13:ggufv2:Q3_K_M hypothesis 0.424638 45 0 0.00943639 5 llama-2-chat:13:ggufv2:Q5_K_M hypothesis 0.408017 45 0 0.00906704 5 llama-2-chat:7:ggufv2:Q3_K_M hypothesis 0.402337 45 0 0.00894082 5 llama-2-chat:13:ggufv2:Q4_K_M hypothesis 0.366299 45 0 0.00813997 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations llama-3.1-instruct:70:ggufv2:Q3_K_S intervention 21.7143 27 0 0.804233 3 llama-3.1-instruct:70:ggufv2:IQ4_XS intervention 20.4286 27 0 0.756614 3 llama-3.1-instruct:70:ggufv2:IQ2_M intervention 20.4286 27 0 0.756614 3 gpt-4o-mini-2024-07-18 intervention 30.0404 45 0.0608581 0.667565 5 gpt-4o-2024-08-06 intervention 17.3762 27 0.0687322 0.643563 3 llama-3.1-instruct:8:ggufv2:Q8_0 intervention 14.9465 27 0 0.553575 3 claude-3-opus-20240229 intervention 14.8366 27 0 0.549502 3 claude-3-5-sonnet-20240620 intervention 14.533 27 0.00814604 0.538261 3 gpt-4-turbo-2024-04-09 intervention 23.5746 45 0.0579382 0.52388 5 llama-3.1-instruct:8:ggufv2:Q5_K_M intervention 13.2322 27 0 0.490083 3 llama-3.1-instruct:8:ggufv2:IQ4_XS intervention 12.6108 27 0 0.467065 3 llama-3.1-instruct:8:ggufv2:Q6_K intervention 11.4946 27 0 0.425726 3 llama-3.1-instruct:8:ggufv2:Q4_K_M intervention 11.4467 27 0 0.423953 3 llama-3.1-instruct:8:ggufv2:Q3_K_L intervention 7.23948 27 1.88853e-18 0.268129 3 gpt-4o-2024-05-13 intervention 5.34631 45 0 0.118807 5 openhermes-2.5:7:ggufv2:Q4_K_M intervention 4.9841 45 0 0.110758 5 gpt-4-0125-preview intervention 4.92171 45 0 0.109371 5 gpt-4-0613 intervention 4.72253 45 0 0.104945 5 openhermes-2.5:7:ggufv2:Q6_K intervention 4.71449 45 0 0.104767 5 openhermes-2.5:7:ggufv2:Q8_0 intervention 4.44465 45 0 0.09877 5 gpt-3.5-turbo-0613 intervention 4.27143 45 0 0.0949206 5 openhermes-2.5:7:ggufv2:Q5_K_M intervention 4.00021 45 0 0.0888935 5 gpt-3.5-turbo-0125 intervention 3.75141 45 0 0.0833647 5 openhermes-2.5:7:ggufv2:Q3_K_M intervention 3.55238 45 0 0.0789418 5 openhermes-2.5:7:ggufv2:Q2_K intervention 2.92766 45 0 0.0650591 5 llama-2-chat:7:ggufv2:Q8_0 intervention 1.56417 27 0 0.0579323 3 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K intervention 2.23683 45 0 0.0497073 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M intervention 2.23319 45 0 0.0496264 5 llama-2-chat:13:ggufv2:Q6_K intervention 1.13241 27 0.000274145 0.0419412 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M intervention 1.66677 45 0 0.0370393 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M intervention 1.23412 45 0 0.0274249 5 code-llama-instruct:7:ggufv2:Q4_K_M intervention 1.17173 45 0 0.0260384 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M intervention 1.15754 45 0 0.025723 5 llama-2-chat:13:ggufv2:Q4_K_M intervention 1.02157 45 0 0.0227015 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 intervention 0.987919 45 0 0.0219538 5 chatglm3:6:ggmlv3:q4_0 intervention 0.881806 45 0 0.0195957 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 intervention 0.879646 45 0 0.0195477 5 llama-2-chat:7:ggufv2:Q6_K intervention 0.514286 27 3.77706e-18 0.0190476 3 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M intervention 0.723791 45 0 0.0160842 5 mistral-instruct-v0.2:7:ggufv2:Q2_K intervention 0.680182 45 0 0.0151152 5 llama-2-chat:70:ggufv2:Q2_K intervention 0.668995 45 0 0.0148666 5 mistral-instruct-v0.2:7:ggufv2:Q6_K intervention 0.640258 45 0 0.0142279 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M intervention 0.550643 45 0 0.0122365 5 llama-2-chat:70:ggufv2:Q5_K_M intervention 0.542302 45 0 0.0120512 5 llama-2-chat:13:ggufv2:Q2_K intervention 0.502722 45 0 0.0111716 5 llama-2-chat:70:ggufv2:Q4_K_M intervention 0.417501 45 0 0.00927779 5 llama-2-chat:7:ggufv2:Q3_K_M intervention 0.416756 45 0 0.00926124 5 llama-3-instruct:8:ggufv2:Q5_K_M intervention 0.410888 45 0 0.00913085 5 llama-2-chat:70:ggufv2:Q3_K_M intervention 0.402319 45 0 0.00894042 5 llama-3-instruct:8:ggufv2:Q4_K_M intervention 0.37923 45 0 0.00842733 5 llama-2-chat:13:ggufv2:Q5_K_M intervention 0.339683 45 0 0.0075485 5 llama-3-instruct:8:ggufv2:Q6_K intervention 0.327257 45 0 0.00727237 5 llama-3-instruct:8:ggufv2:Q8_0 intervention 0.319187 45 0 0.00709304 5 llama-2-chat:13:ggufv2:Q3_K_M intervention 0.265476 45 0 0.00589947 5 llama-2-chat:7:ggufv2:Q5_K_M intervention 0.24986 45 0 0.00555244 5 llama-2-chat:13:ggufv2:Q8_0 intervention 0.244444 45 0 0.0054321 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K intervention 0.2273 45 0 0.0050511 5 llama-2-chat:7:ggufv2:Q2_K intervention 0.118691 45 0 0.00263758 5 llama-2-chat:7:ggufv2:Q4_K_M intervention 0.0769231 45 0 0.0017094 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 ncbi_link 24 27 0 0.888889 3 claude-3-opus-20240229 ncbi_link 20.88 27 1.51082e-17 0.773333 3 gpt-4o-2024-08-06 ncbi_link 15.6389 27 0.00890973 0.579218 3 llama-3.1-instruct:70:ggufv2:IQ4_XS ncbi_link 15.3 27 0 0.566667 3 gpt-4-turbo-2024-04-09 ncbi_link 23.3333 45 0.132508 0.518519 5 llama-3.1-instruct:70:ggufv2:IQ2_M ncbi_link 13.5 27 0 0.5 3 gpt-4o-mini-2024-07-18 ncbi_link 16.8619 45 0.00496904 0.374709 5 llama-3.1-instruct:70:ggufv2:Q3_K_S ncbi_link 9.5 27 0 0.351852 3 gpt-4-0125-preview ncbi_link 6.48768 45 0 0.144171 5 llama-3.1-instruct:8:ggufv2:IQ4_XS ncbi_link 3.86536 27 0 0.143161 3 gpt-4-0613 ncbi_link 6.05933 45 0 0.134652 5 llama-3.1-instruct:8:ggufv2:Q8_0 ncbi_link 2.32755 27 7.55411e-18 0.0862055 3 openhermes-2.5:7:ggufv2:Q6_K ncbi_link 3.5303 45 0 0.0784512 5 openhermes-2.5:7:ggufv2:Q8_0 ncbi_link 3.5303 45 0 0.0784512 5 gpt-4o-2024-05-13 ncbi_link 3.51302 45 0 0.078067 5 openhermes-2.5:7:ggufv2:Q5_K_M ncbi_link 3.47436 45 0 0.077208 5 llama-3.1-instruct:8:ggufv2:Q4_K_M ncbi_link 1.90181 27 7.91155e-07 0.0704374 3 openhermes-2.5:7:ggufv2:Q4_K_M ncbi_link 3.11111 45 0 0.0691358 5 llama-3.1-instruct:8:ggufv2:Q5_K_M ncbi_link 1.59127 27 0 0.0589359 3 openhermes-2.5:7:ggufv2:Q3_K_M ncbi_link 2.37436 45 0 0.0527635 5 llama-3.1-instruct:8:ggufv2:Q6_K ncbi_link 1.31 27 0 0.0485184 3 gpt-3.5-turbo-0613 ncbi_link 2.16667 45 0 0.0481481 5 gpt-3.5-turbo-0125 ncbi_link 1.42925 45 0 0.031761 5 llama-2-chat:13:ggufv2:Q6_K ncbi_link 0.690904 27 0 0.0255891 3 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M ncbi_link 1.03429 45 0 0.0229841 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M ncbi_link 0.884957 45 0 0.0196657 5 mistral-instruct-v0.2:7:ggufv2:Q2_K ncbi_link 0.881705 45 0 0.0195934 5 llama-2-chat:7:ggufv2:Q6_K ncbi_link 0.507313 27 9.44264e-19 0.0187894 3 llama-3.1-instruct:8:ggufv2:Q3_K_L ncbi_link 0.486291 27 2.95082e-19 0.0180108 3 mistral-instruct-v0.2:7:ggufv2:Q5_K_M ncbi_link 0.710989 45 0 0.0157998 5 llama-2-chat:7:ggufv2:Q8_0 ncbi_link 0.410766 27 9.44264e-19 0.0152135 3 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K ncbi_link 0.656812 45 0 0.0145958 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M ncbi_link 0.615714 45 0 0.0136825 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 ncbi_link 0.596131 45 0 0.0132474 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M ncbi_link 0.574422 45 0 0.0127649 5 mistral-instruct-v0.2:7:ggufv2:Q6_K ncbi_link 0.558824 45 0 0.0124183 5 openhermes-2.5:7:ggufv2:Q2_K ncbi_link 0.505458 45 0 0.0112324 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 ncbi_link 0.429927 45 0 0.00955394 5 code-llama-instruct:7:ggufv2:Q4_K_M ncbi_link 0.328564 45 0 0.00730142 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K ncbi_link 0.271548 45 0 0.0060344 5 llama-2-chat:13:ggufv2:Q8_0 ncbi_link 0.255217 45 0 0.00567148 5 llama-2-chat:70:ggufv2:Q2_K ncbi_link 0.253735 45 0 0.00563856 5 llama-2-chat:13:ggufv2:Q4_K_M ncbi_link 0.246231 45 0 0.00547179 5 llama-2-chat:70:ggufv2:Q4_K_M ncbi_link 0.241357 45 0 0.00536348 5 llama-2-chat:13:ggufv2:Q5_K_M ncbi_link 0.236802 45 0 0.00526226 5 llama-3-instruct:8:ggufv2:Q4_K_M ncbi_link 0.233815 45 0 0.00519589 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M ncbi_link 0.230909 45 0 0.00513131 5 llama-2-chat:7:ggufv2:Q4_K_M ncbi_link 0.216341 45 0 0.00480757 5 llama-2-chat:70:ggufv2:Q5_K_M ncbi_link 0.196981 45 0 0.00437735 5 llama-2-chat:13:ggufv2:Q2_K ncbi_link 0.192574 45 0 0.00427942 5 llama-3-instruct:8:ggufv2:Q8_0 ncbi_link 0.179211 45 0 0.00398247 5 llama-2-chat:7:ggufv2:Q3_K_M ncbi_link 0.177339 45 0 0.00394087 5 llama-3-instruct:8:ggufv2:Q6_K ncbi_link 0.173014 45 0 0.00384476 5 llama-2-chat:7:ggufv2:Q5_K_M ncbi_link 0.170952 45 0 0.00379894 5 llama-2-chat:70:ggufv2:Q3_K_M ncbi_link 0.166777 45 0 0.00370615 5 llama-3-instruct:8:ggufv2:Q5_K_M ncbi_link 0.166614 45 0 0.00370254 5 llama-2-chat:7:ggufv2:Q2_K ncbi_link 0.15271 45 0 0.00339354 5 llama-2-chat:13:ggufv2:Q3_K_M ncbi_link 0.150011 45 0 0.00333359 5 chatglm3:6:ggmlv3:q4_0 ncbi_link 0.122857 45 0 0.00273017 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 significance 21.6 27 6.04329e-17 0.8 3 gpt-4o-mini-2024-07-18 significance 36 45 0 0.8 5 gpt-4o-2024-08-06 significance 17.8444 27 0.00285111 0.660905 3 llama-3.1-instruct:70:ggufv2:Q3_K_S significance 15.6 27 6.04329e-17 0.577778 3 claude-3-opus-20240229 significance 13.7935 27 0.013441 0.51087 3 llama-3.1-instruct:70:ggufv2:IQ4_XS significance 13.6 27 6.7987e-17 0.503704 3 gpt-4-turbo-2024-04-09 significance 22.6141 45 0.0472812 0.502536 5 llama-3.1-instruct:70:ggufv2:IQ2_M significance 13.0286 27 6.04329e-17 0.48254 3 llama-3.1-instruct:8:ggufv2:Q6_K significance 6.06508 27 1.51082e-17 0.224633 3 llama-3.1-instruct:8:ggufv2:IQ4_XS significance 6.00361 27 7.55411e-18 0.222356 3 llama-3.1-instruct:8:ggufv2:Q5_K_M significance 5.21313 27 1.51082e-17 0.193079 3 llama-3.1-instruct:8:ggufv2:Q8_0 significance 5.0798 27 2.26623e-17 0.188141 3 llama-3.1-instruct:8:ggufv2:Q4_K_M significance 4.94646 27 3.02164e-17 0.183202 3 llama-3.1-instruct:8:ggufv2:Q3_K_L significance 4.31237 27 2.36066e-17 0.159717 3 gpt-4-0613 significance 5.6 45 0 0.124444 5 gpt-4-0125-preview significance 5.18384 45 0 0.115196 5 gpt-4o-2024-05-13 significance 4.22424 45 0 0.0938721 5 openhermes-2.5:7:ggufv2:Q4_K_M significance 3.92996 45 0 0.0873325 5 openhermes-2.5:7:ggufv2:Q8_0 significance 3.78182 45 0 0.0840404 5 openhermes-2.5:7:ggufv2:Q6_K significance 3.78182 45 0 0.0840404 5 openhermes-2.5:7:ggufv2:Q5_K_M significance 3.77787 45 0 0.0839526 5 openhermes-2.5:7:ggufv2:Q3_K_M significance 3.69091 45 0 0.0820202 5 gpt-3.5-turbo-0613 significance 3.58562 45 0 0.0796804 5 gpt-3.5-turbo-0125 significance 3.51717 45 0 0.0781594 5 mistral-instruct-v0.2:7:ggufv2:Q4_K_M significance 2.93833 45 0 0.0652963 5 mistral-instruct-v0.2:7:ggufv2:Q6_K significance 2.87928 45 0 0.063984 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M significance 2.79423 45 0 0.062094 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 significance 2.62296 45 0 0.0582881 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M significance 2.56724 45 0 0.0570498 5 openhermes-2.5:7:ggufv2:Q2_K significance 2.48514 45 0 0.0552254 5 mistral-instruct-v0.2:7:ggufv2:Q2_K significance 2.4813 45 0 0.05514 5 llama-2-chat:7:ggufv2:Q8_0 significance 1.10159 27 0 0.0407996 3 llama-2-chat:13:ggufv2:Q6_K significance 1.07015 27 1.0623e-18 0.0396352 3 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 significance 1.50696 45 0 0.033488 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K significance 1.34869 45 0 0.0299709 5 llama-2-chat:7:ggufv2:Q6_K significance 0.806474 27 0 0.0298694 3 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M significance 1.31454 45 0 0.0292119 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M significance 1.2312 45 0 0.0273599 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M significance 1.01129 45 0 0.0224731 5 llama-3-instruct:8:ggufv2:Q6_K significance 0.994971 45 0 0.0221105 5 llama-3-instruct:8:ggufv2:Q8_0 significance 0.957259 45 0 0.0212724 5 llama-2-chat:70:ggufv2:Q3_K_M significance 0.758379 45 0 0.0168529 5 llama-2-chat:70:ggufv2:Q2_K significance 0.716547 45 0 0.0159233 5 llama-2-chat:70:ggufv2:Q4_K_M significance 0.68386 45 0 0.0151969 5 llama-3-instruct:8:ggufv2:Q5_K_M significance 0.636128 45 0 0.0141362 5 llama-2-chat:70:ggufv2:Q5_K_M significance 0.518572 45 0 0.0115238 5 llama-2-chat:7:ggufv2:Q4_K_M significance 0.329457 45 0 0.00732127 5 llama-2-chat:13:ggufv2:Q8_0 significance 0.326026 45 0 0.00724502 5 llama-2-chat:7:ggufv2:Q5_K_M significance 0.281188 45 0 0.00624862 5 llama-3-instruct:8:ggufv2:Q4_K_M significance 0.228461 45 0 0.00507691 5 llama-2-chat:13:ggufv2:Q4_K_M significance 0.213246 45 0 0.0047388 5 llama-2-chat:13:ggufv2:Q2_K significance 0.207957 45 0 0.00462127 5 llama-2-chat:13:ggufv2:Q5_K_M significance 0.205271 45 0 0.00456158 5 llama-2-chat:7:ggufv2:Q3_K_M significance 0.194946 45 0 0.00433214 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K significance 0.178078 45 0 0.00395728 5 llama-2-chat:13:ggufv2:Q3_K_M significance 0.131484 45 0 0.00292186 5 code-llama-instruct:7:ggufv2:Q4_K_M significance 0.123914 45 0 0.00275365 5 chatglm3:6:ggmlv3:q4_0 significance 0.118153 45 0 0.00262562 5 llama-2-chat:7:ggufv2:Q2_K significance 0.103278 45 0 0.00229507 5 Full model name Subtask Score achieved Score possible Score SD Accuracy Iterations claude-3-5-sonnet-20240620 stats 27 27 0 1 3 claude-3-opus-20240229 stats 27 27 0 1 3 gpt-4-turbo-2024-04-09 stats 45 45 0 1 5 gpt-4o-2024-08-06 stats 26.5385 27 0 0.982906 3 gpt-4o-mini-2024-07-18 stats 42.5641 45 0 0.945869 5 llama-3.1-instruct:70:ggufv2:IQ4_XS stats 24 27 0 0.888889 3 llama-3.1-instruct:8:ggufv2:Q8_0 stats 18.6622 27 0 0.691194 3 llama-3.1-instruct:8:ggufv2:Q6_K stats 18.6622 27 0 0.691194 3 llama-3.1-instruct:8:ggufv2:IQ4_XS stats 18.3112 27 1.88853e-18 0.678192 3 llama-3.1-instruct:8:ggufv2:Q5_K_M stats 18.3058 27 0 0.677993 3 llama-3.1-instruct:70:ggufv2:Q3_K_S stats 18 27 0 0.666667 3 llama-3.1-instruct:70:ggufv2:IQ2_M stats 18 27 0 0.666667 3 llama-3.1-instruct:8:ggufv2:Q4_K_M stats 17.2171 27 0.033358 0.637672 3 llama-3.1-instruct:8:ggufv2:Q3_K_L stats 15.7067 27 0 0.581731 3 gpt-4-0125-preview stats 8.86667 45 0 0.197037 5 openhermes-2.5:7:ggufv2:Q8_0 stats 8.66667 45 0 0.192593 5 openhermes-2.5:7:ggufv2:Q6_K stats 8.66667 45 0 0.192593 5 openhermes-2.5:7:ggufv2:Q5_K_M stats 8.52821 45 0 0.189516 5 gpt-4-0613 stats 8.51282 45 0 0.189174 5 gpt-4o-2024-05-13 stats 8.51282 45 0 0.189174 5 openhermes-2.5:7:ggufv2:Q4_K_M stats 8.25641 45 0 0.183476 5 openhermes-2.5:7:ggufv2:Q3_K_M stats 8.05641 45 0 0.179031 5 openhermes-2.5:7:ggufv2:Q2_K stats 8 45 0 0.177778 5 gpt-3.5-turbo-0613 stats 7.98135 45 0 0.177363 5 gpt-3.5-turbo-0125 stats 7.12976 45 0 0.158439 5 mistral-instruct-v0.2:7:ggufv2:Q5_K_M stats 6.89091 45 0 0.153131 5 llama-2-chat:13:ggufv2:Q6_K stats 4.05299 27 2.26623e-17 0.150111 3 llama-2-chat:7:ggufv2:Q8_0 stats 3.87128 27 7.55411e-18 0.143381 3 mistral-instruct-v0.2:7:ggufv2:Q4_K_M stats 6.29908 45 0 0.13998 5 mistral-instruct-v0.2:7:ggufv2:Q6_K stats 6.18322 45 0 0.137405 5 llama-3-instruct:8:ggufv2:Q8_0 stats 5.17406 45 0 0.114979 5 mistral-instruct-v0.2:7:ggufv2:Q3_K_M stats 5.1041 45 0 0.113424 5 mistral-instruct-v0.2:7:ggufv2:Q8_0 stats 5.04591 45 0 0.112131 5 llama-2-chat:7:ggufv2:Q6_K stats 2.99816 27 7.55411e-18 0.111043 3 mistral-instruct-v0.2:7:ggufv2:Q2_K stats 4.63496 45 0 0.102999 5 llama-3-instruct:8:ggufv2:Q6_K stats 4.30739 45 0 0.0957198 5 llama-3-instruct:8:ggufv2:Q5_K_M stats 3.9346 45 0 0.0874356 5 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 stats 3.60737 45 0 0.0801638 5 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M stats 3.58841 45 0 0.0797425 5 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K stats 3.21213 45 0 0.0713807 5 llama-2-chat:70:ggufv2:Q4_K_M stats 3.08109 45 0 0.0684688 5 llama-3-instruct:8:ggufv2:Q4_K_M stats 2.98843 45 0 0.0664096 5 llama-2-chat:70:ggufv2:Q2_K stats 2.65216 45 0 0.0589368 5 llama-2-chat:70:ggufv2:Q3_K_M stats 2.44276 45 0 0.0542835 5 llama-2-chat:70:ggufv2:Q5_K_M stats 2.3993 45 0 0.0533177 5 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M stats 2.21549 45 0 0.049233 5 mixtral-instruct-v0.1:46_7:ggufv2:Q3_K_M stats 1.96241 45 0 0.0436091 5 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K stats 1.76057 45 0 0.0391237 5 llama-2-chat:7:ggufv2:Q4_K_M stats 1.43589 45 0 0.0319086 5 llama-2-chat:13:ggufv2:Q4_K_M stats 1.41695 45 0 0.0314878 5 llama-2-chat:13:ggufv2:Q8_0 stats 1.38608 45 0 0.0308019 5 llama-2-chat:7:ggufv2:Q5_K_M stats 1.3859 45 0 0.0307977 5 llama-2-chat:13:ggufv2:Q3_K_M stats 1.35834 45 0 0.0301854 5 llama-2-chat:13:ggufv2:Q5_K_M stats 1.3371 45 0 0.0297134 5 llama-2-chat:13:ggufv2:Q2_K stats 1.12439 45 0 0.0249865 5 code-llama-instruct:7:ggufv2:Q4_K_M stats 0.860471 45 0 0.0191216 5 llama-2-chat:7:ggufv2:Q3_K_M stats 0.804538 45 0 0.0178786 5 llama-2-chat:7:ggufv2:Q2_K stats 0.558031 45 0 0.0124007 5 chatglm3:6:ggmlv3:q4_0 stats 0.17925 45 0 0.00398332 5 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4-turbo-2024-04-09 99 100 0.447214 0.99 5 gpt-4o-mini-2024-07-18 98 100 0.547723 0.98 5 gpt-4o-2024-05-13 96 100 0.83666 0.96 5 <p></p>"},{"location":"benchmark/results/#medical-exam-question-answering","title":"Medical Exam Question Answering","text":"<p>In this set of tasks, we test LLM abilities to answer medical exam questions.</p> Overall Performance Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-08-06 806 948 2.73205 0.850211 3 gpt-4o-mini-2024-07-18 1352 1608 1.44215 0.840796 5 gpt-4-turbo-2024-04-09 1366 1632 3.231 0.83701 5 llama-3.1-instruct:70:ggufv2:IQ4_XS 765 930 0 0.822581 3 claude-3-opus-20240229 754 936 2.88675 0.805556 3 llama-3.1-instruct:70:ggufv2:Q3_K_S 732 915 0 0.8 3 gpt-4-0125-preview 837 1077 4.04145 0.777159 3 claude-3-5-sonnet-20240620 759 981 0 0.7737 3 llama-3.1-instruct:70:ggufv2:IQ2_M 684 885 0 0.772881 3 llama-3.1-instruct:8:ggufv2:Q3_K_L 657 855 0 0.768421 3 llama-3.1-instruct:8:ggufv2:Q8_0 657 858 0 0.765734 3 gpt-4o-2024-05-13 820 1074 4.04145 0.763501 3 llama-3.1-instruct:8:ggufv2:IQ4_XS 654 864 0 0.756944 3 llama-3.1-instruct:8:ggufv2:Q6_K 645 858 0 0.751748 3 llama-3.1-instruct:8:ggufv2:Q5_K_M 636 849 0 0.749117 3 llama-3.1-instruct:8:ggufv2:Q4_K_M 621 837 0 0.741935 3 gpt-4-0613 785 1074 3.4641 0.730912 3 gpt-3.5-turbo-0125 721 1074 7.50555 0.671322 3 llama-3-instruct:8:ggufv2:Q8_0 690 1077 0 0.640669 3 llama-3-instruct:8:ggufv2:Q5_K_M 684 1077 0 0.635097 3 llama-3-instruct:8:ggufv2:Q4_K_M 673 1077 0.57735 0.624884 3 llama-3-instruct:8:ggufv2:Q6_K 672 1077 0 0.623955 3 openhermes-2.5:7:ggufv2:Q4_K_M 628 1071 1.73205 0.586368 3 openhermes-2.5:7:ggufv2:Q8_0 618 1071 0 0.577031 3 openhermes-2.5:7:ggufv2:Q6_K 615 1071 0 0.57423 3 openhermes-2.5:7:ggufv2:Q5_K_M 612 1071 0 0.571429 3 openhermes-2.5:7:ggufv2:Q3_K_M 604 1071 1.73205 0.563959 3 openhermes-2.5:7:ggufv2:Q2_K 579 1074 0 0.539106 3 llama-2-chat:13:ggufv2:Q8_0 462 1071 0 0.431373 3 llama-2-chat:13:ggufv2:Q5_K_M 462 1071 0 0.431373 3 llama-2-chat:13:ggufv2:Q4_K_M 459 1071 0 0.428571 3 llama-2-chat:13:ggufv2:Q6_K 459 1071 0 0.428571 3 llama-2-chat:13:ggufv2:Q3_K_M 459 1071 0 0.428571 3 chatglm3:6:ggmlv3:q4_0 457 1071 21.6162 0.426704 3 llama-2-chat:13:ggufv2:Q2_K 444 1071 0 0.414566 3 llama-2-chat:7:ggufv2:Q6_K 435 1071 0 0.406162 3 llama-2-chat:7:ggufv2:Q8_0 429 1071 0 0.40056 3 llama-2-chat:7:ggufv2:Q5_K_M 429 1071 0 0.40056 3 llama-2-chat:7:ggufv2:Q4_K_M 429 1071 0 0.40056 3 llama-2-chat:7:ggufv2:Q3_K_M 423 1071 0 0.394958 3 llama-2-chat:7:ggufv2:Q2_K 396 1071 0 0.369748 3 mixtral-instruct-v0.1:46_7:ggufv2:Q4_K_M 395 1071 0.57735 0.368814 3 mistral-instruct-v0.2:7:ggufv2:Q8_0 393 1071 0 0.366947 3 mistral-instruct-v0.2:7:ggufv2:Q6_K 393 1071 0 0.366947 3 mistral-instruct-v0.2:7:ggufv2:Q4_K_M 391 1071 0.57735 0.365079 3 mistral-instruct-v0.2:7:ggufv2:Q5_K_M 390 1071 0 0.364146 3 mistral-instruct-v0.2:7:ggufv2:Q3_K_M 386 1071 0.57735 0.360411 3 mixtral-instruct-v0.1:46_7:ggufv2:Q8_0 384 1071 0 0.358543 3 mistral-instruct-v0.2:7:ggufv2:Q2_K 378 1071 0 0.352941 3 mixtral-instruct-v0.1:46_7:ggufv2:Q5_K_M 378 1071 0 0.352941 3 mixtral-instruct-v0.1:46_7:ggufv2:Q6_K 367 1071 0.57735 0.34267 3 mixtral-instruct-v0.1:46_7:ggufv2:Q2_K 353 1071 0.57735 0.329599 3"},{"location":"benchmark/results/#validation-requirements","title":"Validation Requirements","text":"<p>In this set of tasks, we evaluated LLMs based on five validation requirements. The tests address the topic of personalized interventional medicine in geroscience. The tasks were conducted both without and with retrieval-augmented generation (RAG).</p> <p>Without RAG:</p> ComprehensivenessCorrectnessUsefulnessInterpretability/ExplainabilityConsideration of Toxicity Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-11-20 3402 4000 0.06752 0.8505 4 o3-mini 2687 4000 0.06074 0.67175 4 deepseek-r1-distill-llama-70b 1943 4000 0.05637 0.48575 4 gpt-4o-mini-2024-07-18 1791 4000 0.18732 0.44775 4 qwen2.5-14b-instruct 1680.5 4000 0.20535 0.42012 4 llama-3.2-3b-instruct 1109 4000 0.0874 0.27725 4 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-11-20 2924 4000 0.01817 0.731 4 deepseek-r1-distill-llama-70b 2756 4000 0.02485 0.689 4 qwen2.5-14b-instruct 2718 4000 0.01095 0.6795 4 gpt-4o-mini-2024-07-18 2632 4000 0.02564 0.658 4 o3-mini 2616 4000 0.0338 0.654 4 llama-3.2-3b-instruct 2092 4000 0.09114 0.523 4 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-11-20 3566 4000 0.03465 0.8915 4 o3-mini 3147 4000 0.02609 0.78675 4 deepseek-r1-distill-llama-70b 2800.5 4000 0.05497 0.70013 4 gpt-4o-mini-2024-07-18 2764.5 4000 0.09091 0.69113 4 qwen2.5-14b-instruct 2342 4000 0.16641 0.5855 4 llama-3.2-3b-instruct 1746 4000 0.08907 0.4365 4 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-11-20 3748.5 4000 0.04068 0.93712 4 o3-mini 3341.5 4000 0.03216 0.83538 4 deepseek-r1-distill-llama-70b 3210.5 4000 0.04667 0.80263 4 gpt-4o-mini-2024-07-18 3088.5 4000 0.11361 0.77212 4 qwen2.5-14b-instruct 2611 4000 0.19511 0.65275 4 llama-3.2-3b-instruct 2143.5 4000 0.11997 0.53588 4 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-11-20 3952 4000 0.01251 0.988 4 gpt-4o-mini-2024-07-18 3886.5 4000 0.01239 0.97162 4 deepseek-r1-distill-llama-70b 3854.5 4000 0.01086 0.96362 4 o3-mini 3835 4000 0.01818 0.95875 4 llama-3.2-3b-instruct 3554.5 4000 0.04898 0.88862 4 qwen2.5-14b-instruct 3408.5 4000 0.18944 0.85212 4 <p>With RAG:</p> ComprehensivenessCorrectnessUsefulnessInterpretability/ExplainabilityConsideration of Toxicity Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-11-20 3031.5 4000 0.06086 0.75788 4 o3-mini 2758 4000 0.05266 0.6895 4 qwen2.5-14b-instruct 2225 4000 0.04408 0.55625 4 deepseek-r1-distill-llama-70b 2134.5 4000 0.02644 0.53362 4 gpt-4o-mini-2024-07-18 1813 4000 0.17479 0.45325 4 llama-3.2-3b-instruct 1019.5 4000 0.07838 0.25488 4 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-11-20 2908 4000 0.02197 0.727 4 qwen2.5-14b-instruct 2816 4000 0.02329 0.704 4 o3-mini 2720 4000 0.02574 0.68 4 deepseek-r1-distill-llama-70b 2708 4000 0.00447 0.677 4 gpt-4o-mini-2024-07-18 2686 4000 0.02632 0.6715 4 llama-3.2-3b-instruct 2536 4000 0.02426 0.634 4 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-11-20 3267 4000 0.03612 0.81675 4 o3-mini 3070.5 4000 0.03648 0.76762 4 qwen2.5-14b-instruct 2823.5 4000 0.02041 0.70588 4 deepseek-r1-distill-llama-70b 2760 4000 0.02968 0.69 4 gpt-4o-mini-2024-07-18 2581.5 4000 0.0879 0.64538 4 llama-3.2-3b-instruct 1842.5 4000 0.06809 0.46062 4 Full model name Score achieved Score possible Score SD Accuracy Iterations gpt-4o-2024-11-20 3483.5 4000 0.03944 0.87088 4 o3-mini 3288 4000 0.04035 0.822 4 deepseek-r1-distill-llama-70b 3170.5 4000 0.02518 0.79262 4 qwen2.5-14b-instruct 3136.5 4000 0.03515 0.78412 4 gpt-4o-mini-2024-07-18 2811.5 4000 0.09566 0.70288 4 llama-3.2-3b-instruct 2145 4000 0.08587 0.53625 4 Full model name Score achieved Score possible Score SD Accuracy Iterations o3-mini 3809.5 4000 0.0176 0.95238 4 gpt-4o-mini-2024-07-18 3729 4000 0.02935 0.93225 4 qwen2.5-14b-instruct 3725 4000 0.02091 0.93125 4 gpt-4o-2024-11-20 3724.5 4000 0.02567 0.93113 4 deepseek-r1-distill-llama-70b 3682 4000 0.02092 0.9205 4 llama-3.2-3b-instruct 3432 4000 0.03228 0.858 4"},{"location":"features/api/","title":"API Calling","text":""},{"location":"features/api/#overview","title":"Overview","text":"<p>BioChatter provides access to biological databases through chat via a custom API Agent. It is designed to interact with various external APIs and provides a structured approach to generating queries, fetching results, and interpreting the responses from different API services. In comparison to the tool binding approach via MCPs (In Chat Tool Calling), the API agent approach allows for more flexibility and in-depth manipulation of the bindings, queries and responses.</p> <pre><code>sequenceDiagram\n    participant User/Primary Agent\n    participant APIAgent\n    participant External Software\n\n    External Software -&gt;&gt; APIAgent: API definition\n    User/Primary Agent -&gt;&gt; APIAgent: question\n    APIAgent -&gt;&gt; APIAgent: parameterise API\n    APIAgent -&gt;&gt; APIAgent: generate API query\n    APIAgent -&gt;&gt; External Software: submit query (optional)\n    APIAgent -&gt;&gt; External Software: fetch result\n    External Software -&gt;&gt; APIAgent: return result\n    APIAgent -&gt;&gt; APIAgent: summarise / interpret (optional)\n    APIAgent -&gt;&gt; User/Primary Agent: return results</code></pre>"},{"location":"features/api/#example-oncokb-integration","title":"Example: OncoKB Integration","text":"<p>The OncoKB API integration allows users to query oncologically relevant genomic information. Here's a minimal working example:</p> <pre><code>from biochatter.llm_connect import GptConversation\nfrom biochatter.api_agent.base.api_agent import APIAgent\nfrom biochatter.api_agent.web.oncokb import OncoKBQueryBuilder, OncoKBFetcher, OncoKBInterpreter\n\n# Set up a conversation factory (you might need to adjust this based on your setup)\ndef conversation_factory():\n    return GptConversation(model_name=\"gpt-4\", prompts={}, correct=False)\n\n# Create an API agent for OncoKB\noncokb_agent = APIAgent(\n    conversation_factory=conversation_factory,\n    query_builder=OncoKBQueryBuilder(),\n    fetcher=OncoKBFetcher(),\n    interpreter=OncoKBInterpreter()\n)\n\n# Execute a query\nquestion = \"What is the oncogenic potential of BRAF V600E mutation?\"\nresult = oncokb_agent.execute(question)\n\nprint(result)\n</code></pre> <p>This example demonstrates how to set up and use the OncoKB API agent to query genomic information. The execute method handles the entire process of query generation, API interaction, and result interpretation.</p>"},{"location":"features/api/#core-components","title":"Core components","text":"<p>The API agent module is organized into three main submodules:</p> <ul> <li><code>base</code>: Contains core abstractions and base classes</li> <li><code>web</code>: Implementations for web-based APIs (e.g., OncoKB, BLAST)</li> <li><code>python</code>: Implementations for Python package APIs (e.g., Scanpy)</li> </ul>"},{"location":"features/api/#api-agent","title":"API Agent","text":"<p>The main class that orchestrates the API interaction process, located in <code>biochatter.api_agent.base.api_agent</code>.</p> <p>Key Methods:</p> <ul> <li> <p><code>execute(question: str) -&gt; Optional[str]</code>: Executes the full API interaction pipeline.</p> </li> <li> <p><code>parameterise_query(question: str) -&gt; Optional[BaseModel]</code>: Generates a structured query based on the input question.</p> </li> <li> <p><code>fetch_results(query_model: \"BaseModel\") -&gt; Optional[str]</code>: Retrieves results from the API.</p> </li> <li> <p><code>summarise_results(question: str, response_text: str) -&gt; Optional[str]</code>: Interprets and summarizes the API response with respect to the question.</p> </li> </ul>"},{"location":"features/api/#queryparameters","title":"QueryParameters","text":"<p>The <code>QueryParameters</code> class is a Pydantic model that defines the structure and validation rules for API query parameters. It serves several important purposes:</p> <ul> <li> <p>Structure Definition: It clearly defines all the parameters required for an API call, including their types and default values.</p> </li> <li> <p>Validation: Pydantic automatically validates the data, ensuring that all required fields are present and that the data types are correct.</p> </li> <li> <p>Documentation: Each field can include a description, providing clear documentation for developers and assisting the LLM in understanding how to use each parameter.</p> </li> <li> <p>Type Hinting: It provides type hints, improving code readability and enabling better IDE support.</p> </li> <li> <p>Serialization: The Pydantic model can easily be serialized to and deserialized from JSON, facilitating data transfer.</p> </li> </ul>"},{"location":"features/api/#base-classes","title":"Base Classes","text":"<p>Located in <code>biochatter.api_agent.base.agent_abc</code>:</p>"},{"location":"features/api/#basequerybuilder","title":"BaseQueryBuilder","text":"<p>Abstract base class for creating query builders specific to different APIs.</p> <p>Key Methods:</p> <ul> <li> <p><code>create_runnable(query_parameters: BaseModel, conversation: Conversation) -&gt; Callable</code>: Creates runnable object for executing queries.</p> </li> <li> <p><code>parameterise_query(question: str, conversation: Conversation) -&gt; BaseModel</code>: Generates a parameterized query object based on the input question.</p> </li> </ul>"},{"location":"features/api/#basefetcher","title":"BaseFetcher","text":"<p>An abstract base class for implementing API-specific data fetching logic.</p> <p>Key Method:</p> <ul> <li><code>fetch_results(query_model)</code>: Submits the query to the API and retrieves the results.</li> </ul>"},{"location":"features/api/#baseinterpreter","title":"BaseInterpreter","text":"<p>An abstract base class for implementing API-specific response interpretation logic.</p> <p>Key Method:</p> <ul> <li><code>summarise_results(question: str, conversation_factory: Callable, response_text: str) -&gt; str</code>: Summarizes and interprets the API response.</li> </ul>"},{"location":"features/api/#setting-up-a-new-api-integration","title":"Setting up a new API integration","text":""},{"location":"features/api/#overview_1","title":"Overview","text":"<p>Create specific implementations of QueryParameters, BaseQueryBuilder, BaseFetcher, and BaseInterpreter for your target API and design prompts. Instantiate an APIAgent with these components. Use the execute method of APIAgent to process questions and interact with the API.</p>"},{"location":"features/api/#1-specific-queryparameters","title":"1. Specific QueryParameters","text":"<p>For each field in your API call a Pydantic field is created. The description must be clear and concise to be understandable by a LLM so that it will fill the field with the appropriate arguments.</p> <pre><code>from pydantic import BaseModel, Field\n\nclass NewAPIQueryParameters(BaseModel):\n    endpoint: str = Field(..., description=\"API endpoint\")\n    param1: str = Field(..., description=\"Description of param1\")\n    param2: int = Field(default=0, description=\"Description of param2\")\n    # Add more parameters as needed\n</code></pre>"},{"location":"features/api/#2-prompt-design","title":"2. Prompt design","text":""},{"location":"features/api/#query_prompt-instructions-for-structured-output-to-write-newapi-call","title":"QUERY_PROMPT: instructions for structured output to write NewAPI call.","text":"<p>Create a prompt that will allow the LLM to obtain sufficient context to fill the Fields in the NewAPIQueryParameters class. Always keep the first two sentences. Secondly, provide instructions on how to interpret questions. Finally, for LLMs with large context windows the entire API documentation can be copy pasted inside of the prompt. Examples of API calls to retrieve the relevant information to a question should also be provided.</p> <pre><code>NewAPI_QUERY_PROMPT = \"\"\"\nYou are a world class algorithm for creating queries in structured formats. Your task is to use OncoKB Web APIs to answer genomic questions.\n\nAPI DOCUMENTATION AND EXAMPLES\"\"\"\n</code></pre>"},{"location":"features/api/#summary_prompt-instructions-on-how-to-interpret-and-answer-user-questions-based-on-retrieved-information","title":"SUMMARY_PROMPT: Instructions on how to interpret and answer user questions based on retrieved information","text":"<p>Follow prompt design below, replace NewAPI accordingly. If results are not satisfactory, engineer the prompts further.</p> <pre><code>NewAPI_SUMMARY_PROMPT = \"\"\"\nYou have to answer this question in a clear and concise manner: {question} Be factual!\\n\\\nYou are a world leading NewAPI domain specialist who knows everything about NewAPI results.\\n\\\nDo not make up information, only use the provided information and mention how relevant the found information is based on your knowledge about NewAPI\\n\\\nHere is the information relevant to the question found on NewAPI:\\n\\\n{context}\n\"\"\"\n</code></pre>"},{"location":"features/api/#3-implement-querybuilder","title":"3. Implement QueryBuilder","text":"<p>Create a class that inherits from BaseQueryBuilder:</p> <pre><code>from biochatter.api_agent.base.agent_abc import BaseQueryBuilder\n\nclass NewAPIQueryBuilder(BaseQueryBuilder):\n    def create_runnable(self,\n        query_parameters: NewAPIQueryParameters,\n        conversation: Conversation\n        ) -&gt; Callable:\n        # Implement method to create a runnable query object\n        return create_structured_output_runnable(\n            output_schema=query_parameters,\n            llm=conversation.chat,\n            prompt=self.structured_output_prompt,\n        )\n    def parameterise_query(self,\n        question: str,\n        conversation: Conversation\n        ) -&gt; NewAPIQueryParameters:\n        # Implement method to generate API parameters from a question\n        runnable = self.create_runnable(\n            query_parameters=NewAPIQueryParameters,\n            conversation=conversation,\n        )\n        NewAPI_call_obj = runnable.invoke(\n            {\"input\": f\"Answer:\\n{question} based on:\\n {NewAPI_QUERY_PROMPT}\"}\n        )\n       NewAPI_call_obj.question_uuid = str(uuid.uuid4())\n        return NewAPI_call_obj\n</code></pre>"},{"location":"features/api/#4-implement-fetcher","title":"4. Implement Fetcher","text":"<p>Create a class that inherits from BaseFetcher. Adapt request header in initiation if specific API tokens are required, and implement a logic to build the full URL required for the API call from NewAPIQueryParameters, and execute it.</p> <p>The <code>fetch_results</code> method should return the response text from the API. If this is a two-stage process (e.g., submitting a query and then retrieving the results), you should implement the necessary logic to handle this. You can look at the BLAST fetcher for an example (<code>blast.py</code>).</p> <p>It may be useful to tune the retries parameter to handle network issues or, as in the case of BLAST, the computational time required to process the query.</p> <p>NOTE: if the response is too large for your LLM context window you may have to reduce its size in some way.</p> <pre><code>from biochatter.api_agent.base.agent_abc import BaseFetcher\n\nclass NewAPIFetcher(BaseFetcher):\n        def __init__(self,):\n        self.headers = {\n        }\n        self.base_url = \"https://api.new.org/api/\"\n\n    def fetch_results(\n        self,\n        query_model: NewAPIQueryParameters,\n        retries: int = 3,\n    ) -&gt; str:\n    #implement your logic here\n    return results_response.text\n</code></pre>"},{"location":"features/api/#5-implement-interpreter","title":"5. Implement Interpreter","text":"<p>Create a class that inherits from BaseInterpreter and adapt the system prompt to NewAPI.</p> <pre><code>from biochatter.api_agent.base.agent_abc import BaseInterpreter\n\nclass NewAPIInterpreter(BaseInterpreter):\n    def summarise_results(self,\n        question: str,\n        conversation_factory: Callable,\n        response_text: str) -&gt; str:\n\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are a world NewAPI specialist who knows everything about NewAPI topic and its results.\",\n                ),\n                (\"user\", \"{input}\"),\n            ]\n        )\n        summary_prompt = NewAPI_SUMMARY_PROMPT.format(\n            question=question, context=response_text\n        )\n        output_parser = StrOutputParser()\n        conversation = conversation_factory()\n        chain = prompt | conversation.chat | output_parser\n        answer = chain.invoke({\"input\": {summary_prompt}})\n        return answer\n</code></pre>"},{"location":"features/api/#6-integrate-with-apiagent","title":"6. Integrate with APIAgent","text":"<p>Once you have implemented all components, you can use them with the APIAgent:</p> <pre><code>from biochatter.api_agent.base.api_agent import APIAgent\n\nnew_api_agent = APIAgent(\n    conversation_factory=your_conversation_factory,\n    query_builder=NewAPIQueryBuilder(),\n    fetcher=NewAPIFetcher(),\n    interpreter=NewAPIInterpreter()\n)\n\nresult = new_api_agent.execute(\"Your question here\")\n</code></pre>"},{"location":"features/benchmark/","title":"The BioChatter Living Benchmark","text":"<p>For trustworthy application of LLMs to real-world and biomedical problems, it is imperative to understand their performance and limitations. We need to constantly evaluate the multitude of combinations of individual models and versions, their parameters (e.g., temperature), prompt sets, databases and vector databases, and diverse application scenarios. To this end, we are maintaining a living benchmarking framework that allows us to continuously compare the performance of different models and configurations on a variety of tasks. The procedure of the living benchmark follows the idea of test-driven development, in that test cases are added with practical reference to a desired feature or behaviour. For cases where a model does not yield a perfect response out of the box, which will be most cases, modifications to the components of the framework, such as prompts or functions, are made to improve the model's performance. By tracking model performance on these tests over time, we can gain insights into the reliability of the framework and identify the gaps to be addressed.</p>"},{"location":"features/benchmark/#running-the-benchmark","title":"Running the benchmark","text":"<p>The benchmark uses the pytest framework to orchestrate the evaluation of a number of models on a number of tasks. The benchmark is run on a regular basis, and the results are published in the benchmark section. The benchmarking suite can be found in the <code>benchmark</code> directory of the BioChatter repository. It can be executed using standard pytest syntax, e.g., <code>poetry run pytest benchmark</code>. As default behavior it checks, which test cases have already been executed and only executes the tests that have not been executed yet. To run all benchmarks again, use <code>poetry run pytest benchmark --run-all</code>. If you want to develop and debug the benchmark instead of just running it, please refer to the developer docs.</p> <p>To allow flexible extension of the benchmark, we have implemeted a modular test framework that uses pytest fixtures to allow easy addition of new models and tasks. All setup is done in the <code>conftest.py</code> file in the <code>benchmark</code> directory. The benchmarks are organised by module, task, and subtask. Module names (e.g., <code>biocypher_query_generation</code> or <code>rag_interpretation</code>) group tasks by use case and are used to create benchmark dataset objects for the tests. Tasks (e.g., <code>entity_selection</code> or <code>query_generation</code>) are the actual tests that are executed in each module. The name of the test (minus the <code>test_</code> prefix) is used to generate the result file name. Subtasks (e.g., <code>single_word</code> or <code>multi_word</code>) are used to parametrise the tests and track the results of different subtasks in the result files. In addition, we generate md5 hashes of the test data for each subtask and use them to skip tests that have already been executed. Exemplary tests are defined in a YAML file in the <code>benchmark/data</code> directory, while the actual test dataset is encrypted and only available to the benchmarking pipeline. The result files are simple CSVs whose file names are generated from the name of the test function; they can be found in <code>benchmark/results</code> and contain scores for all executed combination of parameters.</p> <p>To achieve modularity, we use pytest fixtures and parametrization. For instance, to add a new model, we can modify the <code>OPENAI_MODEL_NAMES</code> and <code>XINFERENCE_MODELS</code> dictionary in <code>conftest.py</code>. The latter carries model names, sizes, format, and quantisations. The environment that runs the benchmark needs to make available all prerequisites for the different modules. For instance, the tasks requiring connection to an LLM need to provide the necessary credentials and API keys, or a connection to a self-hosted model. Likewise, the benchmarks of retrieval-augmented generation (RAG) processes require a connection to the RAG agent, e.g., a vector database.</p>"},{"location":"features/benchmark/#calibration","title":"Calibration","text":"<p>To ensure valid assessment of LLM performance, we need to ensure calibration and technical validity of the benchmarking framework. More recent LLMs in particular may be problematic when using publicly available benchmark datasets, since they could have been used for training the model. This is particularly relevant in closed-source (e.g., OpenAI) models. Thus, we need to consider strategies for avoiding contamination, such as hand-crafting datasets, carefully testing for contamination, or using perturbation strategies to generate new datasets from existing ones. Advanced scenarios could utilise LLMs as \"examiners,\" allowing more flexible test design and free-form answers. There is much research into these phenomena, all of which should be considered in the maintenance of this testing framework.</p>"},{"location":"features/benchmark/#aspects-of-benchmarking","title":"Aspects of benchmarking","text":"<p>In the following, we will detail the different aspects of benchmarking that we are currently testing. This is a living document that will be updated as we add new tests and test modules.</p>"},{"location":"features/benchmark/#models","title":"Models","text":"<p>Naturally the biggest impact on BioChatter performance comes with the model used. However, model versions can have a significant impact, which can be obfuscated by the fact that model names are often not unique. For instance, OpenAI's GPT models often have versions with significantly diverging capabilities and performance. Open-source models even come in a range of sizes and quantisations, which can have a significant impact on performance.</p>"},{"location":"features/benchmark/#prompts","title":"Prompts","text":"<p>As has been recently studied extensively, prompt engineering can make or break the performance of a model on a given task. As such, it is important to test the default prompts we commonly use, as well as a range of variations to determine factors of prompt performance and robustness. As an added complexity, LLMs are often used to generate prompts, which theoretically allows for procedural generation of an infinite number of prompts, as long as time and resources allow.</p>"},{"location":"features/benchmark/#model-parameters","title":"Model parameters","text":"<p>The parameters of the model can have a significant impact on the performance of the model. We often set model temperature to 0 to provide consistent results, but some applications may benefit from a higher temperature. In testing, we mostly rely on a temperature of 0 due to the complexity of testing highly variable results in most cases.</p>"},{"location":"features/benchmark/#databases","title":"Databases","text":"<p>An important facet of BioChatter and BioCypher is their combination in querying databases. This helps to ameliorate the limitations of LLMs by providing structured and validated knowledge to counteract hallucinations. To ensure the seamless interaction of BioChatter and BioCypher, we need to test the performance of BioChatter on a variety of databases.</p>"},{"location":"features/benchmark/#vector-databases","title":"Vector databases","text":"<p>Similarly to regular databases, vector databases are an important tool to provide validated knowledge to LLMs. Vector databases bring their own set of parameters and application scenarios, which likewise need to be tested. For instance, the length and overlap of fragments, the embedding algorithms, as well as the semantic search algorithms applied can have an impact on LLM conversation performance.</p>"},{"location":"features/benchmark/#tasks","title":"Tasks","text":"<p>There is a wide range of tasks that are potentially useful to BioChatter users. To cover most scenarios of research and development use, as well as clinical applications, we test a variety of tasks and LLM personas.</p>"},{"location":"features/chat/","title":"Basic Usage: Chat","text":"<p>BioChatter provides access to chat functionality via the <code>Conversation</code> class, which is implemented in several child classes (in the <code>llm_connect.py</code> module) to account for differences in APIs of the LLMs.</p>"},{"location":"features/chat/#setting-up-the-conversation","title":"Setting up the conversation","text":"<p>To start a conversation, we can initialise the Conversation class (here exemplified by GPT):</p> <pre><code>from biochatter.llm_connect import GptConversation\n\nconversation = GptConversation(\n    model_name=\"gpt-3.5-turbo\",\n    prompts={},\n)\nconversation.set_api_key(api_key=\"sk-...\")\n</code></pre> <p>The <code>set_api_key</code> method is needed in order to initialise the conversation for those models that require an API key (which is true for GPT).</p> <p>It is possible to supply a dictionary of prompts to the conversation from the outset, which is formatted in a way to correspond to the different roles of the conversation, i.e., primary and correcting models. Prompts with the <code>primary_model_prompts</code> key will be appended to the System Messages of the primary model, and <code>correcting_agent_prompts</code> will be appended to the System Messages of the correction model at setup. If we pass a dictionary without these keys (or an empty one), there will be no system messages appended to the models. They can however be introduced later by using the following method:</p> <pre><code>conversation.append_system_message(\"System Message\")\n</code></pre> <p>Similarly, the user queries (<code>HumanMessage</code>) are passed to the conversation using <code>conversation.append_user_message(\"User Message\")</code>. For purposes of keeping track of the conversation history, we can also append the model's responses as <code>AIMessage</code> using <code>conversation.append_ai_message</code>.</p>"},{"location":"features/chat/#querying-the-model","title":"Querying the model","text":"<p>After setting up the conversation in this way, for instance by establishing a flattery component (e.g. 'You are an assistant to a researcher ...'), the model can be queried using the <code>query</code> function.</p> <pre><code>msg, token_usage, correction = conversation.query('Question here')\n</code></pre> <p>Note that a query will automatically append a user message to the message history, so there is no need to call <code>append_user_message()</code> again. The query function returns the actual answer of the model (<code>msg</code>), the token usage statistics reported by the API (<code>token_usage</code>), and an optional <code>correction</code> that contains the opinion of the corrective agent.</p>"},{"location":"features/chat/#using-openai-models","title":"Using OpenAI models","text":"<p>Using an OpenAI model via the API is generally the easiest way to get started, but requires the provision of an API key to the OpenAI API. To do this, you can designate the <code>OPENAI_API_KEY</code> variable in your environment directly (<code>export OPENAI_API_KEY=sk-...</code>) by adding it to your shell configuration (e.g., the <code>zshrc</code>).</p>"},{"location":"features/chat/#using-anthropic-models-claude","title":"Using Anthropic models (Claude)","text":"<p>Similarly, to use an Anthropic model, you need a billable account with Anthropic API access, and to set the <code>ANTHROPIC_API_KEY</code> variable in your environment.</p> <pre><code>from biochatter.llm_connect import AnthropicConversation\n\nconversation = AnthropicConversation(\n    model_name=\"claude-3-5-sonnet-20240620\",\n    prompts={},\n)\n</code></pre>"},{"location":"features/chat/#using-google-deepmind-models-gemini","title":"Using Google DeepMind models (Gemini)","text":"<p>To use Google's Gemini models, you need a Google AI Studio API key. Set the  <code>GOOGLE_API_KEY</code> variable in your environment, or provide it directly when initializing the conversation. </p> <pre><code>from biochatter.llm_connect import GeminiConversation\n\nconversation = GeminiConversation(\n    model_name=\"gemini-2.0-flash\",\n    prompts={},\n)\n\nconversation.set_api_key(api_key=\"AIza...\")\n</code></pre> <p>Consider that Gemini models (at the time of writing) offer a free usage tier that could be useful for testing purposes. To get an API key, you can follow the instructions here.</p>"},{"location":"features/multimodal/","title":"Multimodal models - Text and image","text":"<p>We support multimodal queries in models that offer these capabilities after the blueprint of the OpenAI API. We can either add an image-containing message to the conversation using the <code>append_image_message</code> method, or we can pass an image URL directly to the <code>query</code> method:</p> <pre><code># Either: Append image message\nconversation.append_image_message(\n    message=\"Here is an attached image\",\n    image_url=\"https://example.com/image.jpg\"\n)\n\n# Or: Query with image included\nmsg, token_usage, correction = conversation.query(\n    \"What's in this image?\",\n    image_url=\"https://example.com/image.jpg\"\n)\n</code></pre>"},{"location":"features/multimodal/#using-local-images","title":"Using local images","text":"<p>Following the recommendations by OpenAI, we can pass local images as base64-encoded strings. We allow this by setting the <code>local</code> flag to <code>True</code> in the <code>append_image_message</code> method:</p> <pre><code>conversation.append_image_message(\n    message=\"Here is an attached image\",\n    image_url=\"my/local/image.jpg\",\n    local=True\n)\n</code></pre> <p>We also support the use of local images in the <code>query</code> method by detecting the netloc of the image URL. If the netloc is empty, we assume that the image is local and read it as a base64-encoded string:</p> <pre><code>msg, token_usage, correction = conversation.query(\n    \"What's in this image?\",\n    image_url=\"my/local/image.jpg\"\n)\n</code></pre>"},{"location":"features/multimodal/#open-source-multimodal-models","title":"Open-source multimodal models","text":"<p>While OpenAI models work seamlessly, open-source multimodal models can be buggy or incompatible with certain hardware. We have experienced mixed success with open models and, while they are technically supported by BioChatter, their outputs currently may be unreliable.</p>"},{"location":"features/open-llm/","title":"Open-source and Local LLMs","text":"<p>There are two self-hosted/local LLM solutions that BioChatter currently supports out-of-the-box.</p> <ul> <li>Xorbits Inference</li> <li>Ollama</li> </ul> <p>Below, we provide installation and usage instructions for both of them.</p>"},{"location":"features/open-llm/#xorbits-inference-xinference","title":"Xorbits Inference (Xinference)","text":"<p>Xorbits Inference is an open-source toolkit for running open-source models, particularly language models. To support BioChatter applications in local and protected contexts, we provide API access via BioChatter classes in a unified way. Briefly, this module allows to connect to any open-source model supported by Xinference via the state-of-the-art and easy-to-use OpenAI API. This allows local and remote access to essentially all relevant open-source models, including these builtin models, at very little setup cost.</p>"},{"location":"features/open-llm/#usage","title":"Usage","text":"<p>Usage is essentially the same as when calling the official OpenAI API, but uses the <code>XinferenceConversation</code> class under the hood. Interaction with the class is possible in the exact same way as with the standard class.</p>"},{"location":"features/open-llm/#connecting-to-the-model-from-biochatter","title":"Connecting to the model from BioChatter","text":"<p>All that remains once Xinference has started your model is to tell BioChatter the API endpoint of your deployed model via the <code>base_url</code> parameter of the <code>XinferenceConversation</code> class. For instance:</p> <pre><code>from biochatter.llm_connect import XinferenceConversation\n\nconversation = XinferenceConversation(\n    base_url=\"http://localhost:9997\",\n    prompts={},\n    correct=False,\n)\nresponse, token_usage, correction = conversation.query(\"Hello world!\")\n</code></pre>"},{"location":"features/open-llm/#deploying-locally-via-docker","title":"Deploying locally via Docker","text":"<p>We have created a Docker workflow that allows the deployment of builtin Xinference models, here. It will soon be available via Dockerhub. There is another workflow that allows mounting (potentially) any compatible model from HuggingFace, here. Note that, due to graphics driver limitations, this currently only works for Linux machines with dedicated Nvidia graphics cards. If you have a different setup, please check below for deploying Xinference without the Docker workflow.</p>"},{"location":"features/open-llm/#deploying-locally-without-docker","title":"Deploying locally without Docker","text":""},{"location":"features/open-llm/#installation","title":"Installation","text":"<p>To run Xinference locally on your computer or a workstation available on your network, follow the official instructions for your type of hardware. Briefly, this includes installing the <code>xinference</code> and <code>ctransformers</code> Python libraries into an environment of your choice, as well as a hardware-specific installation of the <code>llama-ccp-python</code> library.</p>"},{"location":"features/open-llm/#deploying-your-model","title":"Deploying your model","text":"<p>After installation, you can run the model (locally using <code>xinference</code> or in a distributed fashion. After startup, you can visit the local server address in your browser (standard is <code>http://localhost:9997</code>) and select and start your desired model. There is a large selection of predefined models to choose from, as well as the possibility to add your own favourite models to the framework. You will see your running models in the <code>Running Models</code> tab, once they have started.</p> <p>Alternatively, you can deploy (and query) your model via the Xinference Python client:</p> <pre><code>from xinference.client import Client\n\nclient = Client(\"http://localhost:9997\")\nmodel_uid = client.launch_model(model_name=\"chatglm2\")  # download model from HuggingFace and deploy\nmodel = client.get_model(model_uid)\n\nchat_history = []\nprompt = \"What is the largest animal?\"\nmodel.chat(\n    prompt,\n    chat_history,\n    generate_config={\"max_tokens\": 1024}\n)\n</code></pre>"},{"location":"features/open-llm/#ollama","title":"Ollama","text":"<p>Ollama is arguably the biggest open-source project for local LLM hosting right now. In comparison to Xinference it lacks the complete freedom of running any HuggingFace model in a simple fashion, but has the benefit of higher stability for the supported models. The list of supported models is updated diligently by the Ollama community. BioChatter support was added by implementing the LangChain ChatOllama and LangChain OllamaEmbeddings classes, connecting to Ollama APIs.</p>"},{"location":"features/open-llm/#usage_1","title":"Usage","text":"<p>Usage is essentially the same as when calling the official OpenAI API, but uses the <code>OllamaConversation</code> class under the hood. Interaction with the class is possible in the exact same way as with the standard class.</p>"},{"location":"features/open-llm/#connecting-to-the-model-from-biochatter_1","title":"Connecting to the model from BioChatter","text":"<p>Once Ollama has been set up (see below), you can directly use BioChatter to connect to the API endpoint and start any available model. It will be downloaded and launched on-demand. You can now configure the <code>OllamaConversation</code> instance setting the <code>base_url</code> and <code>model_name</code> parameters. For example:</p> <pre><code>from biochatter.llm_connect import OllamaConversation\n\nconversation = OllamaConversation(\n    base_url=\"http://localhost:11434\",\n    prompts={},\n    model_name='llama3',\n    correct=False,\n)\nresponse, token_usage, correction = conversation.query(\"Hello world!\")\n</code></pre>"},{"location":"features/open-llm/#deploying-locally-via-docker_1","title":"Deploying locally via Docker","text":"<p>To deploy Ollama with Docker is extremely easy and well documented. You can follow the official Ollama Docker blog post for that or check the Ollama DockerHub page that will also help you with the installation of the required <code>nvidia-container-toolkit</code> library if you want to use GPUs from Docker containers.</p>"},{"location":"features/open-llm/#deploying-locally-without-docker_1","title":"Deploying locally without Docker","text":""},{"location":"features/open-llm/#installation_1","title":"Installation","text":"<p>You can download and run Ollama also directly on your computer. For this you can just visit the official website that provides you with an installer for any OS. More info on the setup and startup process can be found in the GitHub README.</p>"},{"location":"features/podcast/","title":"Podcast my Paper","text":"<p>We provide a module to perform document processing and text-to-speech to enable listening to any document in podcast style.  The functionality can be accessed through the podcast API or by running the script <code>scripts/podcast_single_document.py</code>.</p> <p>Note</p> <p>This feature is currently not under active development. In particular, due to the more recent involvement of large corporations in similar technologies (e.g., Google's NotebookLM), we are currently not prioritising this feature.</p>"},{"location":"features/podcast/#api-access","title":"API access","text":"<p>The podcast API is available through the <code>podcast</code> module. An end-to-end workflow looks like this (modified from the test module):</p> <pre><code>from biochatter.podcast import Podcaster\nfrom biochatter.vectorstore import DocumentReader\n\n# Load document\nreader = DocumentReader()\ndocument = reader.load_document(\"test/dcn.pdf\")\n\n# Initialise podcaster\npodcaster = Podcaster(document)\n\n# Generate podcast (LLM task)\npodcaster.generate_podcast(characters_per_paragraph=5000)\n\n# Employ text-to-speech to generate audio file (optional)\npodcaster.podcast_to_file(\"test/test.mp3\", model=\"tts-1-hd\", voice=\"alloy\")\n</code></pre> <p>If you do not want audio output, you can simply access the generated text though the function <code>podcaster.podcast_to_text()</code>.</p> <p>This example uses the paid OpenAI text-to-speech API to generate the audio file. The default of the <code>podcast_to_file</code> function is to use the free Google text-to-speech API.  When using OpenAI, due to the input length limit of 4096 characters, the podcast is split into multiple parts indicated by integer suffixes.</p>"},{"location":"features/podcast/#command-line-access","title":"Command line access","text":"<p>To generate a podcast from a single document more quickly, you can use the <code>scripts/podcast_single_document.py</code> script.  It accepts two arguments, the path to the document and the path to the desired output file.  If the output file ends in <code>.mp3</code>, the OpenAI text-to-speech API will be used to generate an audio file.  Otherwise, the script will generate a text file and skip the text-to-speech step.  If using the OpenAI text-to-speech API, multiple files will be generated with integer suffixes.  If you installed BioChatter with poetry, you can run the script like this (from the root directory of the repository):</p> <pre><code>poetry run python scripts/podcast_single_document.py test/dcn.pdf test/test.mp3\n</code></pre>"},{"location":"features/rag/","title":"Retrieval-Augmented Generation","text":""},{"location":"features/rag/#overview","title":"Overview","text":"<p>To provide a unified interface for various ways of retrieval-augmented generation (RAG), we provide an implementation that connects to different types of databases and APIs.  These functions are provided by the <code>RagAgent</code> class, which encapsulates the connection, query, search, retrieval, and summarisation functions of the individual agents.</p> <pre><code>classDiagram\n    class RagAgent {\n        -mode: which tool to use, on usage see below\n        -model_name: LLM model, DEPRECATED, should use conversation_factory\n        -connection_args: connectivity to databases\n        -n_results: how many results to use for RAG\n        -use_prompt: whether to use custom prompt for tool, i.e., essentially turning off the RAG mechanism\n        -schema_config_or_info_dict: knowledge graph schema\n        -conversation_factory: function to create Conversation object, i.e., interact with specific LLM\n        -embedding_func: vector DB function to use for vector embeddings\n        -documentids_workspace: ID of vector database scope for search\n        -agent_desc: description of the RagAgent to be used for LLM-driven tool selection\n        +connect()\n        +get_query_results()\n        +similarity_search()\n        +get_description()\n    }\n\n    class DatabaseAgent {\n        -model_name\n        -connection_args\n        -schema_config_or_info_dict\n        -conversation_factory\n        +connect()\n        +get_query_results()\n    }\n\n    class VectorDatabaseAgentMilvus {\n        -embedding_func\n        -connection_args\n        +connect()\n        +similarity_search()\n    }\n\n    class APIAgent {\n        -BlastFetcher\n        -BlastInterpreter\n        -BlastQueryBuilder\n        +connect()\n        +get_query_results()\n    }\n\n    RagAgent --&gt; DatabaseAgent : uses when mode is 'kg'\n    RagAgent --&gt; VectorDatabaseAgentMilvus : uses when mode is 'vectorstore'\n    RagAgent --&gt; APIAgent : uses when mode is, e.g., 'api_blast'\n</code></pre> <p>Each instance of an agent is responsible for receiving a user's question, using the appropriate method to generate queries, fetching results, and returning them in a structured or summarised format to the agent that called it (often, the \"primary\" conversation between the user and an LLM). The decision on which LLM to use is made in supplying a <code>conversation_factory</code> function to the agent. For tool choice, in addition to manual selection, we can also use the <code>RagAgentSelector</code> class from <code>selector_agent.py</code> to automatically select the most appropriate tool based on the question and the description (<code>agent_desc</code>) of the available agents. For more information on the workflows used inside the individual agents, see the documentation for the Knowledge Graph-, Vector Store-, and API Agents.</p> <pre><code>sequenceDiagram\n    participant User/Primary Agent\n    participant RagAgent\n    participant DatabaseAgent/APIAgent\n\n    User/Primary Agent -&gt;&gt; RagAgent: question\n    RagAgent -&gt;&gt; RagAgent: mode select\n    RagAgent -&gt;&gt; DatabaseAgent/APIAgent: question\n    DatabaseAgent/APIAgent -&gt;&gt; DatabaseAgent/APIAgent: build query\n    DatabaseAgent/APIAgent -&gt;&gt; DatabaseAgent/APIAgent: fetch results\n    DatabaseAgent/APIAgent -&gt;&gt; DatabaseAgent/APIAgent: summarise (optional)\n    DatabaseAgent/APIAgent -&gt;&gt; RagAgent: results\n    RagAgent -&gt;&gt; RagAgent: summarise (optional)\n    RagAgent -&gt;&gt; User/Primary Agent: results</code></pre>"},{"location":"features/rag/#knowledge-graph-rag","title":"Knowledge Graph RAG","text":"<p>To increase accessibility of databases, we can leverage the BioCypher integration of BioChatter.  In BioCypher, we use a YAML configuration (<code>schema_config.yaml</code>) to specify the contents of the knowledge graph and their ontological associations.  We also generate a more extensive, but essentially similar YAML file during the BioCypher creation of a knowledge graph (<code>schema_info.yaml</code>), which contains more information pertinent to LLM interaction with the database.  The current prototypical implementation of query generation through an LLM is implemented in the <code>prompts.py</code> module on the example of a Neo4j knowledge graph connection.</p> <pre><code>sequenceDiagram\n    participant User/Primary Agent\n    participant DatabaseAgent\n    participant Knowledge Graph\n\n    User/Primary Agent -&gt;&gt; DatabaseAgent: question\n    Knowledge Graph -&gt;&gt; DatabaseAgent: schema information\n    DatabaseAgent -&gt;&gt; DatabaseAgent: select entities\n    DatabaseAgent -&gt;&gt; DatabaseAgent: select relationships\n    DatabaseAgent -&gt;&gt; DatabaseAgent: select properties\n    DatabaseAgent -&gt;&gt; DatabaseAgent: generate query\n    DatabaseAgent -&gt;&gt; Knowledge Graph: submit query\n    Knowledge Graph -&gt;&gt; DatabaseAgent: return results\n    DatabaseAgent -&gt;&gt; DatabaseAgent: summarise (optional)\n    DatabaseAgent -&gt;&gt; User/Primary Agent: return results</code></pre>"},{"location":"features/rag/#connecting","title":"Connecting","text":"<p>The database connectivity of BioChatter to BioCypher knowledge graphs is handled by the <code>DatabaseAgent</code> class (in <code>database_agent.py</code>). In addition, database connectivity can also be handled by the application that uses BioChatter, as shown in the example below. In that case, the application implements calls to the <code>BioCypherPromptEngine</code> class directly without going through the <code>DatabaseAgent</code>. To illustrate this simpler procedure, we provide an example of Neo4j query generation as used in the Pole Crime Dataset demo repository.</p>"},{"location":"features/rag/#querying","title":"Querying","text":"<p>The generation of a query based on BioCypher configuration files is a multi-step process. This is partly to account for the limited token input space of some models, and partly to better be able to test and compare the individual steps. The steps can also be wrapped in a single function, <code>generate_query</code>, which handles the entire process. In addition, we can use the Reflexion Agent to iteratively improve a query based on its results.</p>"},{"location":"features/rag/#setup","title":"Setup","text":"<p>We use the <code>BioCypherPromptEngine</code> class to handle the LLM conversation.</p> <pre><code>from biochatter.prompts import BioCypherPromptEngine\nprompt_engine = BioCypherPromptEngine(\n    schema_config_or_info_path=\"test/schema_info.yaml\"\n)\n</code></pre> <p>This will load the <code>schema_config.yaml</code> or <code>schema_info.yaml</code> (preferred) file and set up the conversation.</p>"},{"location":"features/rag/#query-generation","title":"Query generation","text":"<p>Using the <code>generate_query</code> wrapper, we can generate a query from a question and a database language.</p> <pre><code>query = prompt_engine.generate_query(\n    question=\"Which genes are associated with mucoviscidosis?\",\n    query_language=\"Cypher\",\n)\n</code></pre> <p>This will return a query that can be used in the database query language (e.g., Cypher). This end-to-end process executes the steps detailed below, namely, entity selection, relationship selection, and property selection, as well as the generation of the final query using the selected components. While you can run each of these steps individually, you need to ensure that the process is run in the correct order, passing the results of each step to the next step as implemented in the <code>generate_query</code> wrapper. The individual steps below are provided for explanation and testing purposes.</p>"},{"location":"features/rag/#entity-selection","title":"Entity selection","text":"<p>Starting from the <code>schema_config.yaml</code> or <code>schema_info.yaml</code> (preferred) file, we first have the model decide which entities in the database are relevant to the user's question.</p> <pre><code>success = prompt_engine._select_entities(\n    question=\"Which genes are associated with mucoviscidosis?\"\n)\n</code></pre> <p>This will select a number of entities from the database schema to be used subsequently, and return True or False to indicate success.</p>"},{"location":"features/rag/#relationship-selection","title":"Relationship selection","text":"<p>Next, we will use the entities determined in the first step to select relationships between them.  The entities selected in the first step will be stored in the <code>selected_entities</code> attribute of the <code>BioCypherPromptEngine</code> instance, and the question is stored in the <code>question</code> attribute.  Both are automatically used to select relationships.</p> <pre><code>success = prompt_engine._select_relationships()\n</code></pre>"},{"location":"features/rag/#property-selection","title":"Property selection","text":"<p>To not unnecessarily waste token input space, we are only interested in selecting properties of entities that are of interest given the question asked. We do so in the third step, which uses the entities and relationships determined in the first steps.  Again, <code>question</code>, <code>selected_entities</code>, and <code>selected_relationships</code> are automatically used to select properties.</p> <pre><code>success = prompt_engine._select_properties()\n</code></pre> <p>This will select a number of properties to be used in the query, and also return True or False to indicate success.</p>"},{"location":"features/rag/#query-generation_1","title":"Query generation","text":"<p>Finally, we can use the entities and relationships, as well as the selected properties, to ask the LLM to generate a query in the desired language.</p> <pre><code>query = prompt_engine._generate_query(\n    question=\"Which genes are associated with mucoviscidosis?\",\n    entities=[\"Gene\", \"Disease\"],\n    relationships=[\"GeneToDiseaseAssociation\"],\n    properties={\"Disease\": [\"name\", \"ICD10\", \"DSM5\"]},\n    database_language=\"Cypher\",\n)\n</code></pre> <p>This will (hopefully) return a query that can be used in the database query language (e.g., Cypher).</p>"},{"location":"features/rag/#query-interaction","title":"Query interaction","text":"<p>As an optional follow-up, you can interact with the returned query using the <code>BioCypherQueryHandler</code> class (<code>query_interaction.py</code>). It takes the query, the original question and the KG information so that the interaction is still aware of the KG.</p> <pre><code>from biochatter.query_interaction import BioCypherQueryHandler\nquery_handler = BioCypherQueryHandler(\n    query=query,\n    query_lang=\"Cypher\",\n    kg_selected={\n        entities: [\"Gene\", \"Disease\"],\n        relationships: [\"GeneToDiseaseAssociation\"],\n        properties: {\"Disease\": [\"name\", \"ICD10\", \"DSM5\"]}\n    },\n    question=\"Which genes are associated with mucoviscidosis?\"\n)\n</code></pre>"},{"location":"features/rag/#explanation","title":"Explanation","text":"<p>You can retrieve an explanation of the returned query with:</p> <pre><code>explanation = query_handler.explain_query()\n</code></pre>"},{"location":"features/rag/#updating","title":"Updating","text":"<p>Alternatively, you can ask the LLM for an update of the query with:</p> <pre><code>request = \"Only return 10 results and sort them alphabetically\"\nexplanation = query_handler.update_query(request)\n</code></pre> <p>NB: for updates, it might sometimes be relevant that all the KG enitites/relationships/properties are known to the LLM instead of only those that were selected to be relevant for the original question. For this, you can optionally pass them as input to the query handler with <code>kg</code> (similar to <code>kg_selected</code>).</p> <p>(Tip: the prompt_engine object contains both the selected and non-selected entities, relationships, and properties as attributes.)</p>"},{"location":"features/rag/#vectorstore-rag","title":"Vectorstore RAG","text":"<p>To connect to a vector database for using semantic similarity search and retrieval-augmented generation (RAG), we provide an implementation that connects to a Milvus instance (local or remote).  These functions are provided by the modules <code>vectorstore.py</code> (for performing embeddings) and <code>vectorstore_agent.py</code> (for maintaining the connection and search).</p> <p>This is implemented in the BioChatter Light Docker workflow and the BioChatter Docker compose found in this repository.  To start Milvus on its own in these repositories, you can call <code>docker compose up -d standalone</code> (<code>standalone</code> being the Milvus endpoint, which starts two other services alongside it).</p> <pre><code>sequenceDiagram\n    participant User/Primary Agent\n    participant VectorDatabaseAgent\n    participant Vector Database\n    participant Documents\n\n    Documents -&gt;&gt; Vector Database: embed text fragments\n    User/Primary Agent -&gt;&gt; VectorDatabaseAgent: question\n    VectorDatabaseAgent -&gt;&gt; VectorDatabaseAgent: generate artificial answer (optional)\n    VectorDatabaseAgent -&gt;&gt; VectorDatabaseAgent: embed question or artificial answer\n    VectorDatabaseAgent -&gt;&gt; Vector Database: submit search query embedding\n    Vector Database -&gt;&gt; VectorDatabaseAgent: return most similar embedded fragments\n    VectorDatabaseAgent -&gt;&gt; VectorDatabaseAgent: summarise (optional)\n    VectorDatabaseAgent -&gt;&gt; User/Primary Agent: return results</code></pre>"},{"location":"features/rag/#connecting_1","title":"Connecting","text":"<p>To connect to a vector DB host, we can use the corresponding class:</p> <pre><code>from biochatter.vectorstore_agent import VectorDatabaseAgentMilvus\n\ndbHost = VectorDatabaseAgentMilvus(\n    embedding_func=GoogleGenerativeAIEmbeddings(),\n    connection_args={\"host\": _HOST, \"port\": _PORT},\n    embedding_collection_name=EMBEDDING_NAME,\n    metadata_collection_name=METADATA_NAME\n)\ndbHost.connect()\n</code></pre> <p>This establishes a connection with the vector database (using a host IP and port) and uses two collections, one for the embeddings and one for the metadata of embedded text (e.g. the title and authors of the paper that was embedded).</p>"},{"location":"features/rag/#embedding-documents","title":"Embedding documents","text":"<p>To embed text from documents, we use the LangChain and BioChatter functionalities for processing and passing the text to the vector database.</p> <pre><code>from biochatter.vectorstore import DocumentReader()\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# read and split document at `pdf_path`\nreader = DocumentReader()\ndocs = reader.load_document(pdf_path)\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap,\n    separators=[\" \", \",\", \"\\n\"],\n)\nsplit_text = text_splitter.split_documents(docs)\n\n# embed and store embeddings in the connected vector DB\ndoc_id = dbHost.store_embeddings(splitted_docs)\n</code></pre> <p>The dbHost class takes care of calling an embedding model, storing the embedding in the database, and returning a document ID that can be used to refer to the stored document.</p>"},{"location":"features/rag/#semantic-search","title":"Semantic search","text":"<p>To perform a semantic similarity search, all that is left to do is pass a question or statement to the <code>dbHost</code>, which will be embedded and compared to the present embeddings, returning a number <code>k</code> most similar text fragments.</p> <pre><code>results = dbHost.similarity_search(\n    query=\"Semantic similarity search query\",\n    k=3,\n)\n</code></pre>"},{"location":"features/rag/#vectorstore-management","title":"Vectorstore management","text":"<p>Using the collections we created at setup, we can delete entries in the vector database using their IDs. We can also return a list of all collected docs to determine which we want to delete.</p> <pre><code>docs = dbHost.get_all_documents()\nres = dbHost.remove_document(docs[0][\"id\"])\n</code></pre>"},{"location":"features/rag/#api-calling","title":"API Calling","text":"<p>API calling functionality now has its own docs page.</p>"},{"location":"features/reflexion-agent/","title":"Reflexion Agent","text":"<p>While current LLMs have many capabilities, their outputs can be unstable at times. To stabilise responses and allow more complex agent workflows, we have introduced a Reflexion Agent, allowing agents to reflect on their experiences, score their output, and self-improve.</p>"},{"location":"features/reflexion-agent/#workflow","title":"Workflow","text":"<p>The workflow of a Reflexion Agent is composed of individual nodes that can either generate or consume data as follows:</p> <pre><code>flowchart TD\n    START --&gt; Draft\n    Draft --&gt; ExecuteTool\n    ExecuteTool --&gt; Revise\n    Revise --&gt; ShouldContinue?\n    ShouldContinue? --&gt;|no| END\n    ShouldContinue? --&gt;|yes| ExecuteTool</code></pre> Draft: in this node, an LLM is initially prompted to generate a specific text and action. (mem &lt;- AIMessage(...)). Execute tool: this node executes a tool function based on an action/text generated in the previous node. (mem &lt;- ToolMessage(...)). Revise: this node scores the output of the tool call and generates a self-reflection to provide feedback aimed at improving the results. (mem &lt;- AIMessage(...)). Evaluate (ShouldContinue): this node assesses the quality of the generated outputs. Memory (mem): a list of BaseMessage."},{"location":"features/reflexion-agent/#usage","title":"Usage","text":"<p>The <code>ReflexionAgent</code> class can be used to implement a reflexion workflow. Here, we demonstrate the ability to generate Cypher queries based on user's question using the <code>KGQueryReflexionAgent</code> class, which is derived from the abstract <code>ReflexionAgent</code> base class.</p> <p>To use the <code>KGQueryReflexionAgent</code>:</p> <ol> <li>We pass in connection arguments that enable connection to the target graph database and a conversation factory, which can create an instance of GptConversation (see Basic Usage: Chat).</li> </ol> <pre><code>import os\nfrom biochatter.llm_connect import GptConversation\nfrom biochatter.kg_langgraph_agent import KGQueryReflexionAgent\ndef create_conversation():\n    conversation = GptConversation(model_name=\"gpt-3.5-turbo\", prompts={})\n    conversation.set_api_key(os.getenv(\"OPENAI_API_KEY\"), user=\"my_user\")\n    return conversation\n\nconnection_args = {\n    \"host\": \"127.0.0.1\",\n    \"port\": \"7687\",\n}\n\nagent = KGQueryReflexionAgent(\n    connection_args=connection_args,\n    conversation_factory=create_conversation,\n)\n</code></pre> <ol> <li>We generate the basic Knowledge Graph prompt for the LLM based on a user's question with the <code>BioCypherPromptEngine</code>, which provides node info, edge info, and node and edge properties based on the user's question.</li> </ol> <pre><code>from biochatter.prompts import BioCypherPromptEngine\nprompt_engine = BioCypherPromptEngine(\n    model_name=\"gpt-3.5-turbo\",\n    schema_config_or_info_dict=schema_dict,  # the schema definition of our graph\n    conversation_factory=create_conversation,\n)\nkg_prompt = prompt_engine.generate_query_prompt(question)\n</code></pre> <ol> <li>We can now use the agent to generate and reflect on the Cypher query and optimise it.</li> </ol> <pre><code>cypher_query = agent.execute(question, kg_prompt)\n</code></pre>"},{"location":"features/reflexion-agent/#implementation","title":"Implementation","text":"<p>To use the <code>ReflexionAgent</code> class, we need to implement the following abstract methods:</p> <ol> <li> <p>_tool_function(self, state: List[BaseMessage]): execute tool function based on previous action/text and return ToolMessage</p> </li> <li> <p>_create_initial_responder(self, prompt: Option[str]): create draft responder, which is used to generate the initial answer</p> </li> <li> <p>_create_revise_responder(self, prompt: Optional[str]): create revise responder, which is used to score outputs and revise the answers</p> </li> <li> <p>_log_step_message(self, step: int, node: str, output: BaseMessage): parse step message and generate logs</p> </li> <li> <p>_log_final_result(self, output: BaseMessage): parse final result and generate logs</p> </li> <li> <p>_parse_final_result(self, output: BaseMessage): parse final result</p> </li> </ol> <p>As an example, we use the <code>kg_langgraph_agent.py</code> implementation that can reflect on the task of generating a knowledge graph query. The <code>KGQueryReflexionAgent</code> derived from <code>ReflexionAgent</code> is the main class to perform this task. In the <code>KGQueryReflexionAgent</code>, we have implemented the abstract methods described above:</p> <ol> <li> <p>_tool_function(self, state: List[BaseMessage]): connect to kg database and query KG in draft/revise node</p> </li> <li> <p>_create_initial_responder(self, prompt: Option[str]): create initial responder, which prompts LLM to generate the query</p> </li> </ol> <p>initial prompts: <pre><code>(\n    \"system\",\n    (\n        \"As a senior biomedical researcher and graph database expert, \"\n        f\"your task is to generate '{query_lang}' queries to extract data from our graph database based on the user's question. \"\n        \"\"\"Current time {time}. {instruction}\"\"\"\n    ),\n),\n(\n    \"system\",\n    \"Only generate query according to the user's question above.\",\n),\n</code></pre></p> <p>Initial answer schema:</p> <pre><code>class GenerateQuery(BaseModel):\n    \"\"\"Generate the query.\"\"\"\n\n    answer: str = Field(\n        description=\"Cypher query for graph database according to user's question.\"\n    )\n    reflection: str = Field(\n        description=\"Your reflection on the initial answer, critique of what to improve\"\n    )\n    search_queries: List[str] = Field(description=\"query for graph database\")\n</code></pre> <ol> <li>_create_revise_responder(self, prompt: Optional[str]): create revise responder, which prompts LLM to score the outputs, reflects on the outputs, and revises the current query</li> </ol> <p>Revise prompts:</p> <pre><code>\"\"\"\nRevise your previous query using the query result and follow the guidelines:\n1. If you consistently obtain empty results, please consider removing constraints such as relationship constraints to try to obtain a result.\n2. You should use previous critique to improve your query.\n3. Only generate a query without returning any other text.\n\"\"\"\n</code></pre> <p>Revise answer schema:</p> <pre><code>class ReviseQuery(GenerateQuery):\n    \"\"\"Revise your previous query according to your question.\"\"\"\n\n    revised_query: str = Field(description=REVISED_QUERY_\"Revised query\"DESCRIPTION)\n    score: str = Field(description=(\n    \"the score for the query based on its query result\"\n    \" and relevance to the user's question,\"\n    \" with 0 representing the lowest score and 10 representing the highest score.\"))\n</code></pre> <ol> <li> <p>_log_step_message(self, step: int, node: str, output: BaseMessage): parse message from current step and generate logs</p> </li> <li> <p>_log_final_result(self, output: BaseMessage): parse final result and generate logs</p> </li> <li> <p>_parse_final_result(self, output: BaseMessage): parse final result</p> </li> <li> <p>_should_continue(self, state: List[BaseMessage]): assess output and determine if we can exit loop based on the following rules:   1). if loop steps are greater than limit (30 or user defined), exit   2). if score in previous revise node is greater than 7, exit   3). if query result in execute_tool node is not empty, exit</p> </li> </ol>"},{"location":"features/structured_outputs/","title":"Structured Outputs","text":""},{"location":"features/structured_outputs/#overview","title":"Overview","text":"<p>BioChatter can return responses from the LLM in a predefined structure, rather than just plain text. This is particularly useful when you need the model's output to conform to a specific schema, making it easier to parse and use in downstream tasks. This is achieved by providing a Pydantic model to the conversation.</p> <p>Many modern LLMs (especially those from providers like OpenAI, Google, Anthropic) can natively generate outputs that conform to a provided schema. For models that do not natively support structured output, BioChatter attempts to guide the model by appending instructions to the prompt, asking it to generate a JSON object matching the schema.</p>"},{"location":"features/structured_outputs/#defining-a-structure-pydantic-model","title":"Defining a Structure (Pydantic Model)","text":"<p>To define the desired output structure, you use Pydantic's <code>BaseModel</code>. This allows you to specify fields, types, and even validation rules for the data you expect from the LLM.</p> <p>Here's an example of a Pydantic model for gene information:</p> <pre><code>from pydantic import BaseModel\n\nclass GeneInfo(BaseModel):\n    gene_symbol: str\n    full_name: str\n    summary: str\n    chromosome_location: str | None = None # Optional field\n</code></pre> <p>This <code>GeneInfo</code> model tells the LLM to provide information about a gene, including its <code>gene_symbol</code> (string), <code>full_name</code> (string), a <code>summary</code> of its function (string), and an optional <code>chromosome_location</code> (string).</p>"},{"location":"features/structured_outputs/#requesting-structured-output","title":"Requesting Structured Output","text":"<p>You can request structured output by passing your Pydantic model to the <code>structured_model</code> parameter in the <code>query</code> method of the <code>LangChainConversation</code> class.</p> <p>Here's how you can set up a conversation and request a structured response for gene information:</p> <pre><code>from biochatter.llm_connect import LangChainConversation\nfrom pydantic import BaseModel\n\n# Define your Pydantic model\nclass GeneInfo(BaseModel):\n    gene_symbol: str\n    full_name: str\n    summary: str\n    chromosome_location: str | None = None\n\n# Initialize the conversation\nconvo = LangChainConversation(\n    model_provider=\"google_genai\",  # Or any other supported provider\n    model_name=\"gemini-2.0-flash\", # Ensure model supports structured output or use fallback\n    prompts={},\n)\n\n# Set API key if required\nconvo.set_api_key()\n\n# Make the query, passing the Pydantic model\nquestion = \"Provide information about the human gene TP53, including its full name and a summary of its function.\"\nconvo.query(question, structured_model=GeneInfo)\n\n# Access the structured output\n# The last AI message will contain the JSON string of the structured output\nstructured_response_json = convo.messages[-1].content\nprint(structured_response_json)\n\n# You can then parse this JSON string back into your Pydantic model\nimport json\ngene_data = json.loads(structured_response_json)\nmy_gene_info = GeneInfo(**gene_data)\nprint(f\"Gene Symbol: {my_gene_info.gene_symbol}\")\nprint(f\"Full Name: {my_gene_info.full_name}\")\nprint(f\"Summary: {my_gene_info.summary}\")\nif my_gene_info.chromosome_location:\n    print(f\"Location: {my_gene_info.chromosome_location}\")\n</code></pre> <p>If the LLM natively supports structured outputs (e.g., newer OpenAI, Google models), the <code>AIMessage</code> content will typically be a JSON string representation of your Pydantic model. You can then parse this string to get an instance of your model.</p>"},{"location":"features/structured_outputs/#current-limitations","title":"Current Limitations","text":""},{"location":"features/structured_outputs/#no-simultaneous-tools-and-structured-output","title":"No simultaneous Tools and Structured Output","text":"<p>Currently, you cannot use tools (as described in <code>tool_chat.md</code>) and request structured  output by passing both <code>tools</code> and <code>structured_model</code> arguments in the same <code>query()</code>  call. An attempt to do so will raise a <code>ValueError</code>.</p>"},{"location":"features/structured_outputs/#sequential-use-is-possible","title":"Sequential use is possible","text":"<p>However, you can use tools and structured outputs sequentially. This is a powerful  pattern where initial queries can use tools to fetch or compute data, and subsequent  queries can process and structure this data using Pydantic models.</p> <p>Here's a more detailed biological example:</p> <p>Scenario: we want to find the ChEMBL ID for a drug, then get its mechanisms of action and putative targets, and finally structure this information.</p> <p>The process involves several steps:</p> <ol> <li>Defining tools to fetch data from external APIs (ChEMBL and OpenTargets).</li> <li>Defining Pydantic models to specify the desired structure for our final output.</li> <li>Initializing the BioChatter conversation.</li> <li>Making sequential queries: first to get the ChEMBL ID, then to get mechanisms of action, and finally to structure all gathered information.</li> <li>Parsing and displaying the structured output.</li> </ol> <p>Let's look at the code for each step.</p>"},{"location":"features/structured_outputs/#step-1-define-the-tools","title":"Step 1: define the tools","text":"<p>First, we define two tools. These tools will interact with external APIs to fetch the data we need:</p> <ul> <li><code>get_chembl_id</code>: This tool takes a drug name and returns its ChEMBL ID.</li> <li><code>get_mechanisms_of_action</code>: This tool takes a ChEMBL ID and retrieves the drug's mechanisms of action and target information from the OpenTargets API.</li> </ul> <p><pre><code>import requests\nfrom biochatter.llm_connect import LangChainConversation\nfrom pydantic import BaseModel, Field\nfrom langchain_core.tools import tool\nfrom typing import List, Dict, Any, Optional\nimport json\nfrom pprint import pprint\n\n# Tool to get ChEMBL ID\n@tool\ndef get_chembl_id(drug_name: str) -&gt; str:\n    \"\"\"\n    Given a drug name, look up and return its ChEMBL ID using the ChEMBL API.\n    Example: get_chembl_id(drug_name=\"aspirin\")\n    \"\"\"\n    url = \"https://www.ebi.ac.uk/chembl/api/data/chembl_id_lookup/search.json\"\n    params = {\"q\": drug_name}\n    headers = {\"Accept\": \"application/json\"}\n    try:\n        resp = requests.get(url, params=params, headers=headers, timeout=10)\n        resp.raise_for_status()\n        data = resp.json()\n        hits = data.get(\"chembl_id_lookups\", [])\n        if not hits:\n            return f\"Unable to find ChEMBL ID for {drug_name}\"\n        return hits[0].get(\"chembl_id\", f\"No ChEMBL ID found in hit for {drug_name}\")\n    except requests.RequestException as e:\n        return f\"Error querying ChEMBL API for {drug_name}: {str(e)}\"\n    except ValueError: # If JSON parsing fails\n        return f\"Invalid JSON received from ChEMBL API for {drug_name}\"\n# GraphQL query for OpenTargets\n_GRAPHQL_QUERY = \"\"\"\nquery MechanismsOfActionSectionQuery($chemblId: String!) {\n  drug(chemblId: $chemblId) {\n    id\n    mechanismsOfAction {\n      rows {\n        mechanismOfAction\n        targetName\n        targets {\n          id\n          approvedSymbol\n        }\n      }\n    }\n  }\n}\n\"\"\"\n\n@tool\ndef get_mechanisms_of_action(chembl_id: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Fetch mechanisms of action and target information for a given drug (by ChEMBL ID)\n    using the OpenTargets GraphQL API.\n    Example: get_mechanisms_of_action(chembl_id=\"CHEMBL25\")\n    \"\"\"\n    endpoint = \"https://api.platform.opentargets.org/api/v4/graphql\"\n    payload = {\"query\": _GRAPHQL_QUERY, \"variables\": {\"chemblId\": chembl_id}}\n    try:\n        resp = requests.post(endpoint, json=payload, headers={\"Content-Type\": \"application/json\"}, timeout=15)\n        resp.raise_for_status()\n        data = resp.json()\n        if \"errors\" in data:\n            return {\"error\": f\"GraphQL errors for {chembl_id}: {data['errors']}\"}\n        return data.get(\"data\", {})\n    except requests.RequestException as e:\n        return {\"error\": f\"Error querying OpenTargets API for {chembl_id}: {str(e)}\"}\n    except ValueError: # If JSON parsing fails\n        return {\"error\": f\"Invalid JSON received from OpenTargets API for {chembl_id}\"}\n</code></pre> What we obtain: we now have two functions, <code>get_chembl_id</code> and <code>get_mechanisms_of_action</code>, decorated with <code>@tool</code>. These are ready to be used by the BioChatter conversation object to perform their respective tasks.</p>"},{"location":"features/structured_outputs/#step-2-define-the-pydantic-model-for-structured-output","title":"Step 2: define the Pydantic model for structured output","text":"<p>Next, we define the Pydantic models that will determine the structure of our final output. This ensures the LLM returns the information in a consistent and parsable format. -   <code>TargetDetail</code>: Represents details of a biological target. -   <code>ActionMechanism</code>: Describes a mechanism of action, including its targets. -   <code>DrugTargetsOutput</code>: The main model that aggregates all information: the queried drug name, its ChEMBL ID, a list of its mechanisms and targets, and any potential error messages.</p> <p><pre><code>class TargetDetail(BaseModel):\n    approved_symbol: Optional[str] = Field(None, description=\"The approved symbol of the target (e.g., gene symbol).\")\n    target_id: Optional[str] = Field(None, description=\"The ID of the target (e.g., Ensembl ID).\")\n\nclass ActionMechanism(BaseModel):\n    mechanism_of_action: Optional[str] = Field(None, description=\"Description of the mechanism of action.\")\n    target_name: Optional[str] = Field(None, description=\"The name of the target associated with this mechanism.\")\n    targets: List[TargetDetail] = Field(default_factory=list, description=\"List of specific targets involved in this mechanism.\")\n\nclass DrugTargetsOutput(BaseModel):\n    drug_name_queried: str = Field(description=\"The original drug name queried.\")\n    chembl_id_found: Optional[str] = Field(None, description=\"The ChEMBL ID found for the drug.\")\n    mechanisms_and_targets: List[ActionMechanism] = Field(default_factory=list, description=\"List of mechanisms of action and associated targets.\")\n    error_message: Optional[str] = Field(None, description=\"Any error message encountered during the process.\")\n</code></pre> What we obtain: we have defined Python classes (<code>TargetDetail</code>, <code>ActionMechanism</code>, <code>DrugTargetsOutput</code>) that specify the exact schema we want our final data to conform to.</p>"},{"location":"features/structured_outputs/#step-3-initialize-the-conversation","title":"Step 3: initialize the conversation","text":"<p>Now, we initialize the <code>LangChainConversation</code>. This object will manage the interaction with the LLM, including tool usage and structured output requests. <pre><code># Initialize the conversation\nconvo = LangChainConversation(\n    model_provider=\"google_genai\", # Or any other supported provider\n    model_name=\"gemini-2.0-flash\", # Or a model suitable for your provider\n    prompts={} # Using default prompts\n)\nconvo.set_api_key() # Uncomment and set if your provider requires an API key directly\n</code></pre> What we obtain: a <code>convo</code> object is ready to interact with the specified LLM.</p>"},{"location":"features/structured_outputs/#step-4-sequential-queries","title":"Step 4: sequential queries","text":"<p>This is where the core logic of the example unfolds. We make a series of calls to the <code>convo.query()</code> method:</p> <ol> <li>Find ChEMBL ID: We ask the LLM to use the <code>get_chembl_id</code> tool to find the ChEMBL ID for the drug \"imatinib\".</li> <li>Get Mechanisms of Action: Using the ChEMBL ID obtained (implicitly from the conversation history), we ask the LLM to use the <code>get_mechanisms_of_action</code> tool to fetch drug mechanisms and targets.</li> <li>Structure the Output: Finally, we ask the LLM to consolidate all the information gathered in the previous steps into the <code>DrugTargetsOutput</code> structure we defined. No tools are passed in this step; only the <code>structured_model</code> is provided.</li> </ol> <p><pre><code># 4.1. Query to find ChEMBL ID\ndrug_name = \"imatinib\"\nquery1_result = convo.query(\n    f'Get the ChEMBL ID for the drug \"{drug_name}\"',\n    tools=[get_chembl_id]\n)\n# 4.2. Get the mechanisms of action and targets\n# The LLM should use the ChEMBL ID from the previous turn's tool_result\nquery2_result = convo.query(\n    f'Now get its mechanisms of action and targets using the previously found ChEMBL ID.',\n    tools=[get_mechanisms_of_action]\n)\n# 4.3. Get the structured output\n# The LLM will use the conversation history (drug name, ChEMBL ID, mechanism data)\n# to populate the DrugTargetsOutput model.\nresults = convo.query(\n    \"Consolidate all the information gathered about imatinib, including its ChEMBL ID, \"\n    \"and its mechanisms of action and targets, into the predefined structure. \"\n    structured_model=DrugTargetsOutput,\n)\n</code></pre> What we obtain:</p> <ul> <li>After the first query, the conversation history contains the ChEMBL ID for \"imatinib\" (e.g., \"CHEMBL181\").</li> <li>After the second query, the history includes detailed mechanism and target data from OpenTargets.</li> <li>After the third query, <code>structured_response_json</code> should hold a JSON string that conforms to our <code>DrugTargetsOutput</code> Pydantic model, containing all the consolidated information.</li> </ul>"},{"location":"features/structured_outputs/#step-5-parse-and-use-the-structured-output","title":"Step 5: parse and use the structured output","text":"<p>The final step is to parse the JSON string received from the LLM back into our Pydantic model. This allows us to easily access the data in a type-safe way. <pre><code># Parse the JSON string into the Pydantic model\n# Convert the JSON string to a Python dictionary\ndrug_data = json.loads(results[0])\n# Print the structured information\nprint(f\"Drug Name: {drug_data['drug_name_queried']}\")\nprint(f\"ChEMBL ID: {drug_data['chembl_id_found']}\")\nprint(\"\\nMechanisms of Action and Targets:\")\nfor i, mechanism in enumerate(drug_data['mechanisms_and_targets'], 1):\n    print(f\"\\n{i}. Mechanism: {mechanism['mechanism_of_action']}\")\n    print(f\"   Target: {mechanism['target_name']}\")\n    print(\"   Associated Targets:\")\n    for target in mechanism['targets']:\n        print(f\"     - {target['approved_symbol']} (ID: {target['target_id']})\")\nif drug_data['error_message']:\n    print(f\"\\nError: {drug_data['error_message']}\")\n</code></pre> This multi-step approach demonstrates how to:</p> <ol> <li>Use tools to gather information over several conversational turns.</li> <li>Leverage structured outputs to consolidate and format the gathered information according to a predefined schema.</li> </ol>"},{"location":"features/structured_outputs/#complete-example-script","title":"Complete example script","text":"<p>For convenience, here is the full script combining all the steps: <pre><code>import requests\nfrom biochatter.llm_connect import LangChainConversation\nfrom pydantic import BaseModel, Field\nfrom langchain_core.tools import tool\nfrom typing import List, Dict, Any, Optional\nimport json\nfrom pprint import pprint\n\n# --- Step 1: Define the Tools ---\n@tool\ndef get_chembl_id(drug_name: str) -&gt; str:\n    \"\"\"\n    Given a drug name, look up and return its ChEMBL ID using the ChEMBL API.\n    Example: get_chembl_id(drug_name=\"aspirin\")\n    \"\"\"\n    url = \"https://www.ebi.ac.uk/chembl/api/data/chembl_id_lookup/search.json\"\n    params = {\"q\": drug_name}\n    headers = {\"Accept\": \"application/json\"}\n    try:\n        resp = requests.get(url, params=params, headers=headers, timeout=10)\n        resp.raise_for_status()\n        data = resp.json()\n        hits = data.get(\"chembl_id_lookups\", [])\n        if not hits:\n            return f\"Unable to find ChEMBL ID for {drug_name}\"\n        return hits[0].get(\"chembl_id\", f\"No ChEMBL ID found in hit for {drug_name}\")\n    except requests.RequestException as e:\n        return f\"Error querying ChEMBL API for {drug_name}: {str(e)}\"\n    except ValueError: # If JSON parsing fails\n        return f\"Invalid JSON received from ChEMBL API for {drug_name}\"\n\n_GRAPHQL_QUERY = \"\"\"\nquery MechanismsOfActionSectionQuery($chemblId: String!) {\n  drug(chemblId: $chemblId) {\n    id\n    mechanismsOfAction {\n      rows {\n        mechanismOfAction\n        targetName\n        targets {\n          id\n          approvedSymbol\n        }\n      }\n    }\n  }\n}\n\"\"\"\n\n@tool\ndef get_mechanisms_of_action(chembl_id: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Fetch mechanisms of action and target information for a given drug (by ChEMBL ID)\n    using the OpenTargets GraphQL API.\n    Example: get_mechanisms_of_action(chembl_id=\"CHEMBL25\")\n    \"\"\"\n    endpoint = \"https://api.platform.opentargets.org/api/v4/graphql\"\n    payload = {\"query\": _GRAPHQL_QUERY, \"variables\": {\"chemblId\": chembl_id}}\n    try:\n        resp = requests.post(endpoint, json=payload, headers={\"Content-Type\": \"application/json\"}, timeout=15)\n        resp.raise_for_status()\n        data = resp.json()\n        if \"errors\" in data:\n            return {\"error\": f\"GraphQL errors for {chembl_id}: {data['errors']}\"}\n        return data.get(\"data\", {})\n    except requests.RequestException as e:\n        return {\"error\": f\"Error querying OpenTargets API for {chembl_id}: {str(e)}\"}\n    except ValueError: # If JSON parsing fails\n        return {\"error\": f\"Invalid JSON received from OpenTargets API for {chembl_id}\"}\n\n# --- Step 2: Define the Pydantic Model for Structured Output ---\nclass TargetDetail(BaseModel):\n    approved_symbol: Optional[str] = Field(None, description=\"The approved symbol of the target (e.g., gene symbol).\")\n    target_id: Optional[str] = Field(None, description=\"The ID of the target (e.g., Ensembl ID).\")\n\nclass ActionMechanism(BaseModel):\n    mechanism_of_action: Optional[str] = Field(None, description=\"Description of the mechanism of action.\")\n    target_name: Optional[str] = Field(None, description=\"The name of the target associated with this mechanism.\")\n    targets: List[TargetDetail] = Field(default_factory=list, description=\"List of specific targets involved in this mechanism.\")\n\nclass DrugTargetsOutput(BaseModel):\n    drug_name_queried: str = Field(description=\"The original drug name queried.\")\n    chembl_id_found: Optional[str] = Field(None, description=\"The ChEMBL ID found for the drug.\")\n    mechanisms_and_targets: List[ActionMechanism] = Field(default_factory=list, description=\"List of mechanisms of action and associated targets.\")\n    error_message: Optional[str] = Field(None, description=\"Any error message encountered during the process.\")\n\n# --- Step 3: Initialize Conversation ---\n# Assuming LangChainConversation is already initialized and API key is set\n# For a self-contained script, you'd do:\nconvo = LangChainConversation(\n    model_provider=\"google_genai\", # Replace with your provider e.g. \"openai\"\n    model_name=\"gemini-2.0-flash\",  # Replace with your model e.g. \"gpt-4-turbo-preview\"\n    prompts={} # Using default prompts\n)\n# Set API key (read from environment variables)\nconvo.set_api_key()\n\n# --- Step 4: Sequential Queries ---\n# 4.1. Query to find ChEMBL ID\ndrug_name = \"imatinib\" # Example drug\nquery1_result = convo.query(\n    f'Get the ChEMBL ID for the drug \"{drug_name}\"',\n    tools=[get_chembl_id]\n)\n# 4.2. Get the mechanisms of action and targets\n# The LLM should use the ChEMBL ID from the previous turn's tool_result\nquery2_result = convo.query(\n    f'Now get its mechanisms of action and targets using the previously found ChEMBL ID.',\n    tools=[get_mechanisms_of_action]\n)\n# 4.3. Get the structured output\n# The LLM will use the conversation history (drug name, ChEMBL ID, mechanism data)\n# to populate the DrugTargetsOutput model.\nresults = convo.query(\n    \"Consolidate all the information gathered about imatinib, including its ChEMBL ID, \"\n    \"and its mechanisms of action and targets, into the predefined structure. \",\n    structured_model=DrugTargetsOutput,\n)\n\n# --- Step 5: Parse and Use the Structured Output ---\n# Parse the JSON string into the Pydantic model\n# Convert the JSON string to a Python dictionary\ndrug_data = json.loads(results[0])\n# Print the structured information\nprint(f\"Drug Name: {drug_data['drug_name_queried']}\")\nprint(f\"ChEMBL ID: {drug_data['chembl_id_found']}\")\nprint(\"\\nMechanisms of Action and Targets:\")\nfor i, mechanism in enumerate(drug_data['mechanisms_and_targets'], 1):\n    print(f\"\\n{i}. Mechanism: {mechanism['mechanism_of_action']}\")\n    print(f\"   Target: {mechanism['target_name']}\")\n    print(\"   Associated Targets:\")\n    for target in mechanism['targets']:\n        print(f\"     - {target['approved_symbol']} (ID: {target['target_id']})\")\nif drug_data['error_message']:\n    print(f\"\\nError: {drug_data['error_message']}\")\n</code></pre> In this multi-step approach:</p> <ol> <li>The first tool (<code>get_chembl_id</code>) is called to resolve a drug name to its ChEMBL ID.</li> <li>The second tool (<code>get_mechanisms_of_action</code>) uses this ID to fetch detailed data from OpenTargets.</li> <li>The final <code>query()</code> call doesn't use tools but instead provides the <code>DrugTargetsOutput</code> Pydantic model. The LLM is instructed to consolidate information from the preceding conversation turns (the initial drug name, the found ChEMBL ID, and the complex data from OpenTargets) into this predefined structure.</li> </ol> <p>This demonstrates how to chain tool calls and then use structured output to get a clean, predictable summary of the results.</p>"},{"location":"features/tool_chat/","title":"In-Chat Tool Calling","text":""},{"location":"features/tool_chat/#overview","title":"Overview","text":"<p>In addition to Ad Hoc API calling, BioChatter provides a framework for in-chat tool calling. This lets you integrate external tools directly into conversations with the LLM, enabling functionality beyond the model's built-in capabilities. Typical uses include:</p> <ul> <li>Performing complex calculations  </li> <li>Accessing external databases or APIs  </li> <li>Executing custom scripts or code  </li> </ul> <p>Many commercial LLMs (e.g., OpenAI, Anthropic, Google, Mistral) support tool calling natively.  Through ollama, BioChatter also supports a variety of open-source models that are natively capable of tool calling (e.g. mistral 7b,  mistral-small3.1, qwen2.5, cogito).</p> <p>For models without native tool calling, BioChatter provides a fallback by parameterizing the tool call in the prompt and then calling the tool via its own API. See below for more details.</p>"},{"location":"features/tool_chat/#defining-a-tool","title":"Defining a tool","text":"<p>BioChatter uses the LangChain framework to define and manage tools. You can create a new tool by decorating a function with the <code>@tool</code> decorator, for example:</p> <pre><code>from langchain_core.tools import tool\n\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n   \"\"\"Multiply two numbers.\"\"\"\n   return a * b\n\n@tool\ndef add(a: int, b: int) -&gt; int:\n   \"\"\"Add two numbers.\"\"\"\n   return a + b\n</code></pre> <p>LangChain exposes the tool's description and signature to the model, allowing the LLM to understand and use the tool within a chat session. For more information on customizing tools, see the LangChain documentation.</p>"},{"location":"features/tool_chat/#passing-a-tool-to-the-chat","title":"Passing a tool to the Chat","text":"<p>BioChatter\u2019s <code>LangChainConversation</code> class implements in-chat tool calling. Internally, it uses LangChain\u2019s <code>init_chat_model</code> function (see documentation), allowing a consistent interface for loading various LLM providers and models.  </p> <p>You can make tools available to the model in two ways:</p>"},{"location":"features/tool_chat/#1-at-the-beginning-of-the-conversation","title":"1. At the beginning of the conversation","text":"<pre><code>#import the conversation class\nfrom biochatter.llm_connect import LangChainConversation\n\n# Tools definition is recycled from the previous section\n\n# Define the question\nquestion = \"What is 334*54? And what about 345+123?\"\n\n# Gather the tools into a list\ntools = [multiply, add]\n\n# Define the conversation\nconvo = LangChainConversation(\n    model_provider=\"google_genai\", \n    model_name=\"gemini-2.0-flash\",\n    prompts={},\n    tools=tools #&lt;------\n)\n\n# Set the API key (read from the environment variable, based on the model provider)\nconvo.set_api_key()\n\n# Run the conversation\nconvo.query(question)\n</code></pre> <p>Tools passed in this way remain accessible for the entire conversation.</p>"},{"location":"features/tool_chat/#2-for-the-current-query","title":"2. For the current query","text":"<pre><code>#import the conversation class\nfrom biochatter.llm_connect import LangChainConversation\n\n# Tools definition is recycled from the previous section\n\n# Define the question\nquestion = \"What is 334*54? And what about 345+123?\"\n\n# Define the conversation\nconvo = LangChainConversation(\n    model_provider=\"google_genai\", \n    model_name=\"gemini-2.0-flash\",\n    prompts={},\n)\n\n# Set the API key (read from the environment variable, based on the model provider)\nconvo.set_api_key()\n\n# Run the conversation\nconvo.query(question, tools=tools) #&lt;------\n</code></pre> <p>Tools passed in this way are available only for the current query.</p>"},{"location":"features/tool_chat/#mcp-tools","title":"MCP tools","text":"<p>BioChatter supports the Model Context Protocol (MCP) for tool calling through the <code>langchain_mcp_adapters</code> library. Below is a simple example of defining an MCP server and integrating its tools into your chat.</p> <p>First, define the MCP server using the <code>FastMCP</code> class:</p> math_server.py<pre><code># math_server.py\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Math\")\n\n@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n@mcp.tool()\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers\"\"\"\n    return a * b\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n</code></pre> <p>Next, run the MCP server and provide the tools to the model. This involves asynchronous operations, so the code uses <code>async with</code> statements and <code>await</code> keywords:</p> <pre><code># import the needed libraries\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\nfrom langchain_mcp_adapters.tools import load_mcp_tools\nfrom pathlib import Path\nimport sys\n\n#patch event loop if running in a notebook\nif 'ipykernel' in sys.modules:\n    import nest_asyncio\n    nest_asyncio.apply()\n\n# define the path to the math_server.py file\nserver_path = Path('your_path_here')\n\n# define the server parameters\nserver_params = StdioServerParameters(\n    command=\"python\",\n    args=[server_path/\"math_server.py\"]\n)\n\nasync with stdio_client(server_params) as (read, write):\n    async with ClientSession(read, write) as session:\n\n        # Initialize the connection\n        await session.initialize()\n\n        # define the question\n        question = \"What is 2 times 2?\"\n\n        # Get tools\n        tools = await load_mcp_tools(session)\n\n        # define the conversation\n        convo = LangChainConversation(\n            model_provider=\"google_genai\", \n            model_name=\"gemini-2.0-flash\",\n            prompts={},\n            tools=tools,\n            mcp=True\n        )\n\n        # set the API key\n        convo.set_api_key()\n\n        # invoke the model\n        convo.query(question)\n</code></pre>"},{"location":"features/tool_chat/#tool-calling-modalities","title":"Tool calling modalities","text":"<p>When starting a conversation, you can specify one of two tool-calling modes:</p> <ul> <li> <p><code>\"auto\"</code>: If the model returns a tool call (or multiple tool calls), the tool(s) is/are automatically executed, and the result is added to the conversation history as a <code>ToolMessage</code>.  </p> </li> <li> <p><code>\"text\"</code>: If the model returns a tool call, the arguments for the tool are returned as text and stored in the conversation history as an <code>AIMessage</code>.</p> </li> </ul> <p>By default, the tool calling mode is <code>\"auto\"</code>.</p> <pre><code># Automatic tool calling\nconvo = LangChainConversation(\n    model_provider=\"google_genai\", \n    model_name=\"gemini-2.0-flash\",\n    prompts={},\n    tool_calling_mode=\"auto\" #&lt;------\n)\n\n# Textual tool calling\nconvo = LangChainConversation(\n    model_provider=\"google_genai\", \n    model_name=\"gemini-2.0-flash\",\n    prompts={},\n    tool_calling_mode=\"text\" #&lt;------\n)\n</code></pre>"},{"location":"features/tool_chat/#automatic-tool-call-interpretation","title":"Automatic tool call interpretation","text":"<p>BioChatter also allows you to interpret a tool\u2019s output automatically by specifying <code>explain_tool_result=True</code> when you query the model. This is particularly helpful when:</p> <ul> <li>The tool call returns large or complex data that you want summarized.</li> <li>The tool provides context (e.g., RAG) that should inform the final answer.</li> </ul> <p>For example, consider the following tool that performs enrichment analysis on a list of genes:</p> <pre><code>#import the needed libraries\nfrom gseapy import enrichr\n\n#define the new tool\n@tool\ndef enrichr_query(gene_list: list[str]):\n    \"\"\"Run enrichment analysis on a list of genes.\n\n    This tool allows to run enrichment analysis on a list of genes using the `gseapy` library.\n    Using this tool, a model can get information about the biological processes enriched in a set of genes.\n\n    Args:\n        gene_list: list of genes to run enrichment analysis on\n\n    Returns:\n        DataFrame: DataFrame containing the enrichment results\n    \"\"\"\n    # Run enrichment\n    enr = enrichr(\n        gene_list=gene_list,\n        gene_sets='GO_Biological_Process_2021',\n        organism='Human',\n        outdir=None,  # no files will be written\n        cutoff=0.05\n    )\n\n    # Save results as DataFrame\n    df_results = enr.results\n\n    return df_results\n</code></pre> <p>After defining the tool, you can enable automatic interpretation in the conversation:</p> <pre><code>#initialize the conversation\nconvo = LangChainConversation(\n    model_provider=\"google_genai\", \n    model_name=\"gemini-2.0-flash\",\n    prompts={},\n)\n\n#set the API key\nconvo.set_api_key()\n\n#define the question\nquestion = \"Which biological processes are regulated by TP53, BRCA1, BRCA2, PTEN, EGFR, MYC, CDK2, CDK4, CCND1, RB1?\"\n\n#run the conversation\nconvo.query(question, tools=[enrichr_query], explain_tool_result=True)\n\n#print the answer\nprint(convo.messages[-1].content)\n</code></pre> <p>By default, the model attempts to interpret any tool output it receives when you set <code>explain_tool_result=True</code>. You can further customize its interpretation by passing dedicated prompts through the <code>query</code> method:</p> <pre><code>general_instructions_tool_interpretation = \"Your tool interpretation here...\"\nadditional_instructions_tool_interpretation = \"Your additional tool interpretation here...\"\n\n#run the conversation\nconvo.query(\n    question,\n    tools=[enrichr_query],\n    explain_tool_result=True,\n    general_instructions_tool_interpretation=general_instructions_tool_interpretation,\n    additional_instructions_tool_interpretation=additional_instructions_tool_interpretation\n)\n</code></pre> <p>If you wish to inspect the defaults for these prompts, you can print them from the conversation object:</p> <pre><code>print(convo.general_instructions_tool_interpretation)\nprint(convo.additional_instructions_tool_interpretation)\n</code></pre>"},{"location":"features/tool_chat/#tool-calling-for-non-native-models","title":"Tool calling for non-native models","text":"<p>For models that do not natively support tool calling, BioChatter provides a simple workaround by automatically generating a prompt that leverages tools signatures and descriptions describing to the model how to use the tools.</p> <p>The interface for calling the tool is the same as in the native case and works also for MCP tools. Sometimes, it might be necessary to provide additional information to the model in the prompt to help it correctly use the tools. This can be done in two ways:</p>"},{"location":"features/tool_chat/#1-by-providing-additional-instructions-in-the-class-constructor","title":"1. by providing additional instructions in the class constructor","text":"<pre><code>#initialize the conversation\nconvo = LangChainConversation(\n    model_provider=\"ollama\", \n    model_name=\"gemma3:27b\",\n    prompts={},\n    tools=#[your tool list here],\n    additional_tools_instructions=\"...Here your additional instructions...\"\n)\n\n#set the API key\nconvo.set_api_key()\n\n#define the question\nquestion = \"...Here your question...\"\n\n#run the conversation\nconvo.query(question,)\n</code></pre>"},{"location":"features/tool_chat/#2-by-providing-additional-instructions-in-the-query-method","title":"2. by providing additional instructions in the <code>query</code> method","text":"<p><pre><code>#initialize the conversation\nconvo = LangChainConversation(\n    model_provider=\"ollama\", \n    model_name=\"gemma3:27b\",\n    prompts={},\n    tools=#[your tool list here]\n)\n\n#set the API key\nconvo.set_api_key()\n\n#define the question\nquestion = \"...Here your question...\"\n\n#run the conversation\nconvo.query(question,additional_tools_instructions=\"...Here your additional instructions...\")\n</code></pre> Note that by specifying additional instructions in the <code>query</code> method you will override any instructions provided in the class constructor. </p> <p>Remark: Given that this is a simple workaround, sometimes it might not be sufficient to add additional instructions in the prompt to get the model to correctly use the tools. In this case, we suggest to look at Ad Hoc API calling as a more robust solution.</p>"},{"location":"features/wasm/","title":"LLM in your Browser - WebAssembly","text":"<p>Note</p> <p>This feature is currently in beta and not under active development. Browser memory is limited, preventing the use of most current models. If you are interested in developing on this feature, please get in touch.</p>"},{"location":"vignettes/custom-bclight-advanced/","title":"Custom BioChatter Light: Advanced Use Case","text":"<p>For more advanced workflows, you may want to customise the BioChatter Light interface to display a new way of visualising your data or interacting with it. Due to the flexible, modular, and easy-to-use Streamlit framework, you can easily create new tabs and customise the existing ones. Here, we show how to create new tabs on the example of a project management tool we developed as a demonstration use case. The final web app is available at https://project.biochatter.org.</p>"},{"location":"vignettes/custom-bclight-advanced/#background","title":"Background","text":"<p>Managing a scientific group is challenging for multiple reasons, particularly one which has multiple interdependent projects, each of which is pursued by a small team of junior and senior researchers. To enhance productivity and communication, it could be beneficial to have a tool that takes away some of the burden of project management, to increase the available \"thinking time\" for the scientists (for further reading, see for example this article). In the context of our work, there are two components we see as essential: data management (FAIR and transparent) and simple interfaces (driven by conversational AI). We will be using BioCypher and BioChatter for these two components.</p> <p>We will use a GitHub Project board (here) as the data source for our project management tool. This is close to a real-world scenario and allows connectivity to code repositories, issues, pull requests, and other components of computational collaboration. The linked project is \"synthetic\" data for demonstration purposes. The repository to build the KG and deploy the BioChatter Light web app is available here.</p>"},{"location":"vignettes/custom-bclight-advanced/#build-the-kg","title":"Build the KG","text":"<p>GitHub Project access token</p> <p>Be aware that running this script will require a GitHub token with access to the project board. This token should be stored in the environment variable <code>BIOCYPHER_GITHUB_PROJECT_TOKEN</code>.</p> <p>We modified an existing adapter for the GitHub GraphQL API to pull data from the GitHub Project board. Thus, the time investment to build the KG was minimal (~3h); this is one central principle of BioCypher. We adapted the code (<code>project_planning/adapters/github_adapter.py</code>) and KG schema (<code>config/schema_config.yaml</code>) to represent the relevant features of the GitHub Project board. The pre-existing KG build and deploy scripts were used via the <code>docker-compose.yml</code> file. For public deployment, we also added a <code>docker-compose-password.yml</code>, which builds a password-protected version of the KG. Deployment and setup of the cloud VM took another ~2h.</p> <p>The schema of the knowledge graph is as follows:</p> <pre><code>graph LR;\n    Person -- Leads --&gt; Project\n    Project -- PartOf --&gt; Iteration\n    Project -- HasComment --&gt; Comment</code></pre> <p>Other available fields include <code>Title</code>, <code>Assignees</code>, <code>Status</code>, <code>Labels</code>, <code>Linked pull requests</code>, <code>Milestone</code>, <code>Repository</code>, <code>Reviewers</code>, <code>Priority</code>, <code>Size</code>, <code>Estimate</code>, <code>Iteration</code>, <code>Start date</code>, and <code>End date</code>. Some of these, such as <code>Priority</code> and <code>Size</code>, are properties of the project item in our current implementation. These assignments, including the schmema of the graph, can be flexibly adapted by using BioCypher mechanisms.</p>"},{"location":"vignettes/custom-bclight-advanced/#add-the-additional-tabs-to-biochatter-light","title":"Add the additional tabs to BioChatter Light","text":"<p>BioChatter Light has a modular architecture to accommodate flexible layout changes. We also added a configuration that allows turning on or off specific tabs via environment variables. For this project, we added three new tabs: \"Summary\", \"Tasks\", and \"Settings\". The \"Summary\" tab shows an overview of the completed tasks in the current iteration of the project, the \"Tasks\" tab shows the upcoming tasks of the group and each team member, and the \"Settings\" tab allows configuration of the queries and LLM instructions used to generate the content for the other tabs.</p> <p>The solution as a web app is only an illustration of the project management tool; for example, we'd envision the deployed version rather as an integration of common messengers (Zulip, Slack, etc.) that acts as a conversational assistant to the group and its members. The web app is a proof of concept and demonstration of the capabilities of BioChatter Light, simulating feedback to the group and individual users via the simplified interface.</p> <p>The tabs were added to the BioChatter Light codebase in the corresponding module (<code>components/panels/project.py</code>), which contains all three tabs. The Streamlit framework makes this relatively easy; each tab only requires about 100 lines of code in this module and only contains simple components such as columns, buttons, and text fields. We also added environment variables to the configuration (<code>components/config.py</code>) to allow turning on or off the new tabs.</p>"},{"location":"vignettes/custom-bclight-advanced/#configure-the-biochatter-light-docker-container","title":"Configure the BioChatter Light Docker container","text":"<p>As in the previous vignette, we can now configure the BioChatter Light Docker container to show only the new tabs. We provide these settings via the environment variables we introduced above, while turning off the default tabs. We have also added configurable environment variables for setting a page title, header, and subheader for the web app without having to change the source code.</p> <pre><code>services:\n  ## ... build, import, and deploy the KG ...\n  app:\n    image: biocypher/biochatter-light:0.6.10\n    container_name: app\n    ports:\n      - \"8501:8501\"\n    networks:\n      - biochatter\n    depends_on:\n      import:\n        condition: service_completed_successfully\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - BIOCHATTER_LIGHT_TITLE=Project Planner\n      - BIOCHATTER_LIGHT_HEADER=GitHub Project Planning Assistant\n      - BIOCHATTER_LIGHT_SUBHEADER=A BioChatter Demonstration App for integrated project planning using LLMs\n      - DOCKER_COMPOSE=true\n      - CHAT_TAB=false\n      - PROMPT_ENGINEERING_TAB=false\n      - RAG_TAB=false\n      - CORRECTING_AGENT_TAB=false\n      - KNOWLEDGE_GRAPH_TAB=false\n      - LAST_WEEKS_SUMMARY_TAB=true\n      - THIS_WEEKS_TASKS_TAB=true\n      - TASK_SETTINGS_PANEL_TAB=true\n</code></pre> <p>Authentication</p> <p>For using the app with the standard OpenAI LLM, we need to provide the <code>OPENAI_API_KEY</code> environment variable. This key can be obtained from the OpenAI website.</p> <p>You can see the full configuration in the <code>docker-compose.yml</code> file of the project-planning repository. For public deployment, we also added a password-protected version of the KG, which only requires a few additional lines in the <code>docker-compose-password.yml</code> file.</p> <p>To deploy the tool on a cloud VM, we now only need to run the following commands:</p> <pre><code>git clone https://github.com/biocypher/project-planning.git\ndocker-compose -f project-planning/docker-compose-password.yml up -d\n</code></pre>"},{"location":"vignettes/custom-bclight-advanced/#useful-tips-for-deployment","title":"Useful tips for deployment","text":"<p>Many vendors offer cloud VMs with pre-installed Docker and Docker Compose, as well as Nginx for reverse proxying. We recommend using a reverse proxy to provide HTTPS and a domain name for the web app. This can be done with a few lines in the Nginx configuration file. For example, to deploy the project management tool on a cloud VM with a domain name <code>project.biochatter.org</code>, you can use the following Nginx configuration:</p> <pre><code>server {\n    listen 80;\n    server_name project.biochatter.org;\n\n    location / {\n        proxy_pass http://localhost:8501;\n        proxy_http_version 1.1;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_read_timeout 86400;\n    }\n}\n</code></pre> <p>You can find more info here. We also recommend to set up certification with Let's Encrypt for HTTPS. This can be done with the Certbot tool, which is available for most Linux distributions.</p> <p>In addition, you need to make sure that your Neo4j deployment is accessible from your web app, and that the connection is secure. You can either make the DB accessible only on the VM's network, which would allow running it without encryption, or you can set up a secure connection with a password. Both options are implemented in the <code>docker-compose.yml</code> and <code>docker-compose-password.yml</code> files of the <code>project-planning</code> repository. You can find more info here.</p>"},{"location":"vignettes/custom-bclight-advanced/#conclusion","title":"Conclusion","text":"<p>This vignette showcases the creation of a custom BioChatter Light web app for a dedicated purpose, in this case, project management. The app is a demonstration of the flexibility and ease of use of the BioChatter Light framework, which allows for the rapid development of conversational AI interfaces for various applications. The project management tool is a proof of concept and will be further developed into a conversational assistant that can not only summarise, but interact with the group members, and provide administrative support for larger groups and even organisations.</p> <p>The capabilities of GitHub Projects and their API allow the transfer of issues between boards, which allows for a multi-level approach to project management. Higher-level master boards (such as the complex BioCypher ecosystem overview board) can collect the tasks and issues of a larger group, and the project management assistant can help in collating those into manageable chunks for smaller teams (such as the board of our synthetic project). The same abstraction can be used at the organisation level, where the aims and challenges of the organisation are broken down into projects and tasks for larger groups.</p>"},{"location":"vignettes/custom-bclight-simple/","title":"Custom BioChatter Light: Simple Use Case","text":"<p>For prototyping your own text2cypher workflow, it can be useful to have a simple interface for troubleshooting your queries. This vignette demonstrates how to customise the pre-built BioChatter Light Docker compose to turn off all tabs except the Knowledge Graph (KG) tab. This allows the deployment of an integrated KG build, deployment, and web app for LLM-based question answering.</p>"},{"location":"vignettes/custom-bclight-simple/#build-your-kg","title":"Build your KG","text":"<p>First, build your KG. For this example, we use the BioCypher Pole KG as a demo KG. The KG is based on an open-source dataset of crime statistics in Manchester. The schema of the demo KG is described in the Knowledge Graph RAG vignette. For building your own KG, refer to the BioCypher documentation.</p> <p>This KG is built, imported, and deployed in the first three stages of the <code>docker-compose.yml</code> file.</p>"},{"location":"vignettes/custom-bclight-simple/#configure-the-biochatter-light-docker-container","title":"Configure the BioChatter Light Docker container","text":"<p>We provide a simple way to customise the BioChatter Light Docker container to show only select components. We can provide these settings via environment variables, so in the case of running from <code>docker-compose.yml</code>, we can set these in the <code>environment</code> section of the <code>app</code> service.</p> <pre><code>services:\n  ## ... build, import, and deploy the KG ...\n  app:\n    image: biocypher/biochatter-light:0.6.10\n    container_name: app\n    ports:\n      - \"8501:8501\"\n    networks:\n      - biochatter\n    depends_on:\n      import:\n        condition: service_completed_successfully\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - DOCKER_COMPOSE=true\n      - CHAT_TAB=false\n      - PROMPT_ENGINEERING_TAB=false\n      - RAG_TAB=false\n      - CORRECTING_AGENT_TAB=false\n      - KNOWLEDGE_GRAPH_TAB=true\n</code></pre> <p>In this example, we provide our OpenAI API key and set the <code>DOCKER_COMPOSE</code> flag to <code>true</code>, which tells BioChatter Light to connect to the KG on the Docker network, which uses the service name as the hostname, so in this case, <code>deploy</code> instead of the default <code>localhost</code>.</p> <p>Authentication</p> <p>For using the app with the standard OpenAI LLM, we need to provide the <code>OPENAI_API_KEY</code> environment variable. This key can be obtained from the OpenAI website.</p> <p>We then turn off all default tabs (chatting, prompt engineering, RAG, and the correcting agent) and turn on the KG tab. Running the docker compose with these settings will build and deploy the KG and the BioChatter Light web app with only the KG tab enabled.</p> <pre><code>git clone https://github.com/biocypher/pole\ncd pole\ndocker-compose up -d\n</code></pre> <p>We are constantly expanding our repertoire of BioChatter Light tabs, so check back for more options in the future. Creating your own tabs is also accessible via our modular architecture and the simple Streamlit framework for UI design. Check the advanced vignette and the codebase of BioChatter Light for more information.</p> <p>Open-source LLMs</p> <p>You can use a locally hosted open-source LLM model by using the <code>docker-compose-ollama.yml</code> as an example (this deployment requires Ollama to be installed on your machine). See more here.</p>"},{"location":"vignettes/custom-decider-use-case/","title":"Custom BioChatter Light and Next: Cancer Genetics Use Case","text":"<p>This example is part of the BioChatter manuscript supplement. We demonstrate the resulting applications as web apps at https://decider-light.biochatter.org and https://decider-next.biochatter.org. Find more information on how to build and use these apps below.</p>"},{"location":"vignettes/custom-decider-use-case/#background","title":"Background","text":"<p>Personalised medicine tailors treatment to a patient's unique genetic makeup. In cancer care, this approach helps categorize patients and assign them to specific treatment groups in clinical trials. However, interpreting and making decisions based on this data is challenging due to the complexity of genetic variations, the interaction between genes and environmental factors, tumor diversity, patient histories, and the vast amount of data produced by advanced technologies.</p> <p>In the DECIDER consortium, we aim to improve clinical decisions by providing support systems, for instance for the geneticists working on these cases. The code for the use case lives at https://github.com/biocypher/decider-genetics.</p> <p>Below, we show how we build a support application for this use case.</p>"},{"location":"vignettes/custom-decider-use-case/#sources-of-knowledge","title":"Sources of knowledge","text":"<p>We integrate knowledge from diverse resources, using BioCypher to build a knowledge graph of:</p> <ol> <li> <p>Processed whole genome sequencing data of ovarian cancer patients (synthetic data)</p> <ul> <li> <p>genomic changes</p> <ul> <li> <p>classified by consequence (protein truncation, amino acid change)</p> </li> <li> <p>algorithmic prediction of deleteriousness</p> </li> <li> <p>variant identifiers </p> </li> </ul> </li> <li> <p>allele dosages</p> <ul> <li> <p>gene allele copy number (amplifications, deletions, loss-of-heterogeneity)</p> </li> <li> <p>mutation pervasiveness (estimate of number of affected alleles, or suspected subclonality)</p> </li> </ul> </li> <li> <p>proportion of cancer cells in the sample (tumour purity)</p> </li> </ul> </li> <li> <p>the patients' clinical history (synthetic data)</p> <ul> <li> <p>personal information (age at diagnosis, BMI, etc.)</p> </li> <li> <p>treatment history, known side effects, clinical response</p> </li> <li> <p>lab test results (blood, imaging, histopathology)</p> </li> <li> <p>common treatment-relevant mutations (BRCA), HR deficiency, PARP-inhibitor maintenance</p> </li> </ul> </li> <li> <p>data from open resources (real data)</p> <ul> <li> <p>variant annotations (as provided by the genetics pipeline of the DECIDER consortium)</p> </li> <li> <p>gene annotations (as provided by the genetics pipeline of the DECIDER consortium)</p> </li> <li> <p>pathway / process annotations (from public databases such as Gene Ontology)</p> </li> <li> <p>drug annotations (from OncoKB)</p> </li> </ul> </li> </ol> <p>In addition, we provide access to more resources via the RAG and API agents:</p> <ol> <li> <p>relevant publications from PubMed (real data) embedded in a vector database</p> </li> <li> <p>relevant knowledge streamed live from OncoKB (see below) via API access through BioChatter's API agent</p> </li> </ol>"},{"location":"vignettes/custom-decider-use-case/#the-geneticists-workflow","title":"The geneticist's workflow","text":"<p>Personalised cancer therapy is guided by identifying somatic genomic driver events in specific genes, particularly when these involve well-known hotspot mutations. However, unique somatic events in the same genes or pathways can create a \"grey zone\" that requires manual geneticist analysis to determine their clinical significance.</p> <p>To address this, a comprehensive BioCypher backend processes whole-genome sequencing data to catalog somatic changes, annotating their consequences and potential actionability. These data can then be linked to external resources for clinical interpretation. For example, certain mutations in the BRCA1 or ERBB2 genes can indicate sensitivity to specific treatments such as PARP inhibitors or trastuzumab.</p> <p>To fully leverage actionable data, the integration of patient-specific information with literature on drug targets and mechanisms of action or resistance is essential. OncoKB is the primary resource for this information, accessible through drug annotations added to the knowledge graph (KG) and via the BioChatter API calling mechanism.</p> <p>Additionally, semantic search tools facilitate access to relevant biomedical literature, enabling geneticists to quickly verify findings against established treatments or resistance mechanisms.</p> <p>In summary, the main contributions of our use case to the productivity of this workflow are:</p> <ul> <li> <p>making processed and analysed genomic data locally available in a centralised resource by building a custom KG</p> </li> <li> <p>allowing comparison to literature via semantic search inside a vector database with relevant publications</p> </li> <li> <p>providing live access to external resources via the API agent</p> </li> </ul>"},{"location":"vignettes/custom-decider-use-case/#building-the-application","title":"Building the application","text":"<p>We will explain how to use the BioCypher ecosystem, specifically, BioCypher and BioChatter, to build a decision support application for a cancer geneticist. The code base for this use case, including all details on how to set up the KG and the applications, is available at https://github.com/biocypher/decider-genetics. You can find live demonstrations of the application at links provided in the README of the repository. The build procedures can be reproduced by cloning the repository and running <code>docker-compose up -d</code> (or the equivalent for the Next app) in the root directory (note that the default configuration requires authentication with OpenAI services). The process involves the following steps:</p> <ol> <li> <p>Identifying data sources and creating a knowledge graph schema</p> </li> <li> <p>Building the KG with BioCypher from the identified sources</p> </li> <li> <p>Using BioChatter Light to develop and troubleshoot the KG application</p> </li> <li> <p>Customising BioChatter Next to yield an integrated conversational interface</p> </li> <li> <p>Deploying the applications</p> </li> </ol>"},{"location":"vignettes/custom-decider-use-case/#identifying-data-sources-and-creating-a-knowledge-graph-schema","title":"Identifying data sources and creating a knowledge graph schema","text":"<p>We examine the data sources described above and design a KG schema that can accommodate the data. The configuration file, schema_config.yaml, can be seen in the <code>config</code> directory of the repository. The schema should also be designed with LLM access in mind; performance in generating specific queries can be adjusted for in step three (troubleshooting using BioChatter Light). We created a bespoke adapters for the genetics data of the DECIDER cohort according to the output format of the genetics pipeline, and reused existing adapters for the open resources. They can be found in the decider_genetics/adapters directory of the repository. For this use case, we created synthetic data to stand in for the real data for privacy reasons; the synthetic data are available in the <code>data</code> directory.</p> <p>This is the schema of our KG:</p> <pre><code>graph TD;\n    Patient[Patient] --&gt;|PatientToSequenceVariantAssociation| SequenceVariant[SequenceVariant]\n    Patient[Patient] --&gt;|PatientToCopyNumberAlterationAssociation| CopyNumberAlteration[CopyNumberAlteration]\n    SequenceVariant[SequenceVariant] --&gt;|SequenceVariantToGeneAssociation| Gene[Gene]\n    CopyNumberAlteration[CopyNumberAlteration] --&gt;|CopyNumberAlterationToGeneAssociation| Gene[Gene]\n    Gene[Gene] --&gt;|GeneToBiologicalProcessAssociation| BiologicalProcess[BiologicalProcess]\n    Gene[Gene] --&gt;|GeneDruggabilityAssociation| Drug[Drug]</code></pre>"},{"location":"vignettes/custom-decider-use-case/#building-the-kg-with-biocypher","title":"Building the KG with BioCypher","text":"<p>In the dedicated adapters for the DECIDER genetics data, we pull the data from the synthetic data files and build the KG. We perform simplifying computations, as described above, to facilitate standard workflows (such as counting alleles, identifying pathogenic variants, and calculating tumour purity). We mold the data into the specified schema in a transparent and reproducible manner by configuring the adapters (see the decider_genetics/adapters directory).</p> <p>After creating the schema and adapters, we run the build script to populate the KG. BioCypher is configured using the biocypher_config.yaml file in the <code>config</code> directory. Using the Docker Compose workflow included in the BioCypher template repository, we build a containerised version of the KG. We can inspect the KG in the Neo4j browser at <code>http://localhost:7474</code> after running the build script. Any changes, if needed, can be made to the configuration of schema and adapters.</p>"},{"location":"vignettes/custom-decider-use-case/#using-biochatter-light-to-develop-and-troubleshoot-the-kg-application","title":"Using BioChatter Light to develop and troubleshoot the KG application","text":"<p>Upon deploying the KG via Docker, we can use a custom BioChatter Light application to interact with the KG. Briefly, we remove all components except the KG interaction panel via environment variables in the docker-compose.yml file (see also the corresponding vignette). This allows us to start the KG and interact with it using an LLM in a reproducible manner with just one command. We can then test the LLM-KG interaction by asking questions and examining the generated queries and its results from the KG. Once we are satisfied with the KG schema and LLM performance, we can advance to the next step.</p> <p>OpenAI API key needed</p> <p>In the standard configuration, we use the OpenAI API to generate queries. Provide your <code>OPENAI_API_KEY</code> in the shell environment, or modify the application to call a different LLM.</p> <p>The BioChatter Light application, including the KG creation, can be built using <code>docker compose up -d</code> in the root directory of the repository.</p> <p>An online demonstration of this application can be found at https://decider-light.biochatter.org. You can use this demonstration to test the KG - LLM interaction, asking questions such as:</p> <ul> <li> <p>How many patients do we have on record, and what are their names?</p> </li> <li> <p>What was patient1's response to previous treatment, and which treatment did they receive?</p> </li> <li> <p>Which patients have HR deficiency but have not received PARP inhibitors?</p> </li> <li> <p>How many patients had severe adverse reactions, and to which drugs?</p> </li> <li> <p>Does patient1 have a sequence variant in a gene that is druggable? Which drug, and what evidence level has the association?</p> </li> <li> <p>Does patient1 have a sequence variant in a gene that is druggable with evidence level \"1\"? Which drug? Return unique values.</p> </li> <li> <p>Does patient1 have a copy number variant in a gene that is druggable with evidence level \"1\"? Which drug? Return unique values.</p> </li> </ul> <p>The query returned by the model can also be modified and rerun without an additional call to the LLM, allowing for easy troubleshooting and exploration of the KG. The schema information of the KG is displayed in the lower section of the page for reference.</p>"},{"location":"vignettes/custom-decider-use-case/#customising-biochatter-next-to-yield-an-integrated-conversational-interface","title":"Customising BioChatter Next to yield an integrated conversational interface","text":"<p>We can further customise the Docker workflow to start the BioChatter Next application, including its REST API middleware <code>biochatter-server</code>. In addition to deploying all software components, we can also customise its appearance and functionality. Using the biochatter-next.yaml configuration file (in <code>config</code>, as all other configuration files), we can adjust the welcome message, how-to-use section, the system prompts for the LLM, which tools can be used by the LLM agent, the connection details of externally hosted KG or vectorstore, and other parameters. We then start BioChatter Next using a dedicated Docker Compose file, which includes the <code>biochatter-server</code> middleware and the BioChatter Next application.</p> <p>OpenAI API key needed</p> <p>In the standard configuration, we use the OpenAI API to generate queries. Provide your <code>OPENAI_API_KEY</code> in the <code>.bioserver.env</code> file, or modify the application to call a different LLM.</p> <p>The BioChatter Next application, including the customisation of the LLM and the integration of the KG, can be built using <code>docker compose -f docker-compose-next.yml up -d</code> in the root directory of the repository. An online demonstration of this application can be found at https://decider-next.biochatter.org.</p>"},{"location":"vignettes/custom-decider-use-case/#deploying-the-applications","title":"Deploying the applications","text":"<p>The final step is to deploy one or both applications on a server. Using the Docker Compose workflow, we can deploy the applications in many different environments, from local servers to cloud-based solutions. The environment supplied by the Docker software allows for high reproducibility and easy scaling. The BioChatter Light app can be used for testing, but also to provide a simple one-way interface to the KG for users who do not need the full conversational interface. The BioChatter Next app can be configured to connect to KG and vectorstore deployments on different servers, allowing for a distributed architecture and dedicated maintenance of components; but it can also be deployed in tandem from one Docker Compose, for smaller setups or local use.</p>"},{"location":"vignettes/kg/","title":"Vignette: Knowledge Graph RAG","text":"<p>This vignette demonstrates the KG module of BioChatter as used by the BioChatter Next application. We connect to a BioCypher knowledge graph (KG) to retrieve relevant information for a given question. We then use the retrieved information to generate a response to the question. The application can connect to any real-world BioCypher KG by providing the connection details in the <code>KG Settings</code> dialog.</p>"},{"location":"vignettes/kg/#background","title":"Background","text":"<p>For the demonstration purposes of this vignette, we include a demo KG based on an open-source dataset of crime statistics in Manchester, because it allows us to redistribute the KG due to its small size and public domain licence, and because it is easily understood. This is non-trivial for most biomedical datasets; however, we are currently working on a synthetic biomedical example to extend this vignette. This is the schema of the demo KG:</p> <pre><code>graph LR;\n    Person(:Person) -- KNOWS --&gt; Person\n    Person -- FAMILY_REL --&gt; Person\n    Person -- LIVES_AT --&gt; Location(:Location)\n    Person -- PARTY_TO --&gt; Crime(:Crime)\n    Person -- MADE_CALL --&gt; PhoneCall(:PhoneCall)\n    Person -- RECEIVED_CALL --&gt; PhoneCall\n    Crime -- INVESTIGATED_BY --&gt; Officer(:Officer)\n    Crime -- OCCURRED_AT --&gt; Location\n    Object(:Object) -- INVOLVED_IN --&gt; Crime</code></pre> <p>The KG is adapted from a Neo4j tutorial, and is available as a BioCypher adapter including a BioChatter Light integration here. We also include it in an optional BioChatter Next Docker Compose configuration to allow trying it out locally.</p>"},{"location":"vignettes/kg/#usage","title":"Usage","text":"<p>In BioChatter Next, we first activate the KG functionality by clicking on the <code>KG Settings</code> button in the sidebar. In the settings dialog, we can activate the KG functionality and select how many results we want to retrieve. Returning to the conversation and enabling the KG functionality for the current chat (directly above the send button), we can then ask the model about the KG. The language model we use in this vignette is, as in the RAG vignette, <code>gpt-3.5-turbo-0613</code> (deprecated as of July 16 2024). The conversation is pasted below for convenience, including the queries generated by BioChatter.</p> <p></p> <p>In the background, the RagAgent module of BioChatter receives the question and generates a query to retrieve the desired information. This is then passed back to the primary model, which includes it in its answer generation.</p>"},{"location":"vignettes/kg/#conclusion","title":"Conclusion","text":"<p>The native integration of BioCypher KGs into the BioChatter framework allows for a seamless integration of KGs into the conversational AI. This in turn facilitates knowledge accessibility in a wide range of application domains.</p>"},{"location":"vignettes/kg/#note-the-apparent-inability-of-gpt-to-understand-certain-directionalities-and-how-biochatter-compensates-for-this","title":"Note: the apparent inability of GPT to understand certain directionalities, and how BioChatter compensates for this","text":"<p>Interestingly, while <code>gpt-3.5-turbo-0613</code> mostly does a formidable job at translating natural language questions into Cypher queries, it is remarkably obtuse in certain instances. For instance, for the relationship <code>INVESTIGATED_BY</code>, which connects a <code>Crime</code> to an <code>Officer</code>, GPT consistently fails to understand that the relationship implies that the <code>Officer</code> is the one who investigates the <code>Crime</code>. Instead, it consistently interprets the relationship as if the <code>Crime</code> investigates the <code>Officer</code>: it consistently proposes the query <code>MATCH (o:Officer)-[:INVESTIGATED_BY]-&gt;(c:Crime) RETURN c, o</code> instead of the correct <code>MATCH (c:Crime)-[:INVESTIGATED_BY]-&gt;(o:Officer) RETURN c, o</code>. We were not able to change this behaviour with any contextual prompt instructions.</p> <p>For this reason, the BioChatter <code>prompts.py</code> module uses the knowledge we have about the directionality of edges in the BioCypher KG to only propose options for patterns that actually exist in the KG. In the instance of <code>INVESTIGATED_BY</code>, this is the corresponding YAML definition in BioCypher:</p> <pre><code>investigated by:\n    is_a: [fact, core]\n    represented_as: edge\n    label_as_edge: INVESTIGATED_BY\n    input_label: INVESTIGATED_BY\n    source: crime\n    target: officer\n</code></pre> <p>The presence of the <code>source</code> and <code>target</code> annotations allows us to provide only the correct options to the LLM, which in turn allows the LLM to generate the correct query.</p> <p>Strikingly, this even applies to <code>GPT-4</code> and <code>ChatGPT</code>, which we used to generate the Mermaid diagram above. The model immediately proposed a correct Mermaid diagram for most of the entire schema definition YAML file, except for the relationship between <code>Crime</code> and <code>Officer</code>, which it reversed. Via the YAML we provide in the prompt, GPT has access to the correct directionality, but it still fails to understand it.</p> <pre><code>graph LR;\n    Person(:Person) -- KNOWS --&gt; Person\n    Person -- FAMILY_REL --&gt; Person\n    Person -- LIVES_AT --&gt; Location(:Location)\n    Person -- PARTY_TO --&gt; Crime(:Crime)\n    Person -- MADE_CALL --&gt; PhoneCall(:PhoneCall)\n    Person -- RECEIVED_CALL --&gt; PhoneCall\n    Officer(:Officer) -- INVESTIGATED_BY --&gt; Crime\n    Crime -- OCCURRED_AT --&gt; Location\n    Object(:Object) -- INVOLVED_IN --&gt; Crime\n\n    classDef node fill:#f9f,stroke:#333,stroke-width:2px;\n    class Person,Officer,Location,Crime,PhoneCall,Object node;\n</code></pre> <p>Here is the full chat for reference.</p>"},{"location":"vignettes/kg/#conversation","title":"Conversation","text":"<p>The boxes below the BioChatter responses in the conversation show the queries generated by the RagAgent module.</p>"},{"location":"vignettes/kg/#message-from-you","title":"Message From You","text":"<p>How many crimes happened in total?</p>"},{"location":"vignettes/kg/#message-from-biochatter","title":"Message From BioChatter","text":"<p>According to the provided statement, the total number of crimes is 28,762. This information suggests that there have been 28,762 reported crimes in the given context. However, without additional information or context, it is difficult to determine the specific time period, location, or nature of these crimes. It's important to note that crime statistics can vary depending on the source, methodology, and reporting practices. Therefore, it is always advisable to consult official crime reports or local law enforcement agencies for the most accurate and up-to-date information on crime statistics.</p> <pre><code>MATCH (c:Crime)\nRETURN count(c) AS numberOfCrimes\n</code></pre>"},{"location":"vignettes/kg/#message-from-you_1","title":"Message From You","text":"<p>Where happened most crimes?</p>"},{"location":"vignettes/kg/#message-from-biochatter_1","title":"Message From BioChatter","text":"<p>Based on the provided statement, it appears that the location with the highest number of crimes is a parking area, with a total of 811 reported crimes. However, without additional context or information, it is difficult to determine the consistency or inconsistency of this statement with other available information. It is important to note that crime rates and locations can vary significantly depending on various factors such as the region, city, neighborhood, and specific time period being considered. To provide a more comprehensive analysis, it would be helpful to have access to a broader dataset or additional information about the specific location and its crime statistics.</p> <pre><code>MATCH (c:Crime)-[:OCCURRED_AT]-&gt;(l:Location)\nRETURN l.address AS location, COUNT(c) AS crimeCount\nORDER BY crimeCount DESC\n</code></pre>"},{"location":"vignettes/kg/#message-from-you_2","title":"Message From You","text":"<p>Who were the top 3 offenders?</p>"},{"location":"vignettes/kg/#message-from-biochatter_2","title":"Message From BioChatter","text":"<p>Based on the provided information, the top three offenders are:</p> <ol> <li> <p>Phillip Williamson: According to the information given, Phillip Williamson has committed 5 crimes.</p> </li> <li> <p>Jessica Kelly: The information states that Jessica Kelly has also committed 5 crimes.</p> </li> <li> <p>Brian Morales: The data indicates that Brian Morales has committed 4 crimes.</p> </li> </ol> <p>These three individuals, Phillip Williamson, Jessica Kelly, and Brian Morales, are the top offenders based on the number of crimes they have committed.</p> <pre><code>MATCH (p:Person)-[:PARTY_TO]-&gt;(c:Crime)\nRETURN p.name, p.surname, COUNT(c) AS num_crimes\nORDER BY num_crimes DESC\nLIMIT 3\n</code></pre>"},{"location":"vignettes/rag/","title":"Vignette: Retrieval-Augmented Generation (RAG)","text":"<p>This vignette demonstrates the RAG module of BioChatter as used by the BioChatter Next application. This basic use case involves an LLM manuscript reading assistant and a vector database with embedded scientific manuscripts. The manuscripts are recent reviews on the topic of ABC transporters (active transport proteins that often serve as efflux pumps in many tissues, with a major function of clearing exogenous substances) in drug resistance phenomena, which we currently investigate in our DECIDER cohort.  Manuscripts such as these can be absent from the knowledge base of LLMs for various reasons, such as their recency or the fact that they are not open access. RAG opens up the possibility to retrieve relevant information from these manuscripts, and to inject it into the LLM's generation process.</p>"},{"location":"vignettes/rag/#usage","title":"Usage","text":"<p>In BioChatter Next, we first activate the RAG functionality by clicking on the <code>RAG Settings</code> button in the sidebar. In the settings dialog, we can activate the functionality and upload an arbitrary number of documents, which is only limited by the scale of the vector database system. In this case, and for demonstration purposes, we uploaded the four manuscripts, which leads to the state shown in the screenshot below. You can find the citations below.</p> <p></p> <p>Note that we chose to split the documents into fragments of 1000 characters each. We could have split by tokens instead, or changed the fragment length and overlap. Fragment length, overlap, and splitting by tokens or characters are decisions that should be made in tandem with the choice of LLM model, and with the number of fragments that should be retrieved for each query. Most importantly, the total text length should not exceed the input context length of the model. Here, we choose to inject 10 fragments per query.</p> <p>We can now start a new conversation (using the <code>New Persona</code> button in the sidebar), for which we select the <code>Research manuscript helper (RAG)</code> persona, which includes suitable contextual prompts. Upon activating the RAG in the chat (to signal that we wish to perform RAG for our question), we can enter into a dialogue with the assistant. We use <code>gpt-3.5-turbo-0613</code> in this example (deprecated as of July 16 2024), which is a very affordable model. The procedure is demonstrated in the GIF below.</p> <p></p>"},{"location":"vignettes/rag/#comparison-with-chatgpt","title":"Comparison with ChatGPT","text":"<p>We can ask the same question to ChatGPT 4 (only subscription access).  By employing web search, ChatGPT 4 is able to find the same study that was the RAG result we were asked about in our follow-up question (Xu et al. 2015); however, the answer is not as detailed as the answer provided by the RAG process, and includes fewer mentions of explicit studies. While it includes sensible information, there is no way of validating most of the claims made by the model. You can read the full answer here. Overall, the quality of ChatGPT 4 is comparable to the RAG output, but the user has less control over the knowledge used, and the answer cannot include information that is not easily found on the web (for instance due to access restrictions). Of note, we used <code>gpt-3.5-turbo-0613</code> in this example, which is less recent and much less expensive than GPT-4.</p> <p>Asking the naive ChatGPT 3.5 (free access), the same question yields a very different answer. The free version of ChatGPT cannot search the internet, and does not provide any citations for the general answer it provides. You can read the full answer here. For convenience, all three conversations are copied below.</p>"},{"location":"vignettes/rag/#conclusion","title":"Conclusion","text":"<p>In conclusion, the RAG procedure we implement is able to dramatically improve the quality of GPT-3 answers, making them comparable to GPT-4 with web search, and even surpassing it in some aspects. We invite you to try it out yourself, as we provide an instance of BioChatter Next, including GPT access and a vector database instance, free of charge at https://next.biochatter.org.</p>"},{"location":"vignettes/rag/#conversations","title":"Conversations","text":""},{"location":"vignettes/rag/#biochatter-rag","title":"BioChatter RAG","text":""},{"location":"vignettes/rag/#contextual-prompt","title":"Contextual Prompt","text":"<p>You are an assistant to a biomedical researcher.</p>"},{"location":"vignettes/rag/#contextual-prompt_1","title":"Contextual Prompt","text":"<p>You have access to text fragments from one or multiple scientific manuscripts, which were retrieved by their semantic similarity to the user's question.</p>"},{"location":"vignettes/rag/#contextual-prompt_2","title":"Contextual Prompt","text":"<p>Your task is to help the user understand the manuscripts and their relevance to their questions. Be concise in your explanations, provide quotes from the manuscripts where appropriate, and be open about limitations in cases where the text does not contain pertinent information.</p>"},{"location":"vignettes/rag/#message-from-you","title":"Message From You","text":"<p>What is known about the CFTR protein in resistance to ovarian cancer?</p>"},{"location":"vignettes/rag/#message-from-biochatter","title":"Message From BioChatter","text":"<p>Based on the provided information, there are several studies that suggest a potential role of the CFTR (Cystic Fibrosis Transmembrane Conductance Regulator) protein in resistance to ovarian cancer. Here are some key findings from the referenced manuscripts:</p> <ol> <li> <p>Gene expression profiling of ovarian tumors before and after chemotherapy revealed molecular signatures of chemoresistant tumors (M, et al., 2006). This study did not specifically mention CFTR, but it provides insights into the molecular changes associated with chemoresistance.</p> </li> <li> <p>ABCF2 expression, which is related to CFTR, was investigated in clear cell adenocarcinoma of the ovary. The study found a relationship between ABCF2 expression and response to chemotherapy or prognosis (Tsuda H, et al., 2010).</p> </li> <li> <p>Alterations in gene copy numbers, including CFTR, were identified as a mechanism for acquired drug resistance in cancer (Yasui K, et al., 2004).</p> </li> <li> <p>ABCF2, an Nrf2 target gene related to CFTR, was found to contribute to cisplatin resistance in ovarian cancer cells (Bao L, et al., 2017).</p> </li> <li> <p>CFTR was not specifically mentioned in the context of ovarian cancer, but a study on hepatocellular carcinoma found that silencing circ-TCF4.85 inhibited cancer progression through microRNA-486-5p-targeted inhibition of ABCF2 (Gao J, et al., 2020).</p> </li> <li> <p>Constitutive activation of nuclear factor \u03baB (NF-\u03baB) was associated with CFTR expression and promoted cancer progression and poor prognosis in cervical cancer (Wu Z, et al., 2013).</p> </li> <li> <p>High levels of CFTR expression were associated with tumor aggression in ovarian cancer, and knockdown of CFTR suppressed ovarian cancer cell proliferation (Xu J, et al., 2015).</p> </li> </ol> <p>It is important to note that while these studies suggest a potential association between CFTR and resistance to ovarian cancer, the exact mechanisms and clinical implications are still being investigated. Further research is needed to fully understand the role of CFTR in ovarian cancer resistance.</p>"},{"location":"vignettes/rag/#message-from-you_1","title":"Message From You","text":"<p>Tell me more about the studies of aggression and knockdown of CFTR in vivo.</p>"},{"location":"vignettes/rag/#message-from-biochatter_1","title":"Message From BioChatter","text":"<p>Based on the provided information, there are two relevant studies regarding the aggression and knockdown of CFTR in ovarian cancer:</p> <ol> <li> <p>The study by Wu et al. (2013) investigated the constitutive activation of nuclear factor \u03baB (NF-\u03baB) and its contribution to CFTR expression, cancer progression, and poor prognosis in cervical cancer. Although this study is not specific to ovarian cancer, it provides insights into the role of CFTR and NF-\u03baB in promoting cancer progression.</p> </li> <li> <p>The study by Xu et al. (2015) focused specifically on ovarian cancer and found that high levels of CFTR expression were associated with tumor aggression. They also demonstrated that knockdown of CFTR suppressed the proliferation of ovarian cancer cells in vitro and in vivo. This study suggests that CFTR may play a role in promoting ovarian cancer growth.</p> </li> </ol> <p>These studies provide evidence for the involvement of CFTR in cancer aggression and suggest that targeting CFTR could be a potential therapeutic strategy. However, it is important to note that these studies focus on different types of cancer (cervical and ovarian), and the specific mechanisms underlying CFTR's role in ovarian cancer aggression and its knockdown effects are not fully elucidated.</p> <p>It is worth mentioning that the other references provided in the text fragments are not directly related to the aggression and knockdown of CFTR in ovarian cancer. They discuss topics such as the structure and function of ABC transporters, drug resistance, and inflammation in different contexts.</p>"},{"location":"vignettes/rag/#chatgpt-4","title":"ChatGPT 4","text":""},{"location":"vignettes/rag/#anonymous","title":"Anonymous","text":"<p>What is known about the CFTR protein in resistance to ovarian cancer?</p>"},{"location":"vignettes/rag/#chatgpt","title":"ChatGPT","text":"<p>The CFTR protein, known as the cystic fibrosis transmembrane conductance regulator, plays a significant role in ovarian cancer. Research has shown that CFTR is expressed more in ovarian cancer tissues compared to benign ovarian tumors and normal ovaries. This overexpression of CFTR is associated with more advanced stages of ovarian cancer, poor histopathological grade, and higher serum Ca-125 levels, which are all markers of more aggressive disease. Interestingly, the serous type of ovarian cancer shows stronger CFTR staining compared to other types.</p> <p>Moreover, the role of CFTR extends beyond its traditional function as an ion channel. It interacts with various intracellular signaling pathways that are crucial in cancer progression. For example, in ovarian cancer, CFTR overexpression is linked to interactions with the c-Src signaling pathway, which is involved in cellular growth.</p> <p>The implications of CFTR expression in ovarian cancer are significant. In studies where CFTR was downregulated, there was a decrease in cell invasion, migration, proliferation, adhesion, and colony formation in vitro. In vivo experiments also showed that the tumorigenic abilities of ovarian cancer cells were significantly repressed when CFTR expression was reduced.</p> <p>These findings highlight the complex role of CFTR in ovarian cancer and suggest that it could be a potential target for therapeutic intervention. However, it is important to note that the role of CFTR in cancer is multifaceted and varies among different types of cancers. Therefore, a detailed understanding of its function in specific cancer types, like ovarian cancer, is crucial for developing targeted treatments\u200b\u200b\u200b\u200b (https://pubmed.ncbi.nlm.nih.gov/25738998/, https://jmhg.springeropen.com/articles/10.1186/s43042-022-00308-7).</p>"},{"location":"vignettes/rag/#chatgpt-35","title":"ChatGPT 3.5","text":""},{"location":"vignettes/rag/#anonymous_1","title":"Anonymous","text":"<p>What is known about the CFTR protein in resistance to ovarian cancer?</p>"},{"location":"vignettes/rag/#chatgpt_1","title":"ChatGPT","text":"<p>As of my last knowledge update in January 2022, there is limited direct information about the CFTR (Cystic Fibrosis Transmembrane Conductance Regulator) protein's role in resistance to ovarian cancer. CFTR is primarily associated with cystic fibrosis, a genetic disorder that affects the lungs and other organs. However, some research suggests potential links between CFTR and cancer, including ovarian cancer, although the mechanisms are not fully understood.</p> <p>Here are some key points to consider:</p> <p>CFTR Expression: Studies have shown that CFTR is expressed in various tissues, including the female reproductive system. Abnormal CFTR expression or function could potentially impact the development or progression of ovarian cancer. Research is ongoing to understand the role of CFTR in cancer biology. CFTR Mutations: Certain mutations in the CFTR gene have been associated with an increased risk of cancer, although the evidence for ovarian cancer specifically is limited. Mutations in CFTR are more commonly known for their role in cystic fibrosis. Individuals with cystic fibrosis have an increased risk of various cancers, including gastrointestinal cancers, but the link to ovarian cancer is not well-established. Ion Transport and Cancer: CFTR is involved in ion transport across cell membranes, and this function could potentially influence cellular processes relevant to cancer. Changes in ion transport can affect cell proliferation, apoptosis, and other cellular functions that are important in cancer development and progression. Potential Therapeutic Target: Some research suggests that modulating CFTR activity or expression could be a potential target for cancer therapy, including ovarian cancer. However, more research is needed to determine the feasibility and effectiveness of such approaches. It's important to note that the understanding of the role of CFTR in cancer, including ovarian cancer, is an evolving field, and new research findings may have emerged since my last update in January 2022. To get the most up-to-date information on this topic, I recommend consulting recent scientific literature and speaking with medical experts or researchers actively working in the field of ovarian cancer and CFTR biology.</p>"},{"location":"vignettes/rag/#references","title":"References","text":"<ul> <li> <p>[1] ABC transporters affects tumor immune microenvironment to regulate cancer immunotherapy and multidrug resistance</p> </li> <li> <p>[2] ABC transporters and the hallmarks of cancer: roles in cancer aggressiveness beyond multidrug resistance</p> </li> <li> <p>[3] Advances in the structure, mechanism and targeting of chemoresistance-linked ABC transporters</p> </li> <li> <p>[4] ATP-binding cassette (ABC) transporters in cancer: A review of recent updates</p> </li> </ul>"}]}